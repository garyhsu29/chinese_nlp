{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 引用我自己寫的 helper function: query_from_db可以直接從DB獲取資料>\n",
    "2. Import 會用到的 library\n",
    "3. 定義了python request 連線的 header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pre-implement db query function\n",
    "from db_func import query_from_db\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36',  'Connection': 'close'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 將資料print出來的function，方便debug。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_util(urls, processor):\n",
    "    for url in urls:\n",
    "        try:\n",
    "            print()\n",
    "            temp = processor(url[0])\n",
    "            for key in temp.keys():\n",
    "                print('{}: {}'.format(key.capitalize(), temp[key]))\n",
    "            print()\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print()\n",
    "            print(e)\n",
    "            print('ERROR URL:')\n",
    "            print(url)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 選出10個在DB之中有最多資料的新聞來源：\n",
    "```sql\n",
    "SELECT news_source,COUNT(*) \n",
    "FROM news_db.news_rss_feeds\n",
    "GROUP BY news_source\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 11;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  news_source    | COUNT(*) |\n",
    "|------------------|----------|\n",
    "| MSN              | 14204    |\n",
    "| 自由時報         | 4997     |\n",
    "| 大紀元           | 4394     |\n",
    "| 經濟日報         | 3589     |\n",
    "| ~新浪台灣新聞中心~ | ~3323~     |\n",
    "| PCHOME           | 3213     |\n",
    "| ETtoday          | 2585     |\n",
    "| 新頭殼要聞       | 2117     |\n",
    "| Rti 中央廣播電臺 | 2010     |\n",
    "| Yahoo!奇摩股市   | 1976     |\n",
    "| 公視新聞 |1796|  \n",
    "最後因為新浪新聞有很多url是無效的，因此沒有放入 DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 隨機選出50個Yahoo的新聞連結：\n",
    "在開發時我是確保50個新聞連結都通過，但為了怕notebook篇幅太長，我改成五個來demo\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = 'Yahoo!奇摩股市' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_yahoo_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = 'Yahoo!奇摩股市' ORDER BY RAND()\n",
    "LIMIT 5; \"\"\"\n",
    "yahoo_test_urls = query_from_db(random_yahoo_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def yahoo_content_processor(url):\n",
    "    res_dict = {}\n",
    "\n",
    "    r = requests.get(url, headers = headers)\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "    for br in soup.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        title_category = title_tag.string.split(' - ')\n",
    "        res_dict['news_title'] = title_category[0]\n",
    "        if len(title_category) > 1:\n",
    "            res_dict['news_category'] = title_category[1]\n",
    "    fb_app_tag = soup.find('meta', attrs = {'property':'fb:app_id'})\n",
    "    if fb_app_tag:\n",
    "        res_dict['news_fb_app_id'] = str(fb_app_tag['content'])\n",
    "\n",
    "    fb_page_tag = soup.find('meta', attrs = {'property':'fb:pages'})\n",
    "    if fb_page_tag:\n",
    "        res_dict['news_fb_page'] = str(fb_page_tag['content'])\n",
    "\n",
    "    #Optional\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'news_keywords'})\n",
    "    if keywords_tag:\n",
    "        res_dict['news_keywords'] = keywords_tag['content']\n",
    "\n",
    "    description_tag = soup.find('meta', attrs = {'name': 'description'})\n",
    "    if description_tag:\n",
    "        res_dict['news_description'] = description_tag['content']\n",
    "\n",
    "    time_tag = soup.find('time')\n",
    "    if time_tag:\n",
    "        try:\n",
    "            d1 = datetime.datetime.strptime(time_tag['datetime'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = res\n",
    "        except Exception as e:\n",
    "            logging.error(e)\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    article_body_tag = soup.find('article', attrs = {'itemprop':'articleBody'})\n",
    "    temp_content = []\n",
    "    links = []\n",
    "    links_descs = []\n",
    "    if article_body_tag:\n",
    "        p_tags = article_body_tag.find_all('p')\n",
    "        a_tags = article_body_tag.find_all('a')\n",
    "        if p_tags:\n",
    "            for index, p in enumerate(p_tags):\n",
    "                if p.get('content'):\n",
    "                    temp_content.append(p.get_text().strip())\n",
    "        if a_tags:\n",
    "            for a in a_tags:\n",
    "                if a.get_text().strip():\n",
    "                    links.append(a['href'])\n",
    "                    links_descs.append(a.get_text().strip())\n",
    "            res_dict['news_related_url'] = links\n",
    "            res_dict['news_related_url_desc'] = links_descs\n",
    "\n",
    "    if temp_content:    \n",
    "        # Capture the description start with 公告 to content\n",
    "        if title_category[0][:4] == '【公告】':\n",
    "            prefix = title_category[0]\n",
    "        else:\n",
    "            prefix = ''\n",
    "        content = prefix + ' '.join(temp_content).replace('。 ', '。\\n')\n",
    "        res_dict['news'] = content\n",
    "        return res_dict\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News_title: 今彩539第109136期開獎\n",
      "News_category: Yahoo奇摩股市\n",
      "News_fb_app_id: 399384933466174\n",
      "News_fb_page: 114062348679049\n",
      "News_description: （中央社台北2020年6月6日電）今彩539第109136期開獎，中獎號碼29、39、31、35、19。3星彩中獎號碼009，4星彩中獎號碼1576。實際中獎獎號以台彩公布為準。1090606\n",
      "News_published_date: 2020-06-06 12:50:51\n",
      "News: （中央社台北2020年6月6日電）今彩539第109136期開獎，中獎號碼29、39、31、35、19。3星彩中獎號碼009，4星彩中獎號碼1576。\n",
      "實際中獎獎號以台彩公布為準。1090606\n",
      "\n",
      "\n",
      "\n",
      "News_title: 《國際金融》搶進新興市場時機到？分析師揭2風險滅火\n",
      "News_category: Yahoo奇摩股市\n",
      "News_fb_app_id: 399384933466174\n",
      "News_fb_page: 114062348679049\n",
      "News_keywords: 新興市場\n",
      "News_description: 【時報-台北電】隨著疫情和緩，油價逐步回升，加上市場資金充沛，讓許多人將眼光投向了新興市場，希望在此找到新天地，不過，專家認為，「全球復甦狀況低於預期」以及「美陸關係緊張」，恐將成為新興市場的兩大壓力。 巴隆周刊報導，BCA Research首席新興市場分析師Arthur Budaghyan表示，儘管近期市場資金蜂擁進駐新興市場，但全球復甦狀況低於市場預期與陸美關係緊張，仍將成為新興市場的兩大風險。 ...\n",
      "News_published_date: 2020-06-05 09:14:24\n",
      "News: 【時報-台北電】隨著疫情和緩，油價逐步回升，加上市場資金充沛，讓許多人將眼光投向了新興市場，希望在此找到新天地，不過，專家認為，「全球復甦狀況低於預期」以及「美陸關係緊張」，恐將成為新興市場的兩大壓力。\n",
      "巴隆周刊報導，BCA Research首席新興市場分析師Arthur Budaghyan表示，儘管近期市場資金蜂擁進駐新興市場，但全球復甦狀況低於市場預期與陸美關係緊張，仍將成為新興市場的兩大風險。\n",
      "Budaghyan說明，在缺少基本面支撐下，就算資產價格看似持續上漲，但終究也會以崩跌收場。同時，他指出，現在市場的漲勢來自表現不佳的股票，像是新興市場、歐洲和價值股票，恐怕也暗示這是漲勢的最後一棒，而不是開始。\n",
      "其次，Budaghyan表示，雖然他們看好大陸市場，但基於美陸貿易風險加大，導致它們無法推薦任何大陸企業。他進一步提到，工業與貿易需要率先復甦，才能讓市場持續反彈，然而從陸股觀察，還沒看到這樣的趨勢。\n",
      "匯豐（HSBC）分析師更示警，後疫情時代下，新興市場必須積極針對基礎建設、教育、勞動市場和醫療保健的結構和財政改革，否則就將面臨漫長的復甦。(新聞來源：中時電子報 李祈函)\n",
      "\n",
      "\n",
      "\n",
      "News_title: 《觀光股》六角市場需求再現\n",
      "News_category: Yahoo奇摩股市\n",
      "News_fb_app_id: 399384933466174\n",
      "News_fb_page: 114062348679049\n",
      "News_description: 【時報-台北電】六角 （2732） 因市場預期走過第二季業績谷底後，下半年營運可逐步回升，利空出盡題材激勵近日股價仰攻，上周五帶量攻高，終場收漲停價168.5元，創近四個月波段高點，單周股價上漲12.33％，近一月漲11.22％。 　六角2019年EPS達9.7元創新高，董事會決議擬配息8.5元，盈餘配發率達87.63％。六角四大事業體營運已在2～4月時陸續落底，隨著疫情趨緩，加上報復性消費推波，可望加速營運升溫，而5月底迄今，六角加盟商原物料的訂單可開始增量，顯示市場再現需求，後市仍有高點可期。(新聞來源：工商時報─姚舜)...\n",
      "News_published_date: 2020-06-07 08:40:09\n",
      "News: 【時報-台北電】六角 （2732） 因市場預期走過第二季業績谷底後，下半年營運可逐步回升，利空出盡題材激勵近日股價仰攻，上周五帶量攻高，終場收漲停價168.5元，創近四個月波段高點，單周股價上漲12.33％，近一月漲11.22％。\n",
      "六角2019年EPS達9.7元創新高，董事會決議擬配息8.5元，盈餘配發率達87.63％。六角四大事業體營運已在2～4月時陸續落底，隨著疫情趨緩，加上報復性消費推波，可望加速營運升溫，而5月底迄今，六角加盟商原物料的訂單可開始增量，顯示市場再現需求，後市仍有高點可期。(新聞來源：工商時報─姚舜)\n",
      "\n",
      "\n",
      "\n",
      "News_title: 《研究報告》資金寬鬆 陸股後市喊旺\n",
      "News_category: Yahoo奇摩股市\n",
      "News_fb_app_id: 399384933466174\n",
      "News_fb_page: 114062348679049\n",
      "News_keywords: 存款準備金\n",
      "News_description: 【時報-台北電】資金寬鬆的環境下，對證券市場是最大的助力，中國人行4月定向下調存款準備金率1個百分點，共釋放長期資金約4千億人民幣，直接支援實體經濟隨著政策落地，預期大陸股市也會走上震盪盤升的大趨勢，結構性的機會也將增多。 　日盛中國戰略A股基金經理人黃上修表示，從技術上看，市場的主要壓力在3千點上方。在板塊布局方向以消費、TMT、逆周期股均衡配置。 　兩會對積極的財政政策要更加積極有為，穩健的貨幣政策要更加靈活適度作出更為明確的部署，後續寬財政寬貨幣，以及新基建等結構性政策利好將持續釋放，以新基建為政策方向的科技產業，及受惠擴大內需、逆周期行業可望持續受惠。 ...\n",
      "News_published_date: 2020-06-02 00:42:25\n",
      "News: 【時報-台北電】資金寬鬆的環境下，對證券市場是最大的助力，中國人行4月定向下調存款準備金率1個百分點，共釋放長期資金約4千億人民幣，直接支援實體經濟隨著政策落地，預期大陸股市也會走上震盪盤升的大趨勢，結構性的機會也將增多。\n",
      "日盛中國戰略A股基金經理人黃上修表示，從技術上看，市場的主要壓力在3千點上方。在板塊布局方向以消費、TMT、逆周期股均衡配置。\n",
      "兩會對積極的財政政策要更加積極有為，穩健的貨幣政策要更加靈活適度作出更為明確的部署，後續寬財政寬貨幣，以及新基建等結構性政策利好將持續釋放，以新基建為政策方向的科技產業，及受惠擴大內需、逆周期行業可望持續受惠。\n",
      "第一金中國世紀基金經理人張帆指出，今年正是大陸十三五計畫的收官年，但遭逢疫情的衝擊，造成第一季GDP負成長6.8％。大陸經濟過去是由出口、投資、內需三駕馬車拉動，又以出口和投資占大宗；但隨著勞動成本上漲，各國企業興起撤廠、撤資的想法，而中美貿易戰、新冠肺炎疫情讓問題進一步浮上檯面，將迫使大陸政府推動產業升級轉型，並強化內需市場，減緩出口、投資降溫的衝擊。\n",
      "安本標準投信投資長彭炫通表示，目前大陸各項活動緩步恢復正常，除了新增病例已降至極低，另一方面，工廠已大致恢復生產，上海也幾乎完全恢復正常，購物人潮與交通流量開始逐漸增加。\n",
      "由於不少企業改為在家工作，雲端、人工智慧、第五代行動通訊（5G）等科技皆加快了發展腳步。遊戲公司與社群網路皆展現強勁動能，用戶參與程度有增無減。最後，由於各國普遍採取封鎖措施，商業活動由線下轉到線上的趨勢更加明顯。(新聞來源：工商時報─記者孫彬訓／台北報導)\n",
      "\n",
      "\n",
      "\n",
      "News_title: 《國際產業》流感疫苗需求增 美連鎖藥局積極下單\n",
      "News_category: Yahoo奇摩股市\n",
      "News_fb_app_id: 399384933466174\n",
      "News_fb_page: 114062348679049\n",
      "News_keywords: 流感, 疫苗\n",
      "News_description: 【時報-台北電】美國連鎖藥局積極預備更多流感疫苗並且向藥廠增加訂單，以因應10月的流感高峰期，另也預期在第二波新冠肺炎可能引爆的疑慮下，美國人民接種疫苗的意願應會大增。 　美國最大連鎖藥局之一的CVS Health Corp表示，正在確認能取得足夠流感疫苗，因為預料會有更多人接種疫苗，以避免感染流感。 　另一家連鎖藥局Rite Aid也將流感疫苗的採購量提高40％，沃爾瑪（Walmart）和Walgreens ...\n",
      "News_published_date: 2020-06-07 05:20:34\n",
      "News: 【時報-台北電】美國連鎖藥局積極預備更多流感疫苗並且向藥廠增加訂單，以因應10月的流感高峰期，另也預期在第二波新冠肺炎可能引爆的疑慮下，美國人民接種疫苗的意願應會大增。\n",
      "美國最大連鎖藥局之一的CVS Health Corp表示，正在確認能取得足夠流感疫苗，因為預料會有更多人接種疫苗，以避免感染流感。\n",
      "另一家連鎖藥局Rite Aid也將流感疫苗的採購量提高40％，沃爾瑪（Walmart）和Walgreens Boots Alliance皆認同會有更多美國人接種流感疫苗的看法。\n",
      "疫苗藥廠也同樣大量增產以因應需求。澳洲疫苗製造商CSL旗下的Seqirus指出，客戶需求已經躍升10％。英國藥廠葛蘭素史克（GlaxoSmithKline）也提及，已作好提高產量的準備。\n",
      "路透／Ipsos在5月13～19日針對4,428位美國成人所作的調查，結果顯示約60％的受訪者擬於今秋接種流感疫苗。相較之下，過去願意接種疫苗的受訪者僅占不到一半比重。\n",
      "儘管接種流感疫苗不會防止感染新冠病毒，後者到目前為止尚未出現有效疫苗，但公衛官員表示，人們施打流感疫苗，有助於減少被傳染流感，避免到時候流感與新冠肺炎病患同時擠滿醫院的情況。\n",
      "美國范德比大學醫學中心傳染病專家夏夫納（William Schaffner）說：「今年秋冬之際，我們將面臨流感與新冠病毒的雙重襲擊。但至少流感是我們能對付的部分。」 根據美國疾病控制與預防中心（CDC）資料，藥廠2019年生產將近1.7億劑流感疫苗。截至上月為止，美國在2019～2020流感季節已造成74萬人住院，6.2萬人死亡。\n",
      "另據華爾街業者Bernstein資料，全球流感疫苗市場約50億美元，美國接種疫苗人數每增加1個百分點，就可為藥廠增加7,500萬美元的收入。(新聞來源：工商時報─陳怡均／綜合外電報導)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_util(yahoo_test_urls, yahoo_content_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 隨機選出50個 MSN 的新聞連結：\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = 'MSN' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_msn_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = 'MSN' ORDER BY RAND()\n",
    "LIMIT 5; \"\"\"\n",
    "msn_test_urls = query_from_db(random_msn_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def msn_content_processor(url):\n",
    "    res_dict = {}\n",
    "    prefix = ['相關報導', '※', '＊', '更多三立新聞網報導', '延伸閱讀', '資料來源', '更多中時電子報精彩報導', '看了這篇文章的人，也']\n",
    "    pattern = re.compile(r'^{}'.format('|'.join(prefix)))\n",
    "    \n",
    "    if url.startswith('https://www.msn.com/zh-tw/video/'):\n",
    "        return\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding='utf-8'\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "\n",
    "    if not soup:\n",
    "        return \n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        res_dict['news_title'] = title_tag.text.strip()\n",
    "        \n",
    "    \n",
    "    category_tag = soup.find('div', attrs = {'class':'logowrapper'})\n",
    "    if category_tag:\n",
    "        _, category = category_tag.get_text().strip().split('\\n')\n",
    "        res_dict['news_category'] = category\n",
    "        \n",
    "    description_tag = soup.find('meta', attrs = {'name': 'description'})\n",
    "    if description_tag:\n",
    "        res_dict['news_description'] = description_tag['content']\n",
    "\n",
    "    time_tag = soup.find('div', attrs = {'class': 'timeinfo-txt'})\n",
    "    if time_tag:\n",
    "        date_time_tag = time_tag.find('time')\n",
    "        try:\n",
    "            d1 = datetime.datetime.strptime(date_time_tag['datetime'], \"%Y-%m-%dT%H:%M:%S.000Z\") \n",
    "            d1 -= datetime.timedelta(hours=8)\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            date_res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = date_res\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(date_time_tag['datetime'], \"%Y-%m-%dT%H:%M:%SZ\") \n",
    "                d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e2:\n",
    "                content_parser.logger.info('MSN date error, url: {}'.format(url))\n",
    "    \n",
    "    article_body_tag = soup.find('section', attrs = {'itemprop':'articleBody'})\n",
    "    temp_content, links, links_descs = [], [], []\n",
    "    if article_body_tag:\n",
    "        p_tags = article_body_tag.find_all('p')\n",
    "        div_tags = article_body_tag.find_all('div')\n",
    "        a_tags = article_body_tag.find_all('a')\n",
    "        if p_tags:\n",
    "            for p in p_tags:\n",
    "                #print(p.get_text())\n",
    "                # Ignore the image caption\n",
    "                if p.find('span'):\n",
    "                    p.span.decompose()\n",
    "                if p.find('a'):\n",
    "                    p.a.decompose()\n",
    "                if p.text.strip():\n",
    "                    if pattern.match(p.text.strip()):\n",
    "                        break\n",
    "                    elif p.text.strip()[0] in ('▲','▼'):\n",
    "                        continue\n",
    "                    temp_content.append(p.text.strip())\n",
    "        elif div_tags:\n",
    "            for div in div_tags:\n",
    "                if div.find('span'):\n",
    "                    div.span.decompose()\n",
    "                if div.find('a'):\n",
    "                    div.a.decompose()\n",
    "                if div.text.strip():\n",
    "                    if pattern.match(div.text.strip()):\n",
    "                        break\n",
    "                    elif div.text.strip()[0] in ('▲','▼'):\n",
    "                        continue\n",
    "                    temp_content.append(div.text.strip())\n",
    "            \n",
    "        if len(a_tags):\n",
    "            for a in a_tags:\n",
    "                if len(a):\n",
    "                    if a['href'] == '#':\n",
    "                        continue\n",
    "                    if a.get_text().strip() and 'www' in a['href']:\n",
    "                        links.append(a['href'])\n",
    "                        links_descs.append(a.get_text().strip())\n",
    "            res_dict['news_related_url'] = links\n",
    "            res_dict['news_related_url_desc'] = links_descs\n",
    "\n",
    "    if len(temp_content):\n",
    "        content = '\\n'.join(temp_content)\n",
    "        res_dict['news'] = content\n",
    "\n",
    "    if not res_dict or 'news' not in res_dict: \n",
    "        #content_parser.logger.error('MSN url: {} did not process properly'.format(url))\n",
    "        print('MSN url: {} did not process properly'.format(url))\n",
    "        return \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News_title: 你正悄悄脫水中？ 6大原因讓你意想不到\n",
      "News_category: 生活\n",
      "News_description: 天氣超熱，你喝水了嗎？身體一旦缺水包括皮膚狀況、新陳代謝都受影響，還可能令人煩躁焦慮。只是除了曝曬、運動等因素，還有些脫水原因，可能讓你意想不到。 人體60%都是水分，根據康乃狄克大學（University of Connecticut）的研究，光是少掉1.5%的水分，情緒、活力、認知功能都會受衝擊。 脫水表示人體失去的水分多於攝取的水分，依程度分為輕度至重度，輕度脫水可自行居家護理，嚴重的話則可能要就醫。不過一般想到脫水，只覺得和水喝太少、流汗、太陽太大有關，其實脫水背後的因素包括年紀、飲食、甚至疾病： 1. 生理期 一般不會想到月經也可能導致脫水，但經期絕對要多多補充水分。雌激素與黃體素都會影響體內含水量，尤其有經前症候群的人，更需要多喝水。有些女性經血量特別多，連帶剝奪體內水分，如果你更換衛生棉的頻率超過每2小時1次，就要注意經期脫水的可能性。 2. 低醣飲食...\n",
      "News_published_date: 2020-06-04 00:57:27\n",
      "News_related_url: ['https://www.commonhealth.com.tw/article/article.action?nid=75597&utm_source=msn&utm_medium=referral&utm_campaign=msn', 'https://www.commonhealth.com.tw/article/article.action?nid=81409&utm_source=msn&utm_medium=referral&utm_campaign=msn', 'https://www.commonhealth.com.tw/article/article.action?nid=64963&utm_source=msn&utm_medium=referral&utm_campaign=msn', 'https://www.commonhealth.com.tw/article/article.action?nid=61963&utm_source=msn&utm_medium=referral&utm_campaign=msn', 'https://www.commonhealth.com.tw/blog/blogTopic.action?nid=2193&utm_source=msn&utm_medium=referral&utm_campaign=msn', 'https://www.commonhealth.com.tw/article/article.action?nid=81751&utm_source=msn&utm_medium=referral&utm_campaign=msn']\n",
      "News_related_url_desc: ['身體缺不缺水\\u3000看尿液顏色就知道！一圖秒懂', '尿液出現這情況要警覺！\\u3000中醫：可能是和腎臟病相關的蛋白尿症狀', '小甜甜「腎結石」痛到住院！一天該喝多少水才可預防？\\u300010個必知問題', '消除水腫要少喝水？錯！\\u3000醫分享「最有效的方法」', '預防膀胱炎與尿道炎\\u3000「喝水3原則」很重要', '※本文由《康健雜誌》授權報導，未經同意禁止轉載，點此查看原始文章']\n",
      "News: 天氣超熱，你喝水了嗎？身體一旦缺水包括皮膚狀況、新陳代謝都受影響，還可能令人煩躁焦慮。只是除了曝曬、運動等因素，還有些脫水原因，可能讓你意想不到。\n",
      "人體60%都是水分，根據康乃狄克大學（University of Connecticut）的研究，光是少掉1.5%的水分，情緒、活力、認知功能都會受衝擊。\n",
      "脫水表示人體失去的水分多於攝取的水分，依程度分為輕度至重度，輕度脫水可自行居家護理，嚴重的話則可能要就醫。不過一般想到脫水，只覺得和水喝太少、流汗、太陽太大有關，其實脫水背後的因素包括年紀、飲食、甚至疾病：\n",
      "一般不會想到月經也可能導致脫水，但經期絕對要多多補充水分。雌激素與黃體素都會影響體內含水量，尤其有經前症候群的人，更需要多喝水。有些女性經血量特別多，連帶剝奪體內水分，如果你更換衛生棉的頻率超過每2小時1次，就要注意經期脫水的可能性。\n",
      "很多人為了減重採取低醣飲食，但其實碳水化合物能增加體內的水分儲存，這就是為什麼有些人進行低醣飲食而快速減重，減掉的多半是水分。\n",
      "再加上米、麵等碳水化合物在烹煮時都會吸水，不吃碳水化合物，等於也少掉了一些水分攝取。因此如果採取低醣飲食減重，記得要多喝水，避免體內缺水。\n",
      "孕婦水腫的可能原因之一就是缺水，因為身體一旦缺水，體內會自然想留滯水分，讓水腫情況更嚴重。另外孕期心輸出量（cardiac output）增加，對水分需求更大，孕吐也會導致水分流失。\n",
      "產後哺乳期間因為母乳大部分都是水分，因此也很容易缺水，一定要比平常攝取更多的水分。\n",
      "（脫水原因不只是高溫而已。圖片來源：pixabay）\n",
      "脫水的另一個原因是藥物的影響。例如降血壓藥就可能有利尿劑成分，其他將腹瀉、嘔吐列為副作用的藥物，也可能導致脫水。\n",
      "營養補充品中，芹菜籽、蒲公英、水田芥（Watercress）等都有利尿效果，最好先諮詢營養師，詢問最佳的攝取量。\n",
      "年齡愈大，身體保水能力以及對於口渴的自覺能力都愈差，因此長者很容易脫水而不自知。年紀大了最好一起床就先喝1～2杯的水，出門時也別忘了隨身攜帶水瓶。\n",
      "（別等到口渴了才喝水。圖片來源：pixabay）\n",
      "糖尿病因為血糖控制不良導致糖分排至尿液裡，造成滲透性利尿，進而可能導致脫水。糖尿病病人容易口渴，但是覺得口渴前，其實身體已經缺水，所以隨身包包最好放一瓶水，臥室床頭也不妨放一杯水，提醒自己多喝水。\n",
      "簡易辨別可能脫水的指標包括：\n",
      "●口渴\n",
      "●小便呈現暗黃色、味道很臭\n",
      "●頭暈、頭痛\n",
      "●疲憊\n",
      "●嘴唇乾燥、眼睛乾澀\n",
      "●每天排尿頻率低於4次\n",
      "別等到口渴才喝水，除了時時補充水分，運動前、中、後更要喝水。生病中，特別是有嘔吐、腹瀉症狀的人也要加強水分攝取。\n",
      "\n",
      "\n",
      "\n",
      "News_title: 三年歷經三次民意大考，被罷免的韓國瑜還能有下一步？\n",
      "News_category: 生活\n",
      "News_description: 喧騰近半年的罷韓投票案終於落幕，激情也將回歸常態。但外界更好奇的是，被罷免的韓國瑜，未來該怎麼走？又將如何衝擊藍綠版圖？ 三年暴起暴落的政治路，韓國瑜暫時走進死胡同，他還有下一步嗎？高雄市長補選又將如何牽動藍綠版圖？ 在罷免通過的歷史時刻，韓國瑜今日（6／6）帶領高雄市政府全體一級主管，在四維行政中心大廳，向市民深深一鞠躬，並發表離別演說，表達「兩個感謝、三個遺憾和一個祝福」。 幾乎同時間，罷韓同意票超過90萬，已超過他2018年風光當選市長、大勝對手陳其邁15萬票時拿下的89.2萬票…… 雖然臉上仍勉強維持下笑容，仍看得出，韓國瑜略帶倦容，三年內打兩次選舉大戰、歷經三次民意大考，韓國瑜是史上第一人。幕僚直言：「市長已身心俱疲。」 張智傑攝 罷韓過後，韓國瑜仍是藍營焦點 成也高雄民意，敗也高雄民意。高雄人曾把韓國瑜捧上政治神壇，但現在也是高雄人親手終結他的政治生涯。...\n",
      "News_published_date: 2020-06-05 21:07:08\n",
      "News_related_url: []\n",
      "News_related_url_desc: []\n",
      "News: 三年暴起暴落的政治路，韓國瑜暫時走進死胡同，他還有下一步嗎？高雄市長補選又將如何牽動藍綠版圖？\n",
      "在罷免通過的歷史時刻，韓國瑜今日（6／6）帶領高雄市政府全體一級主管，在四維行政中心大廳，向市民深深一鞠躬，並發表離別演說，表達。\n",
      "幾乎同時間，罷韓同意票超過90萬，已超過他2018年風光當選市長、大勝對手陳其邁15萬票時拿下的89.2萬票……\n",
      "雖然臉上仍勉強維持下笑容，仍看得出，韓國瑜略帶倦容，三年內打兩次選舉大戰、歷經三次民意大考，韓國瑜是史上第一人。幕僚直言：「市長已身心俱疲。」\n",
      "張智傑攝\n",
      "成也高雄民意，敗也高雄民意。高雄人曾把韓國瑜捧上政治神壇，但現在也是高雄人親手終結他的政治生涯。\n",
      "所謂「成也蕭何，敗也蕭何，」韓流效應帶領國民黨在2018年縣市長一役大獲全勝。當時，韓國瑜個人聲勢如日中天，後來甚至直攻總統大位，雖然最後功虧一簣，但在這幾場政治賭局中，不僅關乎韓國瑜個人輸贏，也牽動國民黨的未來。\n",
      "師範大學政治系教授范世平直言，韓流政治神話破滅，但也曾是政壇奇蹟；而在過去，外省人不可能在高雄拿到80萬票，但韓國瑜拿到了，然而如今高雄人也可以對他提出罷免。在在證明高雄人對政治反應快速，造就「鮮猛」的政治氛圍，在各國政壇十分罕見。\n",
      "那麼，韓國瑜還能有下一步嗎？黨內人士盛傳他會參選下一屆黨主席，再次成為藍軍共主。范世平評估有望，畢竟韓粉支持堅定，但也斷言，罷免若通過，韓在高雄失去舞台，勢必打包走人、不提霸韓選舉結果訴訟。\n",
      "然而，也有人不這麼樂觀。韓國瑜曾經說過「得民心者得天下」，但罷韓這一仗，他輸得太慘，民意基礎已然崩解。\n",
      "此次罷韓票高達93.9萬票，比起當初89.2萬的當選票，還要多出近五萬票，等於韓國瑜的民意優勢完全流失，甚至動搖到藍營基本盤。也就是說，韓國瑜失去最後一個具體的政治籌碼，問鼎藍營共主的氣勢與合理性正在迅速萎縮。\n",
      "那萬一韓國瑜破釜沉舟呢？有時政觀察家指出，若韓國瑜仍有心於政壇，那他卸下市長光環，其實也是卸下政治包袱，不再動輒得咎，更有機會大鳴大放。若是這樣，與民眾黨的「藍白合作」可能提早發生，換取最大在野聯盟空間，一齊將壓力鍋拋回給執政黨。\n",
      "罷韓現場。池孟諭攝\n",
      "那麼，這場罷韓案會如何牽動高雄政治版圖？\n",
      "原本外界揣測，韓國瑜會在罷免後，提起選舉訴訟，以此反制民進黨的補選部署，但從韓昨晚的告白影片到今天的發言看來，打官司不會是選項。\n",
      "若是照此走向，三個月內將進行的市長補選，民進黨要派人選，有過實戰經驗的行政院副院長陳其邁，無疑是綠營必然人選。而陳作為黨內英系人馬，也有助於平衡高雄菊系、海派等派系角力。\n",
      "陳其邁。（資料照，陳之俊攝）\n",
      "而遍體鱗傷的國民黨，未必會貿然再戰，民眾黨雖然氣候未成，但立委蔡璧如已遷戶籍到高雄，備受關注。政界揣測，很可能是為了民眾黨首次在直轄市插旗暖身，同時為柯文哲在2024選總統，先打下南部基礎。\n",
      "不過，即使補選看似必勝，陳其邁及民進黨的未來也不會好過。地方政壇人士評估，泛藍在市議會黨團仍佔多數優勢，民進黨拿回市府政權後，不見得施政能順利展開，況且民進黨也得盡快為2022年高雄市長與議會改選布局，很可能又再掀起一場藍綠惡鬥、市政空轉。\n",
      "對高雄市民而言，即將換一個市長執政，卻不見得迎來新氣象，「市政會大步向前？不必抱太大期望啦。」該為政壇人士搖頭說。\n",
      "回顧台灣選舉史，大小選戰要衝高投票率，「藍、綠對決」是基本要件，但這次罷韓是反向選舉，執政的韓市府與國民黨都盡量壓低姿態、冷處理。\n",
      "投票前，韓國瑜及黨主席江啟臣則雙雙拍了影片，溫情訴求選民繼續支持、不要投票，卻仍無法力挽狂瀾。\n",
      "對國民黨而言，布局2022是眼下最嚴峻的挑戰。表面上看來，高雄市沒了，藍營還有其他14個藍營縣市，但在罷韓過後，國民黨不僅元氣大傷，面子也掛不住，整體氣勢大不如前。\n",
      "黨員更關心的是，韓如果選擇回歸黨務，對於力求轉型的國民黨，究竟是加分還是羈絆，牽動著後年選情。\n",
      "最後，罷韓案後是否會掀起一波又一波的罷免漣漪？\n",
      "「短期內應該不會再出現（罷免市長）。」范世平認為，畢竟罷免案仍有一定門檻及難度，韓國瑜效應不見得會出現在其他首長身上。\n",
      "然而，近日已經有媒體話題燒向「罷柯」「罷侯」（指台北市長柯文哲、新北市長侯友宜），後續動向還待觀察。\n",
      "在選罷法修法之後，今天台灣已經以25％（占選舉人總數）門檻成功罷免一位直轄市長，誰能肯定遍地烽火式的罷免行動不會發生？\n",
      "\n",
      "\n",
      "\n",
      "News_title: 雙子座-進程上不要太好急。\n",
      "News_category: 新聞\n",
      "News_description: 今日整體：★，容易生病。要多穿些衣服喔！一起工作可以互相鼓勵傾訴，一定會成功的喲！和好友分工合作就是 ...\n",
      "News_published_date: 2020-06-06 08:01:00\n",
      "News_related_url: []\n",
      "News_related_url_desc: []\n",
      "News: 今日整體：★\n",
      "容易生病。要多穿些衣服喔！一起工作可以互相鼓勵傾訴，一定會成功的喲！和好友分工合作就是訣竅。\n",
      "今日指南：進程上不要太好急。\n",
      "幸運處方：拋掉憂愁和負擔。\n",
      "今日愛情：★☆\n",
      "工作上的不順遂，讓你心煩意亂，連帶也影響和情人的關係，約會時最好不要觸及工作話題，否則會弄得不歡而散，不是對方不願意體諒，但你的抱怨破壞了氣氛，也連帶影響另一半的情緒。\n",
      "戀愛忠告：需要投注心力去挖掘才行。\n",
      "幸運處方：調整自己配合別人。\n",
      "今日工作：★★★\n",
      "雖然在平常的業務方面，不會有什麼不順，但工作方面可要小心，可能會隱藏著一些意想不到的紛爭和負面因素，如果能夠辦到的事就不要太拘泥了。\n",
      "工作指南：整合社交資源。\n",
      "開運格言：業精於勤，荒於嬉。\n",
      "今日金錢：★★☆\n",
      "財運不如預期，原本躍躍欲試的投資理財策略最好能夠有相關的應變措施。節流重於開源。可以藉由運動開發身體能旺來轉運。\n",
      "理財錦囊：避免因他人的關係投資失利。\n",
      "幸運處方：掌握大眾將是常勝軍。\n",
      "\n",
      "\n",
      "\n",
      "News_title: MLB》認錯道歉！運動家隊要給小聯盟球員薪水\n",
      "News_category: 運動\n",
      "News_description: 受不了輿論壓力與各方讉責，運動家隊球團老闆費雪（John Fisher）美國時間6日終於公開認錯，並同意繼續支付盟球員薪水，直到預定的小聯盟球季結束，6月份第1周每人400美元也將獲得追補。\n",
      "News_published_date: 2020-06-06 17:14:51\n",
      "News_related_url: ['https://www.chinatimes.com/realtimenews/20171113001478-260403?utm_source=msn_news_r_un&utm_medium=rss', 'https://www.chinatimes.com/realtimenews/20200526000020-260403?utm_source=msn_news_r_un&utm_medium=rss', 'https://www.chinatimes.com/realtimenews/20180214000836-260403?utm_source=msn_news_r_un&utm_medium=rss', 'https://www.chinatimes.com/realtimenews/20200602001755-260403?utm_source=msn_news_r_un&utm_medium=rss']\n",
      "News_related_url_desc: ['NBA》有錢人沒資格抱怨？勇士柯瑞反擊', '《時來運轉》買中華職棒運彩 送大明星簽名球', 'NBA》帕波維奇：白人過太爽 要重視黑人', 'NBA》帕總：川普是個無腦白癡懦夫']\n",
      "News: 受不了輿論壓力與各方讉責，運動家隊球團老闆費雪（John Fisher）美國時間6日終於公開認錯，並同意繼續支付盟球員薪水，直到預定的小聯盟球季結束，6月份第1周每人400美元也將獲得追補。\n",
      "費雪表示他犯了一個錯誤，就是在5月26日宣布6月起停止支付小聯盟球員薪水。其他球隊雖亦有類似決定，但運動家隊特別受到外界質疑，以致來自各方的批評聲浪越來越大。\n",
      "運動家隊之所以緊急踩煞車，主要是擔心這個決定會造成不利影響，使球隊在棒球圈聲名狼藉，球季結束後很可能簽不到自由球員。\n",
      "運動家隊先前解僱或讓三分之二的球探、球團發展部門員工放無薪假，如今費雪也決定為了他們成立一個緊急救援基金，協助他們渡過難關。\n",
      "據富比士雜誌統計，費雪的身價高達22億美元。他在球團發表的公開聲明中說，「經過幾天聽取球迷和其他人士的意見後，我們將立即開始向小聯盟球員支付薪水。」聲明又說，「這些球員代表我們的未來，顯然我們決定不付錢是不對的。」\n",
      "費雪重申，「我們的決定是錯誤的，我向我們的小聯盟球員和其他受到影響的人致歉！」\n",
      "\n",
      "\n",
      "\n",
      "News_title: 射手座-開朗樂觀的態度有助健康。\n",
      "News_category: 新聞\n",
      "News_description: 今日整體：★，投資、貸款活動，可能會令你的經濟顯得更加緊縮，不妨三思，進出股市或與人借貸都要謹慎為上 ...\n",
      "News_published_date: 2020-06-06 08:01:00\n",
      "News_related_url: []\n",
      "News_related_url_desc: []\n",
      "News: 今日整體：★\n",
      "投資、貸款活動，可能會令你的經濟顯得更加緊縮，不妨三思，進出股市或與人借貸都要謹慎為上。需要多加注意呼吸系統、四肢健康，該休息的時候就要休息，不要勉強自己。\n",
      "今日指南：開朗樂觀的態度有助健康。\n",
      "幸運處方：合作力量大。\n",
      "今日愛情：★★\n",
      "近期內愛情有疑雲罩頂時，你憑真本能來反應，或許會留下小心眼的把柄給對方，卻畢竟是為自己爭取釐清的機會，若任疑雲在心中盤旋，必定會釀成狂風暴雨，到時候誤會怎樣也解釋不清了。\n",
      "戀愛忠告：可以多請女性長輩介紹。\n",
      "幸運處方：人生不是得到就是學到。\n",
      "今日工作：★★☆\n",
      "思路不太清楚，與人溝通時，無法表達自己的想法。心情方面起伏大，看似已經明朗的路，卻危機重重，現實與理想之間，還是好好衡量一下吧。\n",
      "工作指南：注重團隊精神，不要太自我。\n",
      "幸運處方：調整自己配合別人。\n",
      "今日金錢：★★\n",
      "野心過大或操作失當，使你不錯的金錢運搞得一時會周轉不靈。\n",
      "理財錦囊：把餘錢放進銀行，不要輕易出手。\n",
      "幸運處方：歡樂能克服一切的挫折。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_util(msn_test_urls, msn_content_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 隨機選出50個自由時報的新聞連結：\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '自由時報' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltn_msn_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '自由時報' ORDER BY RAND()\n",
    "LIMIT 5; \"\"\"\n",
    "ltn_test_urls = query_from_db(ltn_msn_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltn_content_processor(url):\n",
    "    postfix = [\"\"\"一手掌握經濟脈動\n",
    "    點我訂閱自由財經Youtube頻道\"\"\", \n",
    "    \"\"\"☆民眾如遇同居關係暴力情形，可撥打113保護專線，或向各地方政府家庭暴力防治中心求助☆\"\"\", \n",
    "    \"\"\"☆健康新聞不漏接，按讚追蹤粉絲頁☆更多重要醫藥新聞訊息，請上自由健康網\"\"\",\n",
    "    \"\"\"《自由開講》是一個提供民眾對話的電子論壇，不論是對政治、經濟或社會、文化等新聞議題，有意見想表達、有話不吐不快，都歡迎你熱烈投稿。文長700字內為優，來稿請附真實姓名（必寫。有筆名請另註）、職業、聯絡電話、E-mail帳號。本報有錄取及刪修權，不付稿酬；請勿一稿多投，錄用與否將不另行通知。投稿信箱：LTNTALK@gmail.com\"\"\",\n",
    "    \"\"\"「武漢肺炎專區」請點此，更多相關訊息，帶您第一手掌握。\"\"\",\n",
    "    \"\"\"\"\"\"]\n",
    "    res_dict = {}\n",
    "    #headers = {'user-agent': 'Mozilla/5.0 (Macintosh Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36',  'Connection': 'close'}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "    if not soup:\n",
    "        return \n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        title_category = title_tag.string.split(' - ')\n",
    "        res_dict['news_title'] = title_category[0].strip()\n",
    "        if len(title_category) > 1:\n",
    "            res_dict['news_category'] = title_category[1].strip()\n",
    "        else:\n",
    "            try:\n",
    "                res_dict['news_title'], res_dict['news_category'] = title_tag.string.split('|')\n",
    "            except:\n",
    "                pass\n",
    "    fb_app_tag = soup.find('meta', attrs = {'property':'fb:app_id'})\n",
    "    if fb_app_tag:\n",
    "        res_dict['news_fb_app_id'] = str(fb_app_tag['content'])\n",
    "\n",
    "    fb_page_tag = soup.find('meta', attrs = {'property':'fb:pages'})\n",
    "    if fb_page_tag:\n",
    "        res_dict['news_fb_page'] = str(fb_page_tag['content'])\n",
    "\n",
    "    #Optional\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'news_keywords'})\n",
    "    if keywords_tag:\n",
    "        res_dict['news_keywords'] = keywords_tag['content']\n",
    "\n",
    "    description_tag = soup.find('meta', attrs = {'name': 'description'})\n",
    "    if description_tag:\n",
    "        res_dict['news_description'] = description_tag['content']\n",
    "\n",
    "    time_tag_1 = soup.find('meta', attrs = {'property': 'article:published_time'})\n",
    "    time_tag_2 = soup.find('meta', attrs = {'name': 'pubdate'})\n",
    "    if time_tag_1:\n",
    "        try:\n",
    "            d1 = datetime.datetime.strptime(time_tag_1.get('content'), \"%Y-%m-%dT%H:%M:%S+08:00\") \n",
    "            d1 -= datetime.timedelta(hours=8)\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            date_res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = date_res\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(time_tag_1.get('content'), \"%Y-%m-%dT%H:%M:%SZ\") \n",
    "                d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e2:\n",
    "                content_parser.logger.info('LTN date error {}, URL: {}'.format(e2, url))\n",
    "\n",
    "    elif time_tag_2:\n",
    "        try:\n",
    "            d1 = datetime.datetime.strptime(time_tag_2.get('content'), \"%Y-%m-%dT%H:%M:%S+08:00\")\n",
    "            d1 -= datetime.timedelta(hours=8)\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            date_res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = date_res\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(time_tag_2.get('content'), \"%Y-%m-%dT%H:%M:%SZ\") \n",
    "                d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e2:\n",
    "                content_parser.logger.info('LTN date error: {}, URL:{}'.format(e2, url))\n",
    "\n",
    "    article_body_tag = soup.find('div', attrs = {'itemprop':'articleBody'})\n",
    "    article_body_tag2 = soup.find('div', attrs = {'class':'text boxTitle boxText'})\n",
    "    temp_content = []\n",
    "    links = []\n",
    "    links_descs = []\n",
    "\n",
    "    if article_body_tag:\n",
    "        p_tags = article_body_tag.find_all('p', attrs = {'class': None})\n",
    "        a_tags = article_body_tag.find_all('a')\n",
    "        if p_tags:\n",
    "            for p in p_tags:\n",
    "                if p.find('span'):\n",
    "                    p.span.decompose()\n",
    "                if p.text:\n",
    "                    if p.text.strip() not in postfix :\n",
    "                        temp_content.append(p.text.strip())\n",
    "                    else:\n",
    "                        break\n",
    "        if len(a_tags):\n",
    "            for a in a_tags:\n",
    "                if len(a):\n",
    "                    if a['href'] == '#':\n",
    "                        continue\n",
    "                    if a.get_text().strip() and 'www' in a['href']:\n",
    "                        links.append(a['href'])\n",
    "                        links_descs.append(a.get_text().strip())\n",
    "            res_dict['news_related_url'] = links\n",
    "            res_dict['news_related_url_desc'] = links_descs\n",
    "    elif article_body_tag2:\n",
    "        p_tags = article_body_tag2.find_all('p', attrs = {'class': None})\n",
    "        a_tags = article_body_tag2.find_all('a')\n",
    "        if p_tags:\n",
    "            temp = []\n",
    "            for p in p_tags:\n",
    "                if p.find('span'):\n",
    "                    p.span.decompose()\n",
    "                if p.text:\n",
    "                    if p.text.strip() not in postfix :\n",
    "                        temp_content.append(p.text.strip())\n",
    "                    else:\n",
    "                        break\n",
    "        if len(a_tags):\n",
    "            for a in a_tags:\n",
    "                if len(a):\n",
    "                    if a['href'] == '#':\n",
    "                        continue\n",
    "                    if a.get_text().strip() and 'www' in a['href']:\n",
    "                        links.append(a['href'])\n",
    "                        links_descs.append(a.get_text().strip())\n",
    "            res_dict['news_related_url'] = links\n",
    "            res_dict['news_related_url_desc'] = links_descs\n",
    "\n",
    "    content = '\\n'.join(temp_content).strip()\n",
    "    if content:\n",
    "        res_dict['news'] = content\n",
    "\n",
    "    if not res_dict or 'news' not in res_dict:\n",
    "        content_parser.logger.error('LTN url: {} did not process properly'.format(url))\n",
    "        return\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News_title: 成功預言韓國瑜遭三殺！ 李正皓抖出「郭董落淚」內幕\n",
      "News_category: 自由娛樂\n",
      "News_fb_app_id: 140490219413038\n",
      "News_fb_page: 524751764333507\n",
      "News_keywords: 李正皓, 韓國瑜, 自由娛樂, ltn\n",
      "News_description: 〔記者徐郁雯／台北報導〕2018年韓國瑜以超強韓流之姿，選上高雄市長，只是他落跑選總統，不到2年光景，韓流終於被摧毀殆盡。曾是藍營名嘴的李正皓，因批評韓國瑜遭開除黨籍，如今他也悲憤吐露內情：「當時我被開除時，一群韓粉指著我的鼻子對著我罵背骨、叛徒。」表示當時是不想看國民黨被「三殺」，沒想到仍挽回不了\n",
      "News_published_date: 2020-06-07 06:49:46\n",
      "News_related_url: []\n",
      "News_related_url_desc: []\n",
      "News: 〔記者徐郁雯／台北報導〕2018年韓國瑜以超強韓流之姿，選上高雄市長，只是他落跑選總統，不到2年光景，韓流終於被摧毀殆盡。曾是藍營名嘴的李正皓，因批評韓國瑜遭開除黨籍，如今他也悲憤吐露內情：「當時我被開除時，一群韓粉指著我的鼻子對著我罵背骨、叛徒。」表示當時是不想看國民黨被「三殺」，沒想到仍挽回不了悲劇。\n",
      "李正皓表示，「去年，郭台銘董事長、我以及許多藍營朋友都曾公開預言，韓國瑜如果參選總統，唯一的下場就是讓國民黨被三殺（總統、立委、市長罷免），如今全應驗了。 」也說他們沒有超能力，只是有道德勇氣，認為腦袋正常的人，都知道當市長三個月落跑下場，只是國民黨不願正視。\n",
      "李正皓指出，「去年的國民黨從下到上都瀰漫著一股投機的氣息」，認為韓國瑜一人可以救全黨，揭露去年郭台銘落淚，並非有求韓國瑜，「而是一種預知了國民黨被三殺但又無力回天的心急。」談及被開除黨籍，李正皓指出，韓粉狂罵他背骨，但他寧願賭上前途，也要指出韓國瑜不適任原因，是因為不想看國民黨被三殺。\n",
      "如今，三殺預言成真，李正皓沉痛表示：「韓國瑜與國民黨的投機導致了今天的悲劇。」更說未來20年，國民黨在南台灣沒有可能再起，而當時捧韓的人全都裝死消失，「那所有的應報最後會誰來承擔，還是『國民黨』這三個字來承擔。 」預言國民黨這個百年招牌，將被一群投機份子玩到一文不值。\n",
      "\n",
      "\n",
      "\n",
      "News_title: 波蘭航空載逾百名台人返國 締造2國史上首次載客直航\n",
      "News_category: 政治\n",
      "News_fb_app_id: 140490219413038\n",
      "News_fb_page: 394896373929368\n",
      "News_keywords: 波蘭,武漢肺炎,新型冠狀病毒,防疫物資,COVID-19,波蘭航空,新型冠狀病毒病\n",
      "News_description: 波蘭台北辦事處今天表示，在波蘭航空協助下，超過110名台灣人今自華沙返抵桃園機場，是波蘭與台灣之間史上首次載客直航。波蘭航空公司包機今清晨抵達台灣，該包機載運116名滯留波蘭的國人，包括國人114人、外籍旅客2人、機組員15人，入境人數為131人。許多旅客都身著防護衣，戴口罩及護目鏡，在下機後進行檢疫程序後，搭乘檢疫遊覽車前往北部檢疫所集中檢疫14天。\n",
      "News_published_date: 2020-06-07 04:12:24\n",
      "News_related_url: []\n",
      "News_related_url_desc: []\n",
      "News: 在波蘭航空協助下，超過110位台灣人自華沙返抵桃園機場，波蘭台北辦事處表示這是波蘭與台灣之間史上首次載客直航。（擷取自波蘭台北辦事處臉書）\n",
      "〔記者呂伊萱／台北報導〕波蘭台北辦事處今天表示，在波蘭航空協助下，超過110名台灣人今自華沙返抵桃園機場，是波蘭與台灣之間史上首次載客直航。\n",
      "波蘭航空公司包機今清晨抵達台灣，該包機載運116名滯留波蘭的國人，包括國人114人、外籍旅客2人、機組員15人，入境人數為131人。許多旅客都身著防護衣，戴口罩及護目鏡，在下機後進行檢疫程序後，搭乘檢疫遊覽車前往北部檢疫所集中檢疫14天。\n",
      "波蘭台北辦事處今天以中文、英文與波蘭文3種語言分享此事表示，「我們很高興與大家分享，在波蘭航空的協助下，超過110位台灣人從波蘭返台。波蘭航空自華沙啟航，於6月7日凌晨5點50分抵達桃園機場。該包機由林氏國際顧問公司（ Lin's International Consulting Co., Ltd. ）承包，這是波蘭與台灣之間的史上首次載客直航。」\n",
      "波蘭航空在今年4月23日，也曾包機來台載運在台灣採購的10噸防疫物資返回波蘭。\n",
      "\n",
      "\n",
      "\n",
      "News_title: 超壯觀！中部午後強降雨 驚見雨瀑移動美景\n",
      "News_category: 生活\n",
      "News_fb_app_id: 140490219413038\n",
      "News_fb_page: 394896373929368\n",
      "News_keywords: 雨瀑,雨瀑奇景\n",
      "News_description: 近期受鋒面影響，南投各地都出現午後短時強降雨，雖然雨勢大，但都下得不久，就有南投民眾在鹿谷大崙山直擊雨瀑移動美景，只見烏雲籠罩天際，大雨不斷傾瀉，大範圍雨瀑則隨著強風移動，猶如揭開窗簾一般，之後則是雨過天青，大地恢復原貌，精彩自然美景，令人驚豔。拍攝民眾林建興表示，大崙山海拔約1500公尺，往南可見到雲林與彰化的平地，近日則因午後常有強降雨，在山區的雨下過之後，平地就會跟著下。\n",
      "News_published_date: 2020-06-07 07:16:49\n",
      "News_related_url: []\n",
      "News_related_url_desc: []\n",
      "News: 中台灣近日常有午後強降雨，有民眾在鹿谷山區直擊彰雲地區的雨瀑移動美景。（圖由林建興提供）\n",
      "〔記者劉濱銓／南投報導〕近期受鋒面影響，南投各地都出現午後短時強降雨，雖然雨勢大，但都下得不久，就有南投民眾在鹿谷大崙山直擊雨瀑移動美景，只見烏雲籠罩天際，大雨不斷傾瀉，大範圍雨瀑則隨著強風移動，猶如揭開窗簾一般，之後則是雨過天青，大地恢復原貌，精彩自然美景，令人驚豔。\n",
      "拍攝民眾林建興表示，大崙山海拔約1500公尺，往南可見到雲林與彰化的平地，近日則因午後常有強降雨，在山區的雨下過之後，平地就會跟著下。\n",
      "由於山上雨後空氣變得更清新乾淨，視野也相當開闊沒有雲層遮蔽，才會看到雨瀑的產生，尤其雨瀑還隨著風勢大規模移動，猶如揭開眼前的「雨幕」，千變萬化的氣象景致，著實令人嘆為觀止。\n",
      "中台灣近日常有午後強降雨，有民眾在鹿谷山區直擊彰雲地區的雨瀑移動美景。（圖由林建興提供）\n",
      "\n",
      "\n",
      "\n",
      "News_title: 中職》林哲瑄已隨隊一軍 10日預計正式登錄\n",
      "News_category: 自由體育\n",
      "News_fb_app_id: 140490219413038\n",
      "News_fb_page: 511286612305978\n",
      "News_keywords: 中職, 自由體育, ltn\n",
      "News_description: 富邦悍將原本今天要作客桃園，但和樂天桃猿之戰因雨延賽。賽前，富邦悍將林哲瑄已經隨隊一軍，他說，狀況已經好很多，評估已可歸隊，預計下週應能正式歸隊。林哲瑄說，昨天二軍自辦賽時他已下場守備，因下雨關係，比賽只打4局，但狀況大致已經好轉，他說：「一些\n",
      "News_published_date: 2020-06-07 08:37:07\n",
      "News_related_url: []\n",
      "News_related_url_desc: []\n",
      "News: 〔記者林宥辰／桃園報導〕富邦悍將原本今天要作客桃園，但和樂天桃猿之戰因雨延賽。賽前，富邦悍將林哲瑄已經隨隊一軍，他說，狀況已經好很多，評估已可歸隊，預計下週應能正式歸隊。\n",
      "林哲瑄說，昨天二軍自辦賽時他已下場守備，因下雨關係，比賽只打4局，但狀況大致已經好轉，他說：「一些飛球的判斷可能還要習慣一下，不過整體來說狀況還不錯。」\n",
      "林哲瑄也提到，這是他第一次因手肘不適導致必須長期休養，休息時間也比想像中長，自己也感到焦急：「花了滿多時間，除了治療、也配合防護員的課表。」\n",
      "富邦悍將總教練洪一中則說，若今天如期開打，本就有考慮今天就將林哲瑄登錄進一軍，而他下週應該就能如期歸隊。\n",
      "\n",
      "\n",
      "\n",
      "News_title: 連6週上漲！明起汽油調漲0.7元、柴油0.6元\n",
      "News_category: 自由財經\n",
      "News_fb_app_id: 140490219413038\n",
      "News_fb_page: 2049728081906577\n",
      "News_keywords: 中油,本週油價,汽柴油價格,汽油,油價\n",
      "News_description: 台灣中油公司宣佈，自明（8）日凌晨零時起汽、柴油價格每公升各調漲0.7元及0.6元。（資料照）\n",
      "〔即時新聞／綜合報導〕本週受北半球進入夏天開車用油旺季、OPEC+擬維持減產目標970萬桶/日不變等因素\n",
      "News_published_date: 2020-06-07 04:06:14\n",
      "News_related_url: ['https://www.youtube.com/channel/UCdm3nYbbJ3gcbbOWhGDNqhw']\n",
      "News_related_url_desc: ['點我訂閱自由財經Youtube頻道']\n",
      "News: 〔即時新聞／綜合報導〕本週受北半球進入夏天開車用油旺季、OPEC+擬維持減產目標970萬桶/日不變等因素影響，帶動國際原油價格上漲。台灣中油公司宣佈，自明（8）日凌晨零時起汽、柴油價格每公升各調漲0.7元及0.6元。\n",
      "調整後的參考零售價格分別為92無鉛汽油每公升20.6元、95無鉛汽油每公升22.1元、98無鉛汽油每公升24.1元、超級柴油每公升18.0元，實際零售價格以各營業點公告為準。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_util(ltn_test_urls,ltn_content_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 隨機選出50個大紀元的新聞連結：\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '大紀元' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_test_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '大紀元' ORDER BY RAND()\n",
    "LIMIT 5; \"\"\"\n",
    "epoch_test_urls = query_from_db(epoch_test_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_content_processor(url):\n",
    "    res_dict = {}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding='utf-8'\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "    if not soup:\n",
    "        return \n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        title_category = title_tag.string.split(' | ', 1)\n",
    "        res_dict['news_title'] = title_category[0]\n",
    "        \n",
    "    \n",
    "    category_tag = soup.find('meta', attrs = {'property':'article:section'})\n",
    "    if category_tag:\n",
    "        res_dict['news_category'] = str(category_tag['content'])\n",
    "        \n",
    "        \n",
    "    fb_app_tag = soup.find('meta', attrs = {'property':'fb:app_id'})\n",
    "    if fb_app_tag:\n",
    "        res_dict['news_fb_app_id'] = str(fb_app_tag['content'])\n",
    "\n",
    "    fb_page_tag = soup.find('meta', attrs = {'property':'fb:pages'})\n",
    "    if fb_page_tag:\n",
    "        res_dict['news_fb_page'] = str(fb_page_tag['content'])\n",
    "\n",
    "    #Optional\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'news_keywords'})\n",
    "    if keywords_tag:\n",
    "        res_dict['news_keywords'] = keywords_tag['content']\n",
    "\n",
    "    description_tag = soup.find('meta', attrs = {'name': 'description'})\n",
    "    if description_tag:\n",
    "        res_dict['news_description'] = description_tag['content']\n",
    "\n",
    "    time_tag = soup.find('meta', attrs = {'property': 'article:published_time'})\n",
    "\n",
    "    if time_tag:\n",
    "        try:\n",
    "            d1 = datetime.datetime.strptime(time_tag.get('content'), \"%Y-%m-%dT%H:%M:%S+08:00\") \n",
    "            d1 -= datetime.timedelta(hours=8)\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            date_res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = date_res\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(time_tag.get('content'), \"%Y-%m-%dT%H:%M:%SZ\") \n",
    "                d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e2:\n",
    "                print(e2)\n",
    "                content_parser.logger.info('Epoch date error {}'.format(e2))\n",
    "\n",
    "    article_body_tag = soup.find('div', attrs = {'itemprop':'articleBody'})\n",
    "    #print(article_body_tag)\n",
    "    temp_content = []\n",
    "    links = []\n",
    "    links_descs = []\n",
    "    if article_body_tag:\n",
    "        p_tags = article_body_tag.find_all('p', attrs = {'class': None})\n",
    "        a_tags = article_body_tag.find_all('a')\n",
    "        if p_tags:\n",
    "            \n",
    "            for p in p_tags:\n",
    "                if p.find('script', attrs = {'type': 'text/javascript'}):\n",
    "                    continue\n",
    "                temp_content.append(p.get_text().strip())\n",
    "                \n",
    "        if len(a_tags):\n",
    "            for a in a_tags:\n",
    "                if len(a):\n",
    "                    if a['href'] == '#':\n",
    "                        continue\n",
    "                    if a.get_text().strip() and 'www' in a['href']:\n",
    "                        links.append(a['href'])\n",
    "                        links_descs.append(a.get_text().strip())\n",
    "            res_dict['news_related_url'] = links\n",
    "            res_dict['news_related_url_desc'] = links_descs\n",
    "    content = '\\n'.join(temp_content).strip()\n",
    "    if content:\n",
    "        res_dict['news'] = content\n",
    "\n",
    "    if not res_dict or 'news' not in res_dict:\n",
    "        return\n",
    "        content_parser.logger.error('Epoch url: {} did not process properly'.format(url))\n",
    "\n",
    "    return res_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News_title: 「政審」又來了！中國人答不答應？\n",
      "News_category: 各地分網\n",
      "News_fb_app_id: 1571885662850711\n",
      "News_fb_page: 156786811025453\n",
      "News_keywords: 政審,文革,熱點互動,大紀元\n",
      "News_description: 中共的「政審」為何又回來了？中共建政以來是如何用「政審」來箝制民眾的？高校為何越來越成為中共進一步嚴控思想和言論的重地？ 今晚美東時間9:30，新唐人電視台熱點互動直播節目將邀請專家深度解析，歡迎觀眾朋友們屆時收看並參與討論。\n",
      "News_published_date: 2018-11-14 19:37:49\n",
      "News_related_url: ['https://www.epochtimes.com/b5/tag/%e6%94%bf%e5%af%a9.html', 'https://www.epochtimes.com/b5/tag/%e6%96%87%e9%9d%a9.html', 'https://www.epochtimes.com/b5/tag/%e6%94%bf%e5%af%a9.html', 'https://www.epochtimes.com/b5/tag/%e7%86%b1%e9%bb%9e%e4%ba%92%e5%8b%95.html']\n",
      "News_related_url_desc: ['政審', '文革', '政審', '熱點互動']\n",
      "News: 【大紀元2018年11月15日訊】日前，重慶、福建接連發布消息，將對2019年高考生進行「政審」，有反對「四項基本原則」、「反對憲法言行」者不能參加考試。消息引發輿論強烈反響，被批向文革倒退。網絡更是瘋傳一篇檄文，斥責中共官員不知道自己是誰。另一方面，廣西高校日前又傳出清查全校4萬多師生手機電腦內容的事件。\n",
      "中共的「政審」為何又回來了？中共建政以來是如何用「政審」來箝制民眾的？高校為何越來越成為中共進一步嚴控思想和言論的重地？ 今晚美東時間9:30，新唐人電視台熱點互動直播節目將邀請專家深度解析，歡迎觀眾朋友們屆時收看並參與討論。\n",
      "美東時間：週三（11月14日）晚9點半到10點10分\n",
      "北京時間：週四（11月15日）早9點半到10點10分\n",
      "歡迎在節目直播期間發手機簡訊即時發送您的評論或提問，與我們文字互動。\n",
      "手機簡訊評論或提問：347-903-8806\n",
      "熱線電話：+1-646-519-2879\n",
      "YouTube直播：請訂閱（subscribe）NTDChinaNews頻道\n",
      "直播鏈接：https://www.youtube.com/c/NTDChinaNewsChannel/live\n",
      "網絡同步收看：www.ntdtv.com進入中文網頁，點擊上方「直播」，即可收看。\n",
      "電話同步收聽：撥打+1-832-551-5015隨時收聽新唐人電視台24小時即時節目。\n",
      "中國大陸的觀眾可以通過愛博電視（iPPOTV）直接收看，無需翻牆軟件。\n",
      "愛博電視獲取方式：\n",
      "（1）通過動態網、無界等翻牆軟件登錄www.ippotv.com下載；\n",
      "（2）或用海外電子郵箱，如gmail或hotmail給ippotv2011@gmail.com發一封電子郵件，郵件主題為「1234」，即可收到最新版本的愛博電視。\n",
      "Apple、Android手機或平板電腦用戶可通過Apple App Store或Google Play Store下載iNTD應用軟體。\n",
      "責任編輯：李天琦\n",
      "\n",
      "\n",
      "\n",
      "News_title: 法國兩大巨星睽違30年同台飆戲 向友誼致敬\n",
      "News_category: 各地分網\n",
      "News_fb_app_id: 1571885662850711\n",
      "News_fb_page: 156786811025453\n",
      "News_keywords: 友誼,癌症,陪你玩最大,大紀元\n",
      "News_description: 劇情敘述，學生時代起，亞瑟跟凱薩就是患難與共的摯友，因為一場陰錯陽差的誤會，凱薩以為亞瑟得了絕症，只剩下最後幾個月的生命，因此他不顧反對硬搬進亞瑟家，想陪好友完成所有人生心願，把剩下的每一天都活出最瘋狂的色彩\n",
      "News_published_date: 2020-05-01 07:49:26\n",
      "News_related_url: ['https://www.epochtimes.com/b5/tag/%e9%99%aa%e4%bd%a0%e7%8e%a9%e6%9c%80%e5%a4%a7.html', 'https://www.epochtimes.com/b5/tag/%e7%99%8c%e7%97%87.html', 'https://www.epochtimes.com/b5/tag/%e5%8f%8b%e8%aa%bc.html', 'https://www.epochtimes.com/b5/tag/%e7%99%8c%e7%97%87.html', 'https://www.epochtimes.com/b5/tag/%e9%99%aa%e4%bd%a0%e7%8e%a9%e6%9c%80%e5%a4%a7.html']\n",
      "News_related_url_desc: ['陪你玩最大', '癌症', '友誼', '癌症', '陪你玩最大']\n",
      "News: 【大紀元2020年05月01日訊】法國電影《陪你玩最大》描述一對個性迥異的好友，因為一次醫療誤會，讓得了癌症的凱薩誤以為是好友亞瑟得了絕症，因此他拚命想陪亞瑟將剩下的生命活出瘋狂色彩，而凱薩的積極也讓亞瑟遲遲說不出口事情的真相。\n",
      "本片由威尼斯影帝法布萊斯魯奇尼（Fabrice Luchini）與法國巨星派屈克布瑞爾（Patrick Bruel）主演。\n",
      "兩位巨星在電影中飾演一對超級好友，巧合的是，現實生活中，兩位演員也是好朋友，但這次的合作睽違了30年才再度於大銀幕同台，對此，法布萊斯除了興奮外，亦表示希望藉由這部片，向所有友誼致敬。\n",
      "他說：「因為我們本來就是好朋友，所以拍戲時，常不由自主陷進角色中無法自拔，幸好現實中我們都健康，也希望大家看完電影，立刻高聲對好友表達愛，如果好友就在身邊，記得給彼此一個大大擁抱。」\n",
      "而電影中的真摯情誼竟與執導本片的兩位導演亞歷山大德拉派特利耶 （Alexandre de La Patellière）與馬修戴拉波特 （Matthieu Delaporte）的真實經驗有關，導演馬修說：「在發想劇本的時候，我發現自己的身體狀況似乎有癌症的病狀，我非常擔心、也不敢告訴家人，還好亞歷山大發現我的不對勁，並陪我去醫院檢查，結果我真的得了癌症，但幸好有及早發現，不然就來不及治療了，而這樣的經驗也催生了《陪你玩最大》的劇本，希望藉由我們的故事傳達正面溫暖的力量。」\n",
      "劇情敘述，學生時代起，亞瑟跟凱薩就是患難與共的摯友，因為一場陰錯陽差的誤會，凱薩以為亞瑟得了絕症，只剩下最後幾個月的生命，因此他不顧反對硬搬進亞瑟家，想陪好友完成所有人生心願，把剩下的每一天都活出最瘋狂的色彩……\n",
      "《陪你玩最大》台灣5月8日上映。\n",
      "責任編輯：蘇漾\n",
      "\n",
      "\n",
      "\n",
      "News_title: F-22猛禽戰機武器獲升級 可應對大國戰爭\n",
      "News_category: 北美新聞\n",
      "News_fb_app_id: 1571885662850711\n",
      "News_fb_page: 156786811025453\n",
      "News_keywords: F-22戰機,隱形戰機,大紀元\n",
      "News_description: 新集成的美國空軍F-22猛禽（Raptor）戰機武器現已投入使用，伴隨多年來的軟件升級，這個新武器系統為F-22隱形戰機提供擴展的空對空和空對地攻擊技術，使這款戰機在大國戰爭中更具優勢。\n",
      "News_published_date: 2020-06-07 04:56:37\n",
      "News_related_url: ['https://www.epochtimes.com/b5/tag/%e9%9a%b1%e5%bd%a2%e6%88%b0%e6%a9%9f.html', 'https://www.epochtimes.com/b5/tag/%e9%9a%b1%e5%bd%a2%e6%88%b0%e6%a9%9f.html', 'https://www.epochtimes.com/b5/tag/f-22%e6%88%b0%e6%a9%9f.html']\n",
      "News_related_url_desc: ['隱形戰機', '隱形戰機', 'F-22戰機']\n",
      "News: 【大紀元2020年06月07日訊】（大紀元記者許家琳綜合報導）新集成的美國空軍F-22猛禽（Raptor）戰機武器現已投入使用，伴隨多年來的軟件升級，這個新武器系統為F-22隱形戰機提供擴展的空對空和空對地攻擊技術，使這款戰機在大國戰爭中更具優勢。\n",
      "綜合福克斯新聞等媒體近期報導，這次武器升級稱為增量3.2B（Increment 3.2B），包括AIM-120D導彈的高級變體和AIM-9X響尾蛇空對空導彈，並帶來了改進的對地打擊技術。進行了多年的軟件升級現在已經準備好參戰。\n",
      "雷神公司的AIM-9X武器研發人員表示，Block 2變體增加了重新設計的引信和數字點火安全裝置，加強了地面操作和飛行中的安全。雷神公司一份聲明說，Block 2還擁有升級版電子技術，能夠進行重大改進，包括使用新的武器數據鏈來支持發射後鎖定功能，可超視距交戰。\n",
      "武器升級的另一部分包括對F-22進行工程設計，以發射AIM-120D，這是一種超視距先進中程空對空導彈（AMRAAM），專為全天候晝夜攻擊而設計；雷神公司的數據顯示，這是一種「自主導引」導彈，具有主動雷達制導功能。\n",
      "F-22製造商洛克希德·馬丁公司發言人約翰·洛辛格（John Losinger）告訴「勇士專家」網站，這次升級主要側重於提高武器和雷達效能的軟件，硬件幾乎沒有改變。\n",
      "雖然基於安全因素，與F-22武器增強相關的一些技術細節尚不完全清楚，但雷神公司開發人員表示，制導能力增強、精準瞄準和耐用性擴大了F-22的戰術任務範圍。\n",
      "F-22已在歐洲和其它戰略地區進行了前沿部署，如果F-22能夠以更高精度進行攻擊，自然會帶來一種新能力，使敵機處於危險之中。\n",
      "F-22武器補充還包括針對地面的攻擊武器，包括精確制導炸彈，例如名為GBU-32和GBU-39的聯合直接攻擊制導武器，以及小直徑炸彈。\n",
      "F-22戰鬥機一直以「超級巡航」能力而著稱，而更精準的F-22發射武器將使這款戰機更好利用這種技術。該技術通過允許戰鬥機維持超音速而無需打開後燃器，這使得F-22能以更少的燃料飛得更快、更遠，從而增加執行作戰任務的時間和範圍。\n",
      "這種能力歸因於F-22的發動機推力和空氣動力配置。這將為攻擊任務留出更多停留時間，使飛行員能夠更好搜索並確定特定目標。\n",
      "空軍的3.2B武器升級是F-22猛禽新「敏捷軟件開發」戰略的一部分，該戰略旨在為隱型戰鬥機快速配備新傳感器、改進的雷達和航空電子設備、更快的計算機處理器，以及大大增強的武器技術。\n",
      "改進版F-22武器是更廣泛空襲戰略的一部分，該戰略旨在幫助美國空軍對付諸如俄羅斯Su-57隱形戰鬥機之類的近似匹敵（near–peer）競爭平台，尋求關鍵空對空優勢。2014年，F-22猛禽對阿富汗的塔利班設施成功進行了地面攻擊。\n",
      "為了針對作戰狀態做準備，空軍武器開發人員已經在埃格林（Eglin）、內利斯（Nellis）、希爾（Hill）和廷德爾（Tyndall）空軍基地，通過特定的「射擊」演習，對3.2升級進行了測試。\n",
      "洛克希德-馬丁公司負責F-22項目的副總裁肯·麥錢特曾表示：「我們的目的是確保我們能保持首先看到，首先開火，首先致地方於死地。」\n",
      "\n",
      "責任編輯：李寰宇\n",
      "\n",
      "\n",
      "\n",
      "News_title: 美麗的伐伊\n",
      "News_category: 副刊\n",
      "News_fb_app_id: 1571885662850711\n",
      "News_fb_page: 156786811025453\n",
      "News_description: 老村長告訴我，她的婆婆是一個非常善良的長輩，在以前大家忙著在外工作時，伐伊不但是收自家的衣服，也會收全部落的衣服，並且還放到雨水滴不到的地方……\n",
      "News_published_date: 2020-06-06 04:30:17\n",
      "News_related_url: ['https://www.epochtimes.com/b5/tag/%e7%a6%b9%e6%b5%b7.html']\n",
      "News_related_url_desc: ['禹海']\n",
      "News: 老村長告訴我，她的婆婆是一個非常善良的長輩，在以前大家忙著在外工作時，伐伊不但是收自家的衣服，也會收全部落的衣服，並且還放到雨水滴不到的地方……\n",
      "這一天，我靜靜的在院落裡寫一些字，忽聞步屨聲打從前面過，抬起眼，披著圍巾著了碎花深色外衣的伐伊身影剛好走過。\n",
      "有好一陣子，自己就像林間松鼠一樣奔來躍去，難能以沉潛心來寫作業。早上醒睡時分，想做的恰就是要記述一些有關伐伊的故事。\n",
      "伐伊（Fa I）是阿里山鄒族語意裡的祖母，一般上了年紀的婦人，小孩或族人都會這麼稱呼她們。\n",
      "山中空氣清新，自己目前所處的伽雅瑪茶山村位於山腰間，不時可見一些長者揹著他們的「雍古」（籃子）於田林間工作，這種籃子是以藤編而成，上寬下窄，普見於阿里山的原住民村落。我見過伐伊的雍古裡，曾有她老人家自種的菜蔬和撿拾得來的木柴。\n",
      "近些時山上下起了雨，氣溫隨著也降了許多，每人都多著了一件外衣。昨兒個晚上，我行經伐伊的亭子，見幽闇的亭內升起了火，那紅光在夜裡甚是好看，不由就引我信步行去，走近時聽見伐伊與人談話的聲音。\n",
      "印象裡的伐伊極為沉靜甚少開口說話，因此我還趨前禮貌的打了個招呼。那時亭內坐著另個伐伊，兩個年歲加起來有一百五十好幾的老人家就分坐在爐火邊的小凳椅上，有一搭沒一搭的說著族語，閒話家常的兩個伐伊身影在火光的投射中，猶如讓人感處於另一時空。\n",
      "我與伐伊的關係沒有很近，只緣來茶山時都住在她大兒子與媳婦家，較諸一般外地遊客熟稔，於而見面時都會打招呼，當然這招呼都是我先說，她老人家只是微微笑著接納。\n",
      "伐伊對我來說，等同像是自己的祖母。\n",
      "與伐伊打過幾次招呼後，她家人才告訴我，伐伊有些耳聾，跟她講話要大聲一點，不過我始終沒有很大聲的跟她說話。\n",
      "有一回，她們家族於院落中聚會，要做默禱時，就請輩分最長年紀最大的伐伊講話，只見每人都擁到她老人家耳旁拉高分貝，伐伊方如回神般的開始說禱詞。\n",
      "老人家的腦海，有著外人不悉的世界，或許伐伊當時是從兒孫身上馳想到了自己當年移民拓墾的情景。\n",
      "我拍過伐伊一些圖片，有一回是心血來潮的記述了她怎麼從田裡走回家裡。那時她方從田間工做完，背籃裡放了一些菜蔬，伐伊沉靜的以額頭負起雍古肩帶，然後就以常日沉穩的步伐默默走在舖著柏油的部落路面，我始終也沉默地保持一個距離在老人家的後方，看著她的背影走上家門前的小坡方而折回。\n",
      "除了田間的工作，伐伊還勤勞的照料她飼的雞，幾乎每天都會到雞舍轉上一圈，那情景就如部落裡的小孩每日都會徒步到學校一樣。我曾攝得一張伐伊凝望小雞的神情，眼裡盡是慈祥，彷彿那些小雞就像她的小孩。\n",
      "有一天我打掃院落，順便將新盥洗室施工時所滴落的水泥渣及馬桶週遭洗擦，這個工作讓我做得汗流夾背，待沐浴時聽聞瓦片上似有滴水聲，浴罷門外草地是濕漉一片，雨珠子還在草尖上打圈，原本晾在院中繩上的衣服，來餵雞的伐伊都已掛於簷下了。\n",
      "後來伐伊的媳婦，也就是之前的老村長告訴我，她的婆婆是一個非常善良的長輩，在以前大家忙著在外工作時，伐伊不但是收自家的衣服，還會收全部落的衣服，並且還放到雨水滴不到的地方，她自己非常慶幸有著這麼一位好婆婆。\n",
      "我重返茶山時，有一天在院子曬被，順手就持殘餚餵狗，不知何時來的伐伊忽然走到我跟前，向我說了一串話，記憶裡這是從來未有的事，一時間頗讓我有種受寵的感覺。伐伊的連串鄒語我難能了解，只能適時的應上一兩句，然而心底卻像是領到糖果的小孩。\n",
      "前兩天我在亭裡做筆記，看見伐伊行走過來，於而進屋室取了一包餅乾，待伐伊餵好雞回經亭子時，我隨而走到她身旁，喊了她一聲，同時就將那包餅乾放入她的口袋裡，伐伊見我這般帶著淘氣的動作，不由笑了起來，那笑容又慈祥又好看。\n",
      "部落裡不定時會有一些小販上來兜賣一些民生用品，一日來個賣農具的，他在路邊前後放了兩個撐架，上頭再置擺一塊長木板，就列一堆鋤頭柴刀。伐伊行經時，就撿了一把柴刀，試捻它的重量及力道，回頭看見了我，自然的露出一抹淺笑，就比著她要砍柴的動作，那情景猶如孩童般可愛，而那笑容似若清晨初露山頭的陽光般掬人，而也在那片際，我按下了快門，於是美麗的伐伊就永遠停格在我心中。＠\n",
      "責任編輯：王堇\n",
      "\n",
      "\n",
      "\n",
      "News_title: 排除堆積毒素 睡眠讓免疫系統變得更強健\n",
      "News_category: Inspired\n",
      "News_fb_app_id: 1571885662850711\n",
      "News_fb_page: 156786811025453\n",
      "News_keywords: 保健,瘦身,睡眠,美容,美體,大紀元\n",
      "News_description: 當你在睡眠時，膠淋巴系統清除β- 類澱粉蛋白（也就是那種可能導致阿茲海默症的蛋白質）的速度比起你在清醒時還要快。\n",
      "News_published_date: 2020-06-01 13:52:16\n",
      "News_related_url: ['https://www.epochtimes.com/b5/tag/%e7%9d%a1%e7%9c%a0.html', 'https://www.epochtimes.com/b5/tag/%e7%9d%a1%e7%9c%a0.html', 'https://www.books.com.tw/products/0010859125', 'https://www.epochtimes.com/b5/20/5/29/n12146644.htm', 'https://www.epochtimes.com/b5/20/5/29/n12146573.htm']\n",
      "News_related_url_desc: ['睡眠', '睡眠', '抗老聖經：哈佛醫師的七週療程，優化基因表現、預防疾病，讓妳控制體重，重返青春', '女性運動可能逆轉肌膚老化 好眠 帶來安適感', '女性不愛運動的十大理由']\n",
      "News: 【大紀元2020年06月02日訊】你的大腦在晚上睡覺時會歷經一場沐浴時光，移除那些會導致神經退化（基本上就是大腦衰變）的有害及有毒分子。\n",
      "大腦沐浴是這樣進行的：睡眠時，大腦細胞之間的空隙會比清醒時擴張60%，這使得大腦能夠用腦脊液（CSF），也就是大腦和脊椎周圍的透明液體，來排除那些堆積的毒素，這稱為膠淋巴系統。\n",
      "一項研究顯示，當你在睡眠時，膠淋巴系統清除β- 類澱粉蛋白（也就是那種可能導致阿茲海默症的蛋白質）的速度比起你在清醒時還要快。你的膠淋巴系統在你側睡時效果最佳，而非仰睡或趴睡。我會在我彎曲的雙腿之間放一個枕頭，讓自己保持側躺的姿勢，以便讓我的大腦好好地沐浴一下，同時幫助我的下背部紓壓。\n",
      "藍光的邪惡面\n",
      "聽到這些應該就足以讓你想要改善睡眠了吧？然而，問題是現代人的生活—尤其是人工照明—經常會帶來阻礙。\n",
      "醫師兼名劇作家安東．契科夫（Anton Chekhov）有一句名言：「醫學是我的合法妻子，文學是我的情婦；當我厭倦一方的時候，我就會去和另一方過夜。」同樣地，我也喜歡在晚上九點左右，當孩子們都上床睡覺後（而且希望沒有在看手機！）蜷縮著身體在我的iPad 上看書。這樣好嗎？可能不太好。我的習慣讓我無法完整經歷睡眠的五大階段，同時也可能導致罹患癌症、糖尿病、心臟病和變胖的風險。\n",
      "維生素D 對於體內鈣質的有效運輸是很重要的，能夠幫助你維持骨骼強壯。我遺傳到一個不良的維生素D 受體基因，這表示我的身體無法良好地吸收和運輸維生素D，因此我血液中的維生素D 值通常很低，導致我快速骨質流失、骨量減少、骨質疏鬆症、多發性硬化症，以及某些惡性腫瘤像是大腸癌等方面有更高的罹患風險。\n",
      "當我在2006 年得知自己有維生素D 不足基因時，便開始攝取更大量的維生素D。美國國家醫學院（The Institute of Medicine）建議每天攝取600 IU 的維生素D，但如此低的劑量很可能導致我骨質流失的風險增加，同時讓我的維生素D 受體基因保持在關閉狀態。\n",
      "攝取較高劑量的維生素D（我每天攝取5,000 IU）能幫助我避免過度骨質流失和罹患骨質疏鬆症，或許還能避免罹患在老婦人常見的駝背。攝取更高的維生素D 劑量就是一種表觀遺傳改變，它讓我能夠將我的身體使用維生素D 的能力保持在啟動狀態。\n",
      "讓你的生理時鐘失調最快的方法就是用夜晚人工照明（ALAN）來傷害你的眼球。電子螢幕所散發的藍波長在白天沒有問題，因為它能增進你的注意力、反應時間以及心情，但在夕陽西下之後它就會背叛你。\n",
      "並非所有顏色都是一樣的，從電子螢幕散發出來的藍光，像是智慧手機或電子書，會比其他顏色的光更容易抑制褪黑激素的分泌，青春期的孩子尤其容易受影響。光線越強，對褪黑激素生成的影響就越大，即使只花一點點時間看電子螢幕也會造成睡眠不足的後果。\n",
      "螢幕並不是夜晚人工照明唯一的罪魁禍首。去年，我丈夫把我們家的每一個燈泡都換成比較環保的日光燈或LED 燈。總共一百多個花俏的節能燈泡，唉，雖然對環境有利，卻不見得對我們的健康有益。\n",
      "這些燈泡雖然能夠節能，卻比傳統白熾燈散發出更大量的藍光。慘了！所以，如果你家有個環保人士，或者你是個夜貓子、從事輪班性質的工作，或是一位太空人，請花點錢去購買能夠阻絕藍光的室內玻璃，並且確保你在上床前至少一小時不要再看那些螢幕了。\n",
      "第二週療程：睡眠\n",
      "以下是第二週療程的範本計劃。接下來的七天（以及整個療程直到結束），請盡可能遵照這些指導方針。你應該會開始注意到，你在情緒和身體方面都漸漸感到神清氣爽，而且面對壓力也能處理得更好。在幾週內，你的免疫系統將會變得更強健，而你感染一般疾病的機率也會越來越低。\n",
      "請先開始進行精神運動警覺性任務檢測來評估你的睡眠債務。這是一個很簡單的測試，你可以在電腦或手機上進行。\n",
      "基本流程\n",
      "• 臥室裡不要放置電子產品；如果無法做到的話，在就寢前至少一小時，將它們放置在距離身體至少五英尺遠的地方。這樣做能夠改善你睡眠的質和量。\n",
      "四十歲和以上的女性：\n",
      "• 夜晚房間內的溫度保持在攝氏18 度以下。\n",
      "將溫度干擾減少到最低。夏天要這樣做可能不容易而且花費高昂—盡力而為就好。\n",
      "• 解決熱潮紅和夜間盜汗問題。\n",
      "你可能需要考慮短期使用生物同質性賀爾蒙療法來改善睡眠。特別是天然黃體素，100 至200 毫克，有助於改善近更年期和更年期婦女的睡眠。\n",
      "• 避開刺激物。\n",
      "不要接觸咖啡因和焦慮人士，兩者都會過度刺激你的神經系統，讓你難以入眠。是的，我就是在建議你減少和有焦慮症的人士接觸，尤其如果你本身是個極度敏感或善解人意的人。你是你身邊五個人的平均（譯註：五人平均值理論），讓他們都保持在放鬆狀態，如此一來你就不用在上床睡覺時還要糾結於如何取悅他們。\n",
      "• 在早晨運動，或至少在中午一點前。\n",
      "如果你不得不更晚運動的話，請注意你的睡眠時間長度和品質是否會因此下降，然後進行調整。\n",
      "• 創造一個有利於睡眠的環境，幽暗、安靜、舒適、涼爽。你的房間應該要暗到伸手不見五指的程度。\n",
      "• 如果你前一晚未睡滿七小時或是感到疲倦，請花至少二十分鐘的時間小睡片刻。本週至少一次，花二十至三十分鐘時間小睡。\n",
      "• 請注意你是否每晚睡眠超過八個半小時。必要時可以設定鬧鐘。\n",
      "• 讓自己白天時暴露在大量的明亮光線下。這將有助於改善你在夜晚入睡的能力，以及你在白天的心情和警覺性。\n",
      "• 讓自己做個小小的大休息（savasana，瑜珈體式）。\n",
      "大休息，又稱癱屍式，指的是躺在地上，掌心向上，雙腿與臀同寬，完全放鬆全身每一道肌肉的姿勢。這個姿勢能夠幫助我徹底放鬆，不僅是在上完一堂困難的瑜珈課之後，在白天也一樣。我的身體對於日常生活的喧囂容易產生劇烈反應，因此，讓自己做二到五分鐘小小的大休息，讓我能\n",
      "夠擺脫一天的煩憂。\n",
      "• 在十點前上床，至少比平常提早半小時就寢，睡滿七至八個半小時。\n",
      "一週七天，每天晚上同一時間就寢，每天在同一時間起床，即使週末也一樣。大睡補眠是沒有用的，因為你會失去正常的睡眠架構，睡眠品質也會受損。試著在本週讓每晚的睡眠架構維持一模一樣。你的目標是盡可能在十點前上床，準備睡覺，不要和你的伴侶聊天或回覆臨時收到的電子郵件。\n",
      "重新設定你的睡醒週期。\n",
      "• 化合物不是杯子蛋糕，而是藜麥、甘藷以及木薯，這些都會消化得比較慢，而且不會過度讓血糖升高。就寢前三小時請勿進食，所以晚上七點以後就不能吃東西。在晚餐時攝取碳水化合物能有助於啟動減重基因，包括那些為瘦素、飢餓肽和脂聯素編碼的基因。\n",
      "• 在就寢前三小時，對大多數人而言也就是晚上七點，限制使用電子產品螢幕以及夜晚人工照明。至少在就寢前一小時關閉電子產品，像是電視、電腦、手機和平板等。這表示不能在臥室裡看電視或是邊看電視邊睡著。\n",
      "• 固定養成一套就寢前放鬆的流程，像是用浴鹽泡個熱水澡或聽舒心的音樂；在你打算入睡前至少一個小時開始。＠\n",
      "<本文摘自抗老聖經：哈佛醫師的七週療程，優化基因表現、預防疾病，讓妳控制體重，重返青春，高寶書版提供>\n",
      "•女性運動可能逆轉肌膚老化 好眠 帶來安適感\n",
      "•女性不愛運動的十大理由\n",
      "責任編輯：曾晏均\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_util(epoch_test_urls, epoch_content_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 隨機選出50個經濟日報的新聞連結：\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '經濟日報' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udn_test_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '經濟日報' ORDER BY RAND()\n",
    "LIMIT 50; \"\"\"\n",
    "udn_test_urls = query_from_db(udn_test_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udn_content_processor(url):\n",
    "    res_dict = {}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding='utf-8'\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "    if not soup:\n",
    "        return \n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        title_category_lst = title_tag.string.split(' | ', 3)\n",
    "        res_dict['news_title'] = title_category_lst[0]\n",
    "        try:\n",
    "            res_dict['news_category'] = title_category_lst[2]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "    fb_app_tag = soup.find('meta', attrs = {'property':'fb:app_id'})\n",
    "    if fb_app_tag:\n",
    "        res_dict['news_fb_app_id'] = str(fb_app_tag['content'])\n",
    "\n",
    "    fb_page_tag = soup.find('meta', attrs = {'property':'fb:pages'})\n",
    "    if fb_page_tag:\n",
    "        res_dict['news_fb_page'] = str(fb_page_tag['content'])\n",
    "\n",
    "    #Optional\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'news_keywords'})\n",
    "    if keywords_tag:\n",
    "        res_dict['news_keywords'] = keywords_tag['content']\n",
    "\n",
    "    description_tag = soup.find('meta', attrs = {'name': 'description'})\n",
    "    if description_tag:\n",
    "        res_dict['news_description'] = description_tag['content']\n",
    "\n",
    "    time_tag = soup.find('div', attrs = {'class': 'shareBar__info--author'})\n",
    "\n",
    "    if time_tag:\n",
    "        try:\n",
    "\n",
    "            d1 = datetime.datetime.strptime(time_tag.find('span').text, \"%Y-%m-%d %H:%M\")\n",
    "            print(d1)\n",
    "            d1 -= datetime.timedelta(hours=8)\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            date_res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = date_res\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(time_tag.get('content'), \"%Y-%m-%d %H:%M:%S\") \n",
    "                d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e2:\n",
    "                print(e2)\n",
    "                content_parser.logger.info('Epoch date error {}'.format(e2))\n",
    "    article_body_tag = soup.find('div', attrs = {'id':'article_body'})\n",
    "    content_temp, links, links_descs = [], [], []\n",
    "    if article_body_tag:\n",
    "        p_tags = article_body_tag.find_all('p', attrs = {'class': None})\n",
    "        a_tags = article_body_tag.find_all('a')\n",
    "        if p_tags:\n",
    "            for p in p_tags:\n",
    "                if p.get_text().strip():\n",
    "                    content_temp.append(p.get_text().strip())\n",
    "        if len(a_tags):\n",
    "            for a in a_tags:\n",
    "                if len(a):\n",
    "                    if a['href'] == '#':\n",
    "                        continue\n",
    "                    if a.get_text().strip() and 'www' in a['href']:\n",
    "                        links.append(a['href'])\n",
    "                        links_descs.append(a.get_text().strip())\n",
    "            res_dict['news_related_url'] = links\n",
    "            res_dict['news_related_url_desc'] = links_descs\n",
    "                \n",
    "    content = '\\n'.join(content_temp).strip()\n",
    "    if content:\n",
    "        res_dict['news'] = content\n",
    "\n",
    "    if not res_dict or 'news' not in res_dict:\n",
    "        return\n",
    "        content_parser.logger.error('Epoch url: {} did not process properly'.format(url))\n",
    "\n",
    "    return res_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_util(udn_test_urls, udn_content_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 隨機選出50個新浪台灣新聞中心的新聞連結： (skip this phase, too many error urls)\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '新浪台灣新聞中心' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sina_test_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '新浪台灣新聞中心' ORDER BY RAND()\n",
    "LIMIT 50; \"\"\"\n",
    "sina_test_urls = query_from_db(sina_test_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sina_content_processor(url):\n",
    "    res_dict = {}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding='utf-8'\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "    if not soup:\n",
    "        return \n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        title_category = title_tag.string.rsplit('-', 2)\n",
    "        res_dict['news_title'] = title_category[0].strip()\n",
    "        if len(title_category) > 1:\n",
    "            res_dict['news_category'] = title_category[-1].strip()\n",
    "    \n",
    "    \n",
    "    # Sina did not have FB available\n",
    "    #fb_app_tag = soup.find('meta', attrs = {'property':'fb:app_id'})\n",
    "    #if fb_app_tag:\n",
    "    #    res_dict['news_fb_app_id'] = str(fb_app_tag['content'])\n",
    "    \n",
    "    #fb_page_tag = soup.find('meta', attrs = {'property':'fb:pages'})\n",
    "    #if fb_page_tag:\n",
    "    #    res_dict['news_fb_page'] = str(fb_page_tag['content'])\n",
    "\n",
    "    #Optional\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    if keywords_tag:\n",
    "        res_dict['news_keywords'] = keywords_tag['content']\n",
    "\n",
    "    description_tag = soup.find('meta', attrs = {'name': 'description'})\n",
    "    if description_tag:\n",
    "        res_dict['news_description'] = description_tag['content']\n",
    "        \n",
    "    time_tag = soup.find('meta', attrs = {'name': 'article:published_time'})\n",
    "\n",
    "    if time_tag:\n",
    "        try:\n",
    "            d1 = datetime.datetime.strptime(time_tag.get('content'), \"%Y-%m-%dT%H:%M:%S+08:00\") \n",
    "            d1 -= datetime.timedelta(hours=8)\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            date_res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = date_res\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(time_tag.get('content'), \"%Y-%m-%dT%H:%M:%SZ\") \n",
    "                d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e2:\n",
    "                print(e2)\n",
    "                content_parser.logger.info('Sina date error {}, URL: {}'.format(e2, url))\n",
    "\n",
    "    article_body_tag = soup.find('div', attrs = {'id':'article_content'})\n",
    "    if article_body_tag:\n",
    "        p1_tags = article_body_tag.find_all('p')\n",
    "        if p1_tags:\n",
    "            temp = []\n",
    "            for p in p1_tags:\n",
    "                if p.text:\n",
    "                    temp.append(p.text.strip())\n",
    "            content = '\\n'.join(temp).strip()\n",
    "            if content:\n",
    "                res_dict['news'] = content\n",
    "\n",
    "\n",
    "    if not res_dict or 'news' not in res_dict:\n",
    "        return\n",
    "        content_parser.logger.error('Sina url: {} did not process properly'.format(url))\n",
    "\n",
    "    return res_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_util(sina_test_urls, sina_content_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 隨機選出50個PCHOME的新聞連結：\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = 'PCHOME' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pchome_test_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = 'PCHOME' ORDER BY RAND()\n",
    "LIMIT 50; \"\"\"\n",
    "pchome_test_urls = query_from_db(pchome_test_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pchome_content_processor(url):\n",
    "    res_dict = {}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding='utf-8'\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "    if not soup:\n",
    "        return\n",
    "    for br in soup.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        title_category = title_tag.string.rsplit('-', 2)\n",
    "        res_dict['news_title'] = title_category[0].strip()\n",
    "        if len(title_category) > 1:\n",
    "            res_dict['news_category'] = title_category[-2].strip()\n",
    "            \n",
    "    fb_app_tag = soup.find('meta', attrs = {'property':'fb:app_id'})\n",
    "    if fb_app_tag:\n",
    "        res_dict['news_fb_app_id'] = str(fb_app_tag['content'])\n",
    "    \n",
    "    fb_page_tag = soup.find('meta', attrs = {'property':'fb:pages'})\n",
    "    if fb_page_tag:\n",
    "        res_dict['news_fb_page'] = str(fb_page_tag['content'])\n",
    "\n",
    "    #Optional\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    if keywords_tag:\n",
    "        res_dict['news_keywords'] = keywords_tag['content']\n",
    "\n",
    "    description_tag = soup.find('meta', attrs = {'name': 'description'})\n",
    "    if description_tag:\n",
    "        res_dict['news_description'] = description_tag['content']\n",
    "\n",
    "    time_tag = soup.find('time')\n",
    "\n",
    "    if time_tag:\n",
    "        try:\n",
    "            d1 = datetime.datetime.strptime(time_tag.get('pubdate'), \"%Y-%m-%d %H:%M:%S\") \n",
    "            d1 -= datetime.timedelta(hours=8)\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            date_res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = date_res\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(time_tag.get('pubdate'), \"%Y-%m-%d %H:%M:%SZ\") \n",
    "                d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e2:\n",
    "                print(e2)\n",
    "                content_parser.logger.info('PChome date error {}, URL: {}'.format(e2, url))\n",
    "\n",
    "    article_body_tag = soup.find('div', attrs = {'calss':'article_text'})\n",
    "    if article_body_tag:\n",
    "        content = article_body_tag.text.strip()\n",
    "        a_tags = article_body_tag.find_all('a')\n",
    "        if content:\n",
    "            content = re.sub('(\\n)+', '\\n', content)\n",
    "            content = re.sub(r'(相關新聞[\\s\\S]+)', '', content)\n",
    "            res_dict['news'] = content \n",
    "            \n",
    "    if not res_dict or 'news' not in res_dict:\n",
    "        return\n",
    "        content_parser.logger.error('PChome url: {} did not process properly'.format(url))\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_util(pchome_test_urls, pchome_content_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 隨機選出50個Ettoday的新聞連結：\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = 'ETtoday' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ettoday_test_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = 'ETtoday' ORDER BY RAND()\n",
    "LIMIT 50; \"\"\"\n",
    "ettoday_test_urls = query_from_db(ettoday_test_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ettoday_content_processor(url):\n",
    "    res_dict = {}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding='utf-8'\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "    if not soup:\n",
    "        return \n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        title_category = title_tag.string.split(' | ')\n",
    "        res_dict['news_title'] = title_category[0]\n",
    "        if len(title_category) > 1:\n",
    "            res_dict['news_category'] = title_category[1].strip()\n",
    "    fb_app_tag = soup.find('meta', attrs = {'property':'fb:app_id'})\n",
    "    if fb_app_tag:\n",
    "        res_dict['news_fb_app_id'] = str(fb_app_tag['content'])\n",
    "    # Ettoday did not put fb page tag on the content\n",
    "    #fb_page_tag = soup.find('meta', attrs = {'property':'fb:admins'})\n",
    "    #if fb_page_tag:\n",
    "        #res_dict['news_fb_page'] = str(fb_page_tag['content'])\n",
    "\n",
    "    #Optional\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'news_keywords'})\n",
    "    if keywords_tag:\n",
    "        res_dict['news_keywords'] = keywords_tag['content']\n",
    "\n",
    "    description_tag = soup.find('meta', attrs = {'name': 'description'})\n",
    "    if description_tag:\n",
    "        res_dict['news_description'] = description_tag['content']\n",
    "\n",
    "    time_tag = soup.find('meta', attrs = {'property': 'article:published_time'})\n",
    "\n",
    "    if time_tag:\n",
    "        try:\n",
    "            d1 = datetime.datetime.strptime(time_tag.get('content'), \"%Y-%m-%dT%H:%M:%S+08:00\") \n",
    "            d1 -= datetime.timedelta(hours=8)\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            date_res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = date_res\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(time_tag.get('content'), \"%Y-%m-%dT%H:%M:%SZ\") \n",
    "                d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e2:\n",
    "                print(e2)\n",
    "                content_parser.logger.info('Ettoday date error {}, URL: {}'.format(e2, url))\n",
    "\n",
    "    article_body_tag = soup.find('div', attrs = {'itemprop':'articleBody'})\n",
    "    temp_content = []\n",
    "    links, links_descs = [], []\n",
    "    if article_body_tag:\n",
    "        p_tags = article_body_tag.find_all('p', attrs = {'class': None})\n",
    "        a_tags = article_body_tag.find_all('a')\n",
    "        \n",
    "        if p_tags:\n",
    "            for p in p_tags:\n",
    "                # Ignore the image caption\n",
    "                if p.find('strong'):\n",
    "                    continue\n",
    "                if p.text:\n",
    "                    temp_content.append(p.text.strip())\n",
    "        if len(a_tags):\n",
    "            for a in a_tags:\n",
    "                if len(a):\n",
    "                    if a['href'] == '#':\n",
    "                        continue\n",
    "                    if a.get_text().strip() and 'www' in a['href']:\n",
    "                        links.append(a['href'])\n",
    "                        links_descs.append(a.get_text().strip().replace('\\u3000', ' '))\n",
    "            res_dict['news_related_url'] = links\n",
    "            res_dict['news_related_url_desc'] = links_descs\n",
    "            \n",
    "    content = '\\n'.join(temp_content).strip()\n",
    "    if content:\n",
    "        res_dict['news'] = content\n",
    "\n",
    "    if not res_dict or 'news' not in res_dict:\n",
    "        return\n",
    "        content_parser.logger.error('Ettoday url: {} did not process properly'.format(url))\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_util(ettoday_test_urls, ettoday_content_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 隨機選出50個Rti 中央廣播電臺的新聞連結：\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = 'Rti 中央廣播電臺' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rti_test_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = 'Rti 中央廣播電臺' ORDER BY RAND()\n",
    "LIMIT 50; \"\"\"\n",
    "rti_test_urls = query_from_db(rti_test_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rti_content_processor(url):\n",
    "    res_dict = {}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "    reserved_keywords = ['Rti', '中央廣播電臺', 'Radio Taiwan International']\n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        title_category = title_tag.string.split(' - ', 1)\n",
    "        res_dict['news_title'] = title_category[0]\n",
    "    category_tag = soup.find('div', attrs = {'class':'swiper-wrapper'})\n",
    "    if category_tag:\n",
    "        \n",
    "        res_dict['news_category'] = category_tag.find('a', attrs = {'class':'active'}).get_text().strip()\n",
    "\n",
    "    fb_app_tag = soup.find('meta', attrs = {'property':'fb:app_id'})\n",
    "    if fb_app_tag:\n",
    "        res_dict['news_fb_app_id'] = str(fb_app_tag['content'])\n",
    "\n",
    "    #fb_page_tag = soup.find('meta', attrs = {'property':'fb:pages'})\n",
    "    #if fb_page_tag:\n",
    "    #    res_dict['news_fb_page'] = str(fb_page_tag['content'])\n",
    "\n",
    "    #Optional\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    if keywords_tag:\n",
    "        keyword_lst = [keyword for keyword in keywords_tag['content'].split(',') if keyword not in reserved_keywords]\n",
    "        res_dict['news_keywords'] = ','.join(keyword_lst)\n",
    "\n",
    "    description_tags = soup.find_all('meta', attrs = {'name': 'description'})\n",
    "    if description_tags:\n",
    "        res_dict['news_description'] = description_tags[1]['content']\n",
    "\n",
    "    time_tag = soup.find('li', attrs = {'class': 'date'})\n",
    "    if time_tag:\n",
    "        try:\n",
    "            d1 = datetime.datetime.strptime(time_tag.text, \"時間：%Y-%m-%d %H:%M\")\n",
    "            d1 -= datetime.timedelta(hours=8)\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            date_res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = date_res\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(time_tag.text, \"時間：%Y-%m-%d %H:%M:%S\")\n",
    "                d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e2:\n",
    "                content_parser.logger.info('RTI url: {} date not process properly, error message {}'.format(url, e2))\n",
    "\n",
    "    article_body_tag = soup.find('article', attrs = {'class' : None})\n",
    "    temp_content, links, links_descs = [], [], []\n",
    "    if article_body_tag:\n",
    "        p_tags = article_body_tag.find_all('p', attrs = {'class': None})\n",
    "        a_tags = article_body_tag.find_all('a')\n",
    "        if p_tags:\n",
    "            for p in p_tags:\n",
    "                if p.text:\n",
    "                    temp_content.append(p.text.strip())\n",
    "\n",
    "        if len(a_tags):\n",
    "            for a in a_tags:\n",
    "                if len(a):\n",
    "                    if a['href'] == '#':\n",
    "                        continue\n",
    "                    if a.get_text().strip() and 'www' in a['href']:\n",
    "                        links.append(a['href'])\n",
    "                        links_descs.append(a.get_text().strip())\n",
    "            res_dict['news_related_url'] = links\n",
    "            res_dict['news_related_url_desc'] = links_descs\n",
    "            \n",
    "    content = '\\n'.join(temp_content).strip()\n",
    "    if content:\n",
    "        res_dict['news'] = content\n",
    "    if not res_dict or 'news' not in res_dict:\n",
    "        return\n",
    "        content_parser.logger.error('RTI url: {} did not process properly'.format(url))\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_util(rti_test_urls, rti_content_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 隨機選出50個公視新聞網的新聞連結：\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '公視新聞網' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_test_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '公視新聞網' ORDER BY RAND()\n",
    "LIMIT 50; \"\"\"\n",
    "pts_test_urls = query_from_db(pts_test_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pts_content_processor(url):\n",
    "    res_dict = {}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding='utf-8'\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "\n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        title_category = title_tag.string.rsplit(' | ', 1)\n",
    "        res_dict['news_title'] = title_category[0].strip()\n",
    "            \n",
    "    fb_app_tag = soup.find('meta', attrs = {'property':'fb:app_id'})\n",
    "    if fb_app_tag:\n",
    "        res_dict['news_fb_app_id'] = str(fb_app_tag['content'])\n",
    "    \n",
    "\n",
    "    #Optional\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    if keywords_tag:\n",
    "        res_dict['news_keywords'] = keywords_tag['content']\n",
    "\n",
    "    description_tags = soup.find_all('meta', attrs = {'name': 'description'})\n",
    "    if len(description_tags) > 1:\n",
    "        res_dict['news_description'] = description_tags[1]['content']\n",
    "        \n",
    "    time_tag = soup.find('div', attrs = {'class': 'maintype-wapper hidden-lg hidden-md'})\n",
    "    if time_tag:\n",
    "        date_tag = time_tag.find('h2')\n",
    "        if date_tag:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(date_tag.text, \"%Y年%m月%d日\") \n",
    "                #d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e1:\n",
    "                print(e1)\n",
    "                content_parser.logger.info('PTS date error {}, URL: {}'.format(e1, url))\n",
    "\n",
    "    article_body_tag = soup.find('div', attrs = {'class':'article_content'})\n",
    "    if article_body_tag:\n",
    "        content = article_body_tag.text.strip()\n",
    "        if content:\n",
    "            res_dict['news'] = content\n",
    "            \n",
    "    if not res_dict or 'news' not in res_dict:\n",
    "        return\n",
    "        content_parser.logger.error('PTS url: {} did not process properly'.format(url))\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_test_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_util(pts_test_urls, pts_content_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 隨機選出50個新頭殼要聞的新聞連結：\n",
    "```sql\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '新頭殼要聞' ORDER BY RAND()\n",
    "LIMIT 50;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newstalk_test_sql = \"\"\"\n",
    "SELECT news_url FROM news_rss_feeds\n",
    "WHERE news_source = '新頭殼要聞' ORDER BY RAND()\n",
    "LIMIT 50; \"\"\"\n",
    "newstalk_test_urls = query_from_db(newstalk_test_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newstalk_content_processor(url):\n",
    "    res_dict = {}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding='utf-8'\n",
    "    web_content = r.text\n",
    "    soup = BeautifulSoup(web_content, \"lxml\")\n",
    "    if not soup:\n",
    "        return \n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag:\n",
    "        title_category = title_tag.string.split(' | ')\n",
    "        res_dict['news_title'] = title_category[0]\n",
    "        if len(title_category) > 1:\n",
    "            res_dict['news_category'] = title_category[1]\n",
    "    fb_app_tag = soup.find('meta', attrs = {'property':'fb:app_id'})\n",
    "    if fb_app_tag:\n",
    "        res_dict['news_fb_app_id'] = str(fb_app_tag['content'])\n",
    "\n",
    "    fb_page_tag = soup.find('meta', attrs = {'property':'fb:admins'})\n",
    "    if fb_page_tag:\n",
    "        res_dict['news_fb_page'] = str(fb_page_tag['content'])\n",
    "\n",
    "    #Optional\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'news_keywords'})\n",
    "    if keywords_tag:\n",
    "        res_dict['news_keywords'] = keywords_tag['content']\n",
    "\n",
    "    description_tag = soup.find('meta', attrs = {'name': 'description'})\n",
    "    if description_tag:\n",
    "        res_dict['news_description'] = description_tag['content']\n",
    "\n",
    "    time_tag = soup.find('meta', attrs = {'property': 'article:published_time'})\n",
    "\n",
    "    if time_tag:\n",
    "        try:\n",
    "            d1 = datetime.datetime.strptime(time_tag.get('content'), \"%Y-%m-%dT%H:%M:%S+08:00\") \n",
    "            d1 -= datetime.timedelta(hours=8)\n",
    "            db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            date_res = d1.strftime(db_date_format)\n",
    "            res_dict['news_published_date'] = date_res\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                d1 = datetime.datetime.strptime(time_tag.get('content'), \"%Y-%m-%dT%H:%M:%SZ\") \n",
    "                d1 -= datetime.timedelta(hours=8)\n",
    "                db_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                date_res = d1.strftime(db_date_format)\n",
    "                res_dict['news_published_date'] = date_res\n",
    "            except Exception as e2:\n",
    "                print(e2)\n",
    "                content_parser.logger.info('NewsTalk date error {}, URL: {}'.format(e2, url))\n",
    "\n",
    "    article_body_tag = soup.find('div', attrs = {'itemprop':'articleBody'})\n",
    "    #print(article_body_tag)\n",
    "    temp_content, links, links_descs = [], [], []\n",
    "    if article_body_tag:\n",
    "        p_tags = article_body_tag.find_all('p', attrs = {'class': None})\n",
    "        a_tags = article_body_tag.find_all('a')\n",
    "        if p_tags:\n",
    "            for p in p_tags:\n",
    "                if p.text:\n",
    "                    temp_content.append(p.text.strip())\n",
    "\n",
    "        if len(a_tags):\n",
    "            for a in a_tags:\n",
    "                if len(a):\n",
    "                    if a['href'] == '#':\n",
    "                        continue\n",
    "                    if a.get_text().strip() and 'www' in a['href']:\n",
    "                        links.append(a['href'])\n",
    "                        links_descs.append(a.get_text().strip())\n",
    "            res_dict['news_related_url'] = links\n",
    "            res_dict['news_related_url_desc'] = links_descs\n",
    "    content = '\\n'.join(temp_content).strip()\n",
    "    if content:\n",
    "        res_dict['news'] = content\n",
    "\n",
    "    if not res_dict or 'news' not in res_dict:\n",
    "        return\n",
    "        content_parser.logger.error('NewsTalk url: {} did not process properly'.format(url))\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_util(newstalk_test_urls, newstalk_content_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
