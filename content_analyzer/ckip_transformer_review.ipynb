{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "import pickle\n",
    "import logging\n",
    "import mysql.connector\n",
    "import time\n",
    "import spacy\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from db_func import query_from_db\n",
    "\n",
    "# Initialize drivers\n",
    "ws_driver  = CkipWordSegmenter(level=3)\n",
    "pos_driver = CkipPosTagger(level=3)\n",
    "ner_driver = CkipNerChunker(level=3)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_trf\")\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']= 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 Insert Developing Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1584.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2505.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1876.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1069.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2624.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1170.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4271.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2025.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1791.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1270.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1577.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2003.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2062.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2341.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1814.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1865.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1904.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1148.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1969.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2455.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2232.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1754.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2087.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 343.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1688.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2590.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2240.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1989.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3495.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1478.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1879.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3460.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3125.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1872.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3483.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1050.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1887.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1763.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1259.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1791.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1667.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1394.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1468.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1644.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1355.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1859.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2770.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1987.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3344.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1499.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1179.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1565.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1647.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2976.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1275.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1116.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1557.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1669.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1632.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3300.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1116.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1953.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1754.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1672.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1979.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1303.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2057.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1738.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1607.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1197.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2066.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1481.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1848.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3320.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 802.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1855.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3446.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1479.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1132.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2746.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1018.28it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1752.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3269.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1266.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4860.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2298.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1404.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1904.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3355.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1251.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1991.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1827.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1187.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1938.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2041.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1938.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1872.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3300.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1022.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1987.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1980.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2373.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4429.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1277.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1343.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1577.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 853.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1461.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1225.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3320.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1477.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1821.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3153.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5562.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1723.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3480.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3218.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1880.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2012.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1107.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1691.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1377.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1897.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2193.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1434.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 967.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1741.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1741.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1952.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1466.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1490.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1259.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4568.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1264.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3236.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1937.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3483.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1048.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1960.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3483.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2278.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1965.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2247.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2716.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1864.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3236.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2015.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1898.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3401.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3125.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1876.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3123.09it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1400.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1861.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3498.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1510.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1893.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3155.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2331.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1779.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3483.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1414.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1897.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3423.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2385.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2057.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3498.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2298.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1866.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2164.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1390.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1772.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3355.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1582.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2474.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3460.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1717.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1882.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3486.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1334.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1976.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2711.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1321.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1842.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3435.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2651.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1880.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2792.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2468.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2632.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1300.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1724.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2000.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2145.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2109.81it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1494.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2136.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2012.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1976.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3415.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1655.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2074.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1987.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1436.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1934.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3498.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2159.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1792.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2618.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2298.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1891.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2723.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2801.81it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1736.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3331.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1855.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1923.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2163.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2159.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1876.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2058.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1518.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1934.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3404.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1018.28it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2272.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2187.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2092.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1923.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1956.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3116.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1971.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3289.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1321.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1864.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3368.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1666.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1837.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1952.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1964.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1714.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3548.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3243.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1980.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3483.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 123.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1882.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2232.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1197.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1045.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1712.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1466.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1975.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2087.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2316.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1738.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1994.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3289.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1700.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3483.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3379.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1880.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1919.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2624.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4029.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2062.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1326.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4544.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2041.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2006.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1116.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2866.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 566.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3865.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1249.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2132.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1407.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 233.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 188.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1880.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3310.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1296.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1875.81it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1984.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3269.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1969.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3744.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2281.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1964.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3506.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1317.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1905.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2058.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1351.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1664.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3597.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3070.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "2021-02-19 22:26:58,725: ERROR: Finish process 100 examples in 523.4907739162445 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish process 100 examples in 523.4921119213104 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "with open(os.path.join(parent_dir, 'configs', 'loc2server.config'), 'rb') as f:\n",
    "    configs = pickle.load(f)\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "    host = configs['host'],\n",
    "    user = configs['user'],\n",
    "    passwd = configs['passwd'],\n",
    "    database = configs['database'])\n",
    "\n",
    "def insert_sentlevel_info(word_sentence: list, pos_sentence:list , news_sent_id, ws_approach, dep_sentence = ''):\n",
    "    assert len(word_sentence) == len(pos_sentence)\n",
    "    if ws_approach == 'ckip-transformer':\n",
    "        for word_index, (word, word_pos) in enumerate(zip(word_sentence, pos_sentence)):\n",
    "            word_cursor = mydb.cursor()\n",
    "            try:\n",
    "                word_cursor.execute(\"INSERT INTO news_db.news_words (news_sent_id, word_index, word, word_pos, ws_approach) VALUES (%s, %s, %s, %s, %s)\", (news_sent_id, word_index, word, word_pos, ws_approach))\n",
    "                #print(\"INSERT INTO news_db.news_words (news_sent_id, word_index, word, word_pos, ws_approach) VALUES ({}, {}, {}, {}, {})\".format(news_sent_id, word_index, word, word_pos, ws_approach))\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error('Insert word pos error: {}\\n News Sent ID: {}'.format(e, news_sent_id))\n",
    "                print('Insert word pos error: {}\\nNews Sent ID: {}\\nWs_approach: {}'.format(e, news_sent_id, ws_approach))\n",
    "                mydb.rollback()\n",
    "            else:\n",
    "                #print(\"else\")\n",
    "                mydb.commit()\n",
    "            word_cursor.close()\n",
    "    elif ws_approach == 'spacy-transformer':\n",
    "        for word_index, (word, word_pos, word_dep) in enumerate(zip(word_sentence, pos_sentence, dep_sentence)):\n",
    "            word_cursor = mydb.cursor()\n",
    "            try:\n",
    "                word_cursor.execute(\"INSERT INTO news_db.news_words (news_sent_id, word_index, word, word_pos, word_dep, ws_approach) VALUES (%s, %s, %s, %s, %s, %s)\", (news_sent_id, word_index, word, word_pos, word_dep, ws_approach))\n",
    "                #print(\"INSERT INTO news_db.news_words (news_sent_id, word_index, word, word_pos, word_dep, ws_approach) VALUES ({}, {}, {}, {}, {}, {})\".format(news_sent_id, word_index, word, word_pos, word_dep, ws_approach))\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error('Insert word pos error: {}\\n News Sent ID: {}'.format(e, news_sent_id))\n",
    "                print('Insert word pos error: {}\\nNews Sent ID: {}\\nWs_approach: {}'.format(e, news_sent_id, ws_approach))\n",
    "                mydb.rollback()\n",
    "            else:\n",
    "                mydb.commit()\n",
    "            word_cursor.close()\n",
    "    else:\n",
    "        logging.error(\"Error ws_approach\")\n",
    "        sys.exit(0)\n",
    "\n",
    "def insert_ner_info(entity_sent_list, news_sent_id, ner_approach):\n",
    "    for ent_id, (ent_text, ent_type, ent_index) in enumerate(sorted(entity_sent_list, key = lambda x:x[2][0])):\n",
    "        ner_cursor = mydb.cursor()\n",
    "        try:\n",
    "            ner_cursor.execute(\"INSERT INTO news_db.news_ners (news_sent_id, start_index, end_index, ent_type, ent_text, ner_approach) VALUES (%s, %s, %s, %s, %s, %s)\", (news_sent_id, ent_index[0], ent_index[1], ent_type, ent_text, ner_approach))\n",
    "            #print(\"INSERT INTO news_db.news_ners (news_sent_id, start_index, end_index, ent_type, ent_text, ner_approach) VALUES ({}, {}, {}, {}, {}, {})\".format(news_sent_id, ent_index[0], ent_index[1], ent_type, ent_text, ner_approach))\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error('Insert NER error: {}\\n News Sent ID: {}'.format(e, news_sent_id))\n",
    "            print('Insert NER error: {}\\n News Sent ID: {}'.format(e, news_sent_id))\n",
    "            mydb.rollback()\n",
    "        else:\n",
    "            mydb.commit()\n",
    "        ner_cursor.close()\n",
    "        \n",
    "def insert_process_flag(news_sent_id, process_name):\n",
    "    process_cursor = mydb.cursor()\n",
    "    try:\n",
    "        process_cursor.execute(\"INSERT INTO news_db.sent_processes (news_sent_id, process_name) VALUES (%s, %s)\", (news_sent_id, process_name))\n",
    "        #print(\"INSERT INTO news_db.nlp_processes (news_sent_id, process_name) VALUES ({}, {})\".format(news_sent_id, process_name))\n",
    "    except Exception as e:\n",
    "        logging.error('Process Insert Error: {}\\nNews sent ID: {}'.format(e, news_sent_id))\n",
    "        #print('Process Insert Error: {}\\nNews sent ID: {}'.format(e, news_sent_id))\n",
    "        mydb.rollback()\n",
    "    else:\n",
    "        mydb.commit()\n",
    "    process_cursor.close()\n",
    "\n",
    "def sent_level_analysis(raw_df):\n",
    "    for index, (content_sent_id, sent) in raw_df.iterrows():\n",
    "        insert_process_flag(content_sent_id, 'sent-analysis')\n",
    "        try:\n",
    "            word_sentence_list  = ws_driver(text, use_delim=False)\n",
    "            entity_sentence_list = ner_driver(text, use_delim=False)\n",
    "            pos_sentence_list = pos_driver(word_sentence_list, use_delim=False)\n",
    "            spacy_doc = nlp(sent)\n",
    "            word_sent_list_spacy, word_pos_list_spacy, word_dep_list_spacy = zip(*[(token.text, token.tag_, token.dep_) for token in spacy_doc])\n",
    "            entity_sent_list_spacy = [(ent.text, ent.label_, (ent.start_char, ent.end_char)) for ent in spacy_doc.ents]\n",
    "        except Exception as e:\n",
    "            logging.error('NLP process Error: {}\\n Content ID: {}'.format(e, content_sent_id))\n",
    "            print('NLP process Error: {}\\n Content ID: {}'.format(e, content_sent_id))\n",
    "        \n",
    "        \n",
    "        insert_sentlevel_info(word_sentence_list[0],  pos_sentence_list[0], content_sent_id, 'ckip-transformer')\n",
    "        insert_sentlevel_info(word_sent_list_spacy, word_pos_list_spacy, content_sent_id, 'spacy-transformer', word_dep_list_spacy)\n",
    "        insert_ner_info(entity_sentence_list[0], content_sent_id, 'ckip-transformer')\n",
    "        insert_ner_info(entity_sent_list_spacy, content_sent_id, 'spacy-transformer')\n",
    "     \n",
    "\n",
    "raw_df = query_from_db(\"SELECT nns.news_sent_id, nns.sent FROM news_db.news_sents nns WHERE not exists (SELECT 1 FROM news_db.sent_processes sp WHERE sp.process_name = \\'sent-analysis\\' and sp.news_sent_id = nns.news_sent_id) LIMIT 100\")\n",
    "sent_level_analysis(raw_df)\n",
    "mydb.close()\n",
    "logging.error('Finish process {} examples in {} seconds'.format(len(raw_df), time.time() - start))\n",
    "print('Finish process {} examples in {} seconds'.format(len(raw_df), time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk insert developing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = query_from_db(\"SELECT * FROM news_db.financial_sent_view LIMIT 16;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 394.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 2054.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 2152.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 4107.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 4641.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 1766.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 3294.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 1358.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.51s/it]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 2354.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 3059.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 1357.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n",
      "Tokenization: 100%|██████████| 4/4 [00:00<00:00, 425.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "def sent_generator(id_lst, sent_lst, batch_size):\n",
    "    current_idx = 0\n",
    "    max_len = len(sent_lst)\n",
    "    while True:\n",
    "        yield  id_lst[current_idx:current_idx + batch_size], sent_lst[current_idx:current_idx + batch_size]\n",
    "        current_idx += batch_size\n",
    "        if current_idx >= max_len:\n",
    "            break\n",
    "\"INSERT INTO news_db.news_words (news_sent_id, word_index, word, word_pos, word_dep, ws_approach) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "\"INSERT INTO news_db.news_ners (news_sent_id, start_index, end_index, ent_type, ent_text, ner_approach) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "news_words_res = []\n",
    "news_ners_res = []\n",
    "for sent_ids, sents in sent_generator(raw_df['news_sent_id'].tolist(), raw_df['sent'].tolist(), 4):\n",
    "    word_sentence_list  = ws_driver(sents, use_delim=False)\n",
    "    entity_sentence_list = ner_driver(sents, use_delim=False)\n",
    "    pos_sentence_list = pos_driver(word_sentence_list, use_delim=False)\n",
    "    for sent_id, word_res, pos_res in zip(sent_ids, word_sentence_list, pos_sentence_list):\n",
    "        for index, (word, word_pos) in enumerate(zip(word_res, pos_res)):\n",
    "            news_words_res.append((sent_id, index, word, word_pos, None, 'ckip-transformer'))\n",
    "    for sent_id, entities in zip(sent_ids, entity_sentence_list):\n",
    "        for ent_text, ent_type, ent_index in sorted(entities, key = lambda x:x[2][0]):\n",
    "            news_ners_res.append((sent_id, ent_index[0], ent_index[1], ent_type, ent_text, 'ckip-transformer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(262697, 0, '拜登', 'Nc', None, 'ckip-transformer'),\n",
       " (262697, 1, '政府', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 2, '官員', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 3, '承認', 'VE', None, 'ckip-transformer'),\n",
       " (262697, 4, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262697, 5, '眼前', 'Nc', None, 'ckip-transformer'),\n",
       " (262697, 6, '有', 'V_2', None, 'ckip-transformer'),\n",
       " (262697, 7, '許多', 'Neqa', None, 'ckip-transformer'),\n",
       " (262697, 8, '硬仗', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 9, '要', 'D', None, 'ckip-transformer'),\n",
       " (262697, 10, '打', 'VC', None, 'ckip-transformer'),\n",
       " (262697, 11, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262697, 12, '應變', 'VA', None, 'ckip-transformer'),\n",
       " (262697, 13, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262697, 14, '道', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 15, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262697, 16, '一', 'Neu', None, 'ckip-transformer'),\n",
       " (262697, 17, '是', 'SHI', None, 'ckip-transformer'),\n",
       " (262697, 18, '透過', 'P', None, 'ckip-transformer'),\n",
       " (262697, 19, '「', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (262697, 20, '綜合', 'A', None, 'ckip-transformer'),\n",
       " (262697, 21, '立法', 'VA', None, 'ckip-transformer'),\n",
       " (262697, 22, '」', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (262697, 23, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262697, 24, '把', 'P', None, 'ckip-transformer'),\n",
       " (262697, 25, '多', 'Neqa', None, 'ckip-transformer'),\n",
       " (262697, 26, '項', 'Nf', None, 'ckip-transformer'),\n",
       " (262697, 27, '方案', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 28, '包裹成', 'VG', None, 'ckip-transformer'),\n",
       " (262697, 29, '一', 'Neu', None, 'ckip-transformer'),\n",
       " (262697, 30, '件', 'Nf', None, 'ckip-transformer'),\n",
       " (262697, 31, '大', 'VH', None, 'ckip-transformer'),\n",
       " (262697, 32, '法案', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 33, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262697, 34, '好處', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 35, '是', 'SHI', None, 'ckip-transformer'),\n",
       " (262697, 36, '能', 'D', None, 'ckip-transformer'),\n",
       " (262697, 37, '保送', 'VC', None, 'ckip-transformer'),\n",
       " (262697, 38, '一些', 'Neqa', None, 'ckip-transformer'),\n",
       " (262697, 39, '在野黨', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 40, '有', 'V_2', None, 'ckip-transformer'),\n",
       " (262697, 41, '爭議', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 42, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262697, 43, '方案', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 44, '過關', 'VA', None, 'ckip-transformer'),\n",
       " (262697, 45, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262697, 46, '應變', 'VA', None, 'ckip-transformer'),\n",
       " (262697, 47, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262697, 48, '道', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 49, '其二', 'Cbb', None, 'ckip-transformer'),\n",
       " (262697, 50, '是', 'SHI', None, 'ckip-transformer'),\n",
       " (262697, 51, '利用', 'VC', None, 'ckip-transformer'),\n",
       " (262697, 52, '總統', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 53, '職權', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 54, '下達', 'VJ', None, 'ckip-transformer'),\n",
       " (262697, 55, '「', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (262697, 56, '行政', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 57, '命令', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 58, '」', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (262697, 59, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262697, 60, '但', 'Cbb', None, 'ckip-transformer'),\n",
       " (262697, 61, '壞處', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 62, '是', 'SHI', None, 'ckip-transformer'),\n",
       " (262697, 63, '容易', 'VH', None, 'ckip-transformer'),\n",
       " (262697, 64, '受', 'P', None, 'ckip-transformer'),\n",
       " (262697, 65, '法律', 'Na', None, 'ckip-transformer'),\n",
       " (262697, 66, '挑戰', 'VC', None, 'ckip-transformer'),\n",
       " (262697, 67, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262698, 0, '一些', 'Neqa', None, 'ckip-transformer'),\n",
       " (262698, 1, '民主黨', 'Nb', None, 'ckip-transformer'),\n",
       " (262698, 2, '人', 'Na', None, 'ckip-transformer'),\n",
       " (262698, 3, '指出', 'VE', None, 'ckip-transformer'),\n",
       " (262698, 4, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262698, 5, '拜登', 'Nb', None, 'ckip-transformer'),\n",
       " (262698, 6, '把', 'P', None, 'ckip-transformer'),\n",
       " (262698, 7, '施政', 'Nv', None, 'ckip-transformer'),\n",
       " (262698, 8, '支票', 'Na', None, 'ckip-transformer'),\n",
       " (262698, 9, '開', 'VC', None, 'ckip-transformer'),\n",
       " (262698, 10, '得', 'DE', None, 'ckip-transformer'),\n",
       " (262698, 11, '很', 'Dfa', None, 'ckip-transformer'),\n",
       " (262698, 12, '大', 'VH', None, 'ckip-transformer'),\n",
       " (262698, 13, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262698, 14, '但', 'Cbb', None, 'ckip-transformer'),\n",
       " (262698, 15, '不', 'D', None, 'ckip-transformer'),\n",
       " (262698, 16, '代表', 'VK', None, 'ckip-transformer'),\n",
       " (262698, 17, '他', 'Nh', None, 'ckip-transformer'),\n",
       " (262698, 18, '好高騖遠', 'VH', None, 'ckip-transformer'),\n",
       " (262698, 19, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262698, 20, '而是', 'Cbb', None, 'ckip-transformer'),\n",
       " (262698, 21, '他', 'Nh', None, 'ckip-transformer'),\n",
       " (262698, 22, '已', 'D', None, 'ckip-transformer'),\n",
       " (262698, 23, '計算', 'VC', None, 'ckip-transformer'),\n",
       " (262698, 24, '在', 'P', None, 'ckip-transformer'),\n",
       " (262698, 25, '兩黨', 'Neu', None, 'ckip-transformer'),\n",
       " (262698, 26, '爭執', 'VE', None, 'ckip-transformer'),\n",
       " (262698, 27, '妥協', 'VA', None, 'ckip-transformer'),\n",
       " (262698, 28, '後', 'Ng', None, 'ckip-transformer'),\n",
       " (262698, 29, '可能', 'D', None, 'ckip-transformer'),\n",
       " (262698, 30, '刪去', 'VC', None, 'ckip-transformer'),\n",
       " (262698, 31, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262698, 32, '部分', 'Neqa', None, 'ckip-transformer'),\n",
       " (262698, 33, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262698, 34, '如此', 'VH', None, 'ckip-transformer'),\n",
       " (262698, 35, '運作', 'VA', None, 'ckip-transformer'),\n",
       " (262698, 36, '後', 'Ng', None, 'ckip-transformer'),\n",
       " (262698, 37, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262698, 38, '成果', 'Na', None, 'ckip-transformer'),\n",
       " (262698, 39, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262698, 40, '總', 'D', None, 'ckip-transformer'),\n",
       " (262698, 41, '比', 'P', None, 'ckip-transformer'),\n",
       " (262698, 42, '設定', 'VC', None, 'ckip-transformer'),\n",
       " (262698, 43, '小小', 'VH', None, 'ckip-transformer'),\n",
       " (262698, 44, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262698, 45, '目標', 'Na', None, 'ckip-transformer'),\n",
       " (262698, 46, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262698, 47, '最後', 'Nd', None, 'ckip-transformer'),\n",
       " (262698, 48, '仍', 'D', None, 'ckip-transformer'),\n",
       " (262698, 49, '被', 'P', None, 'ckip-transformer'),\n",
       " (262698, 50, '砍', 'VC', None, 'ckip-transformer'),\n",
       " (262698, 51, '得', 'DE', None, 'ckip-transformer'),\n",
       " (262698, 52, '七零八', 'VH', None, 'ckip-transformer'),\n",
       " (262698, 53, '落', 'VA', None, 'ckip-transformer'),\n",
       " (262698, 54, '來得', 'Dfa', None, 'ckip-transformer'),\n",
       " (262698, 55, '漂亮', 'VH', None, 'ckip-transformer'),\n",
       " (262698, 56, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262673, 0, '拜登', 'Nb', None, 'ckip-transformer'),\n",
       " (262673, 1, '政權', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 2, '即將', 'D', None, 'ckip-transformer'),\n",
       " (262673, 3, '接手', 'VC', None, 'ckip-transformer'),\n",
       " (262673, 4, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262673, 5, '國政', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 6, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262673, 7, '政治', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 8, '光譜', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 9, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262673, 10, '指針', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 11, '位置', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 12, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262673, 13, '即將', 'D', None, 'ckip-transformer'),\n",
       " (262673, 14, '由', 'P', None, 'ckip-transformer'),\n",
       " (262673, 15, '保守', 'VH', None, 'ckip-transformer'),\n",
       " (262673, 16, '主義', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 17, '傾向', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 18, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262673, 19, '右側', 'Ncd', None, 'ckip-transformer'),\n",
       " (262673, 20, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262673, 21, '驟然', 'D', None, 'ckip-transformer'),\n",
       " (262673, 22, '撥轉', 'VC', None, 'ckip-transformer'),\n",
       " (262673, 23, '至', 'P', None, 'ckip-transformer'),\n",
       " (262673, 24, '社會主義', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 25, '傾向', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 26, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262673, 27, '左側', 'Ncd', None, 'ckip-transformer'),\n",
       " (262673, 28, '；', 'SEMICOLONCATEGORY', None, 'ckip-transformer'),\n",
       " (262673, 29, '在', 'P', None, 'ckip-transformer'),\n",
       " (262673, 30, '健保', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 31, '、', 'PAUSECATEGORY', None, 'ckip-transformer'),\n",
       " (262673, 32, '環保', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 33, '、', 'PAUSECATEGORY', None, 'ckip-transformer'),\n",
       " (262673, 34, '能源', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 35, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (262673, 36, '勞權', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 37, '議題', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 38, '外', 'Ng', None, 'ckip-transformer'),\n",
       " (262673, 39, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262673, 40, '拜登', 'Nb', None, 'ckip-transformer'),\n",
       " (262673, 41, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262673, 42, '稅制', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 43, '改革', 'VC', None, 'ckip-transformer'),\n",
       " (262673, 44, '藍圖', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 45, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262673, 46, '最', 'Dfa', None, 'ckip-transformer'),\n",
       " (262673, 47, '能', 'D', None, 'ckip-transformer'),\n",
       " (262673, 48, '凸顯', 'VJ', None, 'ckip-transformer'),\n",
       " (262673, 49, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262673, 50, '經濟', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 51, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (262673, 52, '社會', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 53, '即將', 'D', None, 'ckip-transformer'),\n",
       " (262673, 54, '面臨', 'VK', None, 'ckip-transformer'),\n",
       " (262673, 55, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262673, 56, '巨幅', 'A', None, 'ckip-transformer'),\n",
       " (262673, 57, '變革', 'Na', None, 'ckip-transformer'),\n",
       " (262673, 58, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262674, 0, '川普', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 1, '稅改', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 2, '使', 'VL', None, 'ckip-transformer'),\n",
       " (262674, 3, '二○一八年', 'Nd', None, 'ckip-transformer'),\n",
       " (262674, 4, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262674, 5, '稅收', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 6, '占', 'VJ', None, 'ckip-transformer'),\n",
       " (262674, 7, '經濟', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 8, '比重', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 9, '銳減', 'VH', None, 'ckip-transformer'),\n",
       " (262674, 10, '二點五', 'Neu', None, 'ckip-transformer'),\n",
       " (262674, 11, '個', 'Nf', None, 'ckip-transformer'),\n",
       " (262674, 12, '百分點', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 13, '；', 'SEMICOLONCATEGORY', None, 'ckip-transformer'),\n",
       " (262674, 14, '其中', 'Nep', None, 'ckip-transformer'),\n",
       " (262674, 15, '最', 'Dfa', None, 'ckip-transformer'),\n",
       " (262674, 16, '具', 'VJ', None, 'ckip-transformer'),\n",
       " (262674, 17, '代表', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 18, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262674, 19, '減稅', 'VA', None, 'ckip-transformer'),\n",
       " (262674, 20, '作為', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 21, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262674, 22, '莫過於', 'VG', None, 'ckip-transformer'),\n",
       " (262674, 23, '將', 'P', None, 'ckip-transformer'),\n",
       " (262674, 24, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262674, 25, '聯邦', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 26, '公司', 'Nc', None, 'ckip-transformer'),\n",
       " (262674, 27, '所得稅', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 28, '原本', 'D', None, 'ckip-transformer'),\n",
       " (262674, 29, '分', 'VC', None, 'ckip-transformer'),\n",
       " (262674, 30, '四', 'Neu', None, 'ckip-transformer'),\n",
       " (262674, 31, '級', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 32, '累進', 'VH', None, 'ckip-transformer'),\n",
       " (262674, 33, '、', 'PAUSECATEGORY', None, 'ckip-transformer'),\n",
       " (262674, 34, '最', 'Dfa', None, 'ckip-transformer'),\n",
       " (262674, 35, '高', 'VH', None, 'ckip-transformer'),\n",
       " (262674, 36, '稅率', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 37, '百分之卅五', 'Neqa', None, 'ckip-transformer'),\n",
       " (262674, 38, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262674, 39, '結構', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 40, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262674, 41, '自', 'P', None, 'ckip-transformer'),\n",
       " (262674, 42, '二○一八年', 'Nd', None, 'ckip-transformer'),\n",
       " (262674, 43, '一月', 'Nd', None, 'ckip-transformer'),\n",
       " (262674, 44, '一日', 'Nd', None, 'ckip-transformer'),\n",
       " (262674, 45, '起', 'Ng', None, 'ckip-transformer'),\n",
       " (262674, 46, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262674, 47, '改為', 'VG', None, 'ckip-transformer'),\n",
       " (262674, 48, '單一', 'A', None, 'ckip-transformer'),\n",
       " (262674, 49, '稅率', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 50, '百分之廿一', 'Neqa', None, 'ckip-transformer'),\n",
       " (262674, 51, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262674, 52, '拜登', 'Nb', None, 'ckip-transformer'),\n",
       " (262674, 53, '稅改', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 54, '則', 'D', None, 'ckip-transformer'),\n",
       " (262674, 55, '是', 'SHI', None, 'ckip-transformer'),\n",
       " (262674, 56, '計畫', 'VF', None, 'ckip-transformer'),\n",
       " (262674, 57, '將', 'P', None, 'ckip-transformer'),\n",
       " (262674, 58, '稅率', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 59, '一舉', 'D', None, 'ckip-transformer'),\n",
       " (262674, 60, '提高', 'VC', None, 'ckip-transformer'),\n",
       " (262674, 61, '至', 'P', None, 'ckip-transformer'),\n",
       " (262674, 62, '百分之廿八', 'Neqa', None, 'ckip-transformer'),\n",
       " (262674, 63, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262674, 64, '雖', 'Cbb', None, 'ckip-transformer'),\n",
       " (262674, 65, '未', 'D', None, 'ckip-transformer'),\n",
       " (262674, 66, '達', 'VJ', None, 'ckip-transformer'),\n",
       " (262674, 67, '川普', 'Nb', None, 'ckip-transformer'),\n",
       " (262674, 68, '稅改', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 69, '前', 'Ng', None, 'ckip-transformer'),\n",
       " (262674, 70, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262674, 71, '水準', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 72, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262674, 73, '但', 'Cbb', None, 'ckip-transformer'),\n",
       " (262674, 74, '上升', 'VA', None, 'ckip-transformer'),\n",
       " (262674, 75, '幅度', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 76, '足以', 'VL', None, 'ckip-transformer'),\n",
       " (262674, 77, '令', 'VL', None, 'ckip-transformer'),\n",
       " (262674, 78, '人', 'Na', None, 'ckip-transformer'),\n",
       " (262674, 79, '咋舌', 'VA', None, 'ckip-transformer'),\n",
       " (262674, 80, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262675, 0, '此外', 'Cbb', None, 'ckip-transformer'),\n",
       " (262675, 1, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262675, 2, '在', 'P', None, 'ckip-transformer'),\n",
       " (262675, 3, '川普', 'Nb', None, 'ckip-transformer'),\n",
       " (262675, 4, '稅改', 'Na', None, 'ckip-transformer'),\n",
       " (262675, 5, '中', 'Ng', None, 'ckip-transformer'),\n",
       " (262675, 6, '已', 'D', None, 'ckip-transformer'),\n",
       " (262675, 7, '取消', 'VC', None, 'ckip-transformer'),\n",
       " (262675, 8, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262675, 9, '公司', 'Nc', None, 'ckip-transformer'),\n",
       " (262675, 10, '最', 'Dfa', None, 'ckip-transformer'),\n",
       " (262675, 11, '低', 'VH', None, 'ckip-transformer'),\n",
       " (262675, 12, '稅負', 'Na', None, 'ckip-transformer'),\n",
       " (262675, 13, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262675, 14, '將', 'D', None, 'ckip-transformer'),\n",
       " (262675, 15, '再', 'D', None, 'ckip-transformer'),\n",
       " (262675, 16, '粉墨登場', 'VA', None, 'ckip-transformer'),\n",
       " (262675, 17, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262675, 18, '拜登', 'VC', None, 'ckip-transformer'),\n",
       " (262675, 19, '計畫', 'Na', None, 'ckip-transformer'),\n",
       " (262675, 20, '對於', 'P', None, 'ckip-transformer'),\n",
       " (262675, 21, '財報', 'Na', None, 'ckip-transformer'),\n",
       " (262675, 22, '全球', 'Nc', None, 'ckip-transformer'),\n",
       " (262675, 23, '所得', 'Na', None, 'ckip-transformer'),\n",
       " (262675, 24, '淨額', 'Na', None, 'ckip-transformer'),\n",
       " (262675, 25, '達', 'VJ', None, 'ckip-transformer'),\n",
       " (262675, 26, '一億', 'Neu', None, 'ckip-transformer'),\n",
       " (262675, 27, '美元', 'Nf', None, 'ckip-transformer'),\n",
       " (262675, 28, '以上', 'Neqb', None, 'ckip-transformer'),\n",
       " (262675, 29, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262675, 30, '公司', 'Nc', None, 'ckip-transformer'),\n",
       " (262675, 31, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262675, 32, '按', 'P', None, 'ckip-transformer'),\n",
       " (262675, 33, '財報', 'Na', None, 'ckip-transformer'),\n",
       " (262675, 34, '所得', 'Na', None, 'ckip-transformer'),\n",
       " (262675, 35, '課徵', 'VC', None, 'ckip-transformer'),\n",
       " (262675, 36, '百分之十五', 'Neqa', None, 'ckip-transformer'),\n",
       " (262675, 37, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262675, 38, '最', 'Dfa', None, 'ckip-transformer'),\n",
       " (262675, 39, '低', 'VH', None, 'ckip-transformer'),\n",
       " (262675, 40, '稅負', 'Na', None, 'ckip-transformer'),\n",
       " (262675, 41, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 0, '在', 'P', None, 'ckip-transformer'),\n",
       " (262676, 1, '個人稅', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 2, '部分', 'Neqa', None, 'ckip-transformer'),\n",
       " (262676, 3, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 4, '川普', 'Nb', None, 'ckip-transformer'),\n",
       " (262676, 5, '稅改', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 6, '將', 'P', None, 'ckip-transformer'),\n",
       " (262676, 7, '所得', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 8, '最', 'Dfa', None, 'ckip-transformer'),\n",
       " (262676, 9, '高', 'VH', None, 'ckip-transformer'),\n",
       " (262676, 10, '稅率', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 11, '從', 'P', None, 'ckip-transformer'),\n",
       " (262676, 12, '百分之卅九點六', 'Neqa', None, 'ckip-transformer'),\n",
       " (262676, 13, '降為', 'VG', None, 'ckip-transformer'),\n",
       " (262676, 14, '卅七', 'Neu', None, 'ckip-transformer'),\n",
       " (262676, 15, '；', 'SEMICOLONCATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 16, '拜登', 'Nb', None, 'ckip-transformer'),\n",
       " (262676, 17, '計畫', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 18, '對', 'P', None, 'ckip-transformer'),\n",
       " (262676, 19, '課稅', 'VB', None, 'ckip-transformer'),\n",
       " (262676, 20, '所得', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 21, '四十萬', 'Neu', None, 'ckip-transformer'),\n",
       " (262676, 22, '美元', 'Nf', None, 'ckip-transformer'),\n",
       " (262676, 23, '以上', 'Neqb', None, 'ckip-transformer'),\n",
       " (262676, 24, '家庭', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 25, '恢復', 'VHC', None, 'ckip-transformer'),\n",
       " (262676, 26, '百分之卅九點六', 'Neqa', None, 'ckip-transformer'),\n",
       " (262676, 27, '稅率', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 28, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 29, '另一方面', 'Cbb', None, 'ckip-transformer'),\n",
       " (262676, 30, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 31, '現行', 'A', None, 'ckip-transformer'),\n",
       " (262676, 32, '薪資稅', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 33, '中', 'Ng', None, 'ckip-transformer'),\n",
       " (262676, 34, '有關', 'VJ', None, 'ckip-transformer'),\n",
       " (262676, 35, '退休金', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 36, '部分', 'Neqa', None, 'ckip-transformer'),\n",
       " (262676, 37, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 38, '設有', 'VJ', None, 'ckip-transformer'),\n",
       " (262676, 39, '年度', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 40, '上限', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 41, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 42, '超過', 'VJ', None, 'ckip-transformer'),\n",
       " (262676, 43, '上限', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 44, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262676, 45, '薪資', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 46, '不需', 'D', None, 'ckip-transformer'),\n",
       " (262676, 47, '繳納', 'VC', None, 'ckip-transformer'),\n",
       " (262676, 48, '薪資稅', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 49, '（', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 50, '此', 'Nep', None, 'ckip-transformer'),\n",
       " (262676, 51, '上限', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 52, '金額', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 53, '按', 'P', None, 'ckip-transformer'),\n",
       " (262676, 54, '通貨膨脹', 'VH', None, 'ckip-transformer'),\n",
       " (262676, 55, '調整', 'VC', None, 'ckip-transformer'),\n",
       " (262676, 56, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 57, '二○二一年', 'Nd', None, 'ckip-transformer'),\n",
       " (262676, 58, '為', 'VG', None, 'ckip-transformer'),\n",
       " (262676, 59, '十四萬二千八百', 'Neu', None, 'ckip-transformer'),\n",
       " (262676, 60, '美元', 'Nf', None, 'ckip-transformer'),\n",
       " (262676, 61, '）', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 62, '；', 'SEMICOLONCATEGORY', None, 'ckip-transformer'),\n",
       " (262676, 63, '拜登', 'VCL', None, 'ckip-transformer'),\n",
       " (262676, 64, '計畫', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 65, '對於', 'P', None, 'ckip-transformer'),\n",
       " (262676, 66, '高過', 'VJ', None, 'ckip-transformer'),\n",
       " (262676, 67, '四十萬', 'Neu', None, 'ckip-transformer'),\n",
       " (262676, 68, '美元', 'Nf', None, 'ckip-transformer'),\n",
       " (262676, 69, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262676, 70, '薪資', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 71, '開徵', 'VC', None, 'ckip-transformer'),\n",
       " (262676, 72, '薪資稅', 'Na', None, 'ckip-transformer'),\n",
       " (262676, 73, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262677, 0, '此外', 'Cbb', None, 'ckip-transformer'),\n",
       " (262677, 1, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262677, 2, '拜登', 'Nb', None, 'ckip-transformer'),\n",
       " (262677, 3, '也', 'D', None, 'ckip-transformer'),\n",
       " (262677, 4, '計畫', 'VF', None, 'ckip-transformer'),\n",
       " (262677, 5, '取消', 'VC', None, 'ckip-transformer'),\n",
       " (262677, 6, '所得', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 7, '一百萬', 'Neu', None, 'ckip-transformer'),\n",
       " (262677, 8, '美元', 'Nf', None, 'ckip-transformer'),\n",
       " (262677, 9, '以上', 'Neqb', None, 'ckip-transformer'),\n",
       " (262677, 10, '者', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 11, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262677, 12, '長期', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 13, '持有', 'VJ', None, 'ckip-transformer'),\n",
       " (262677, 14, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262677, 15, '資本', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 16, '利得', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 17, '、', 'PAUSECATEGORY', None, 'ckip-transformer'),\n",
       " (262677, 18, '特定', 'A', None, 'ckip-transformer'),\n",
       " (262677, 19, '股利', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 20, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (262677, 21, '利息', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 22, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262677, 23, '優惠', 'VJ', None, 'ckip-transformer'),\n",
       " (262677, 24, '稅率', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 25, '；', 'SEMICOLONCATEGORY', None, 'ckip-transformer'),\n",
       " (262677, 26, '並', 'Cbb', None, 'ckip-transformer'),\n",
       " (262677, 27, '打算', 'VF', None, 'ckip-transformer'),\n",
       " (262677, 28, '將', 'P', None, 'ckip-transformer'),\n",
       " (262677, 29, '遺產', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 30, '及', 'Caa', None, 'ckip-transformer'),\n",
       " (262677, 31, '贈與', 'VD', None, 'ckip-transformer'),\n",
       " (262677, 32, '最', 'Dfa', None, 'ckip-transformer'),\n",
       " (262677, 33, '高', 'VH', None, 'ckip-transformer'),\n",
       " (262677, 34, '稅率', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 35, '自', 'P', None, 'ckip-transformer'),\n",
       " (262677, 36, '目前', 'Nd', None, 'ckip-transformer'),\n",
       " (262677, 37, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262677, 38, '百分之四十', 'Neqa', None, 'ckip-transformer'),\n",
       " (262677, 39, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262677, 40, '提高', 'VC', None, 'ckip-transformer'),\n",
       " (262677, 41, '至', 'P', None, 'ckip-transformer'),\n",
       " (262677, 42, '二○○九年', 'Nd', None, 'ckip-transformer'),\n",
       " (262677, 43, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262677, 44, '百分之四十五', 'Neqa', None, 'ckip-transformer'),\n",
       " (262677, 45, '水準', 'Na', None, 'ckip-transformer'),\n",
       " (262677, 46, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262678, 0, '就', 'P', None, 'ckip-transformer'),\n",
       " (262678, 1, '上述', 'Na', None, 'ckip-transformer'),\n",
       " (262678, 2, '已', 'D', None, 'ckip-transformer'),\n",
       " (262678, 3, '揭櫫', 'VC', None, 'ckip-transformer'),\n",
       " (262678, 4, '方向', 'Na', None, 'ckip-transformer'),\n",
       " (262678, 5, '而論', 'Cbb', None, 'ckip-transformer'),\n",
       " (262678, 6, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262678, 7, '拜登', 'VC', None, 'ckip-transformer'),\n",
       " (262678, 8, '稅改', 'Na', None, 'ckip-transformer'),\n",
       " (262678, 9, '主要', 'A', None, 'ckip-transformer'),\n",
       " (262678, 10, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262678, 11, '基調', 'Na', None, 'ckip-transformer'),\n",
       " (262678, 12, '是', 'SHI', None, 'ckip-transformer'),\n",
       " (262678, 13, '在', 'P', None, 'ckip-transformer'),\n",
       " (262678, 14, '既有', 'A', None, 'ckip-transformer'),\n",
       " (262678, 15, '稅制', 'Na', None, 'ckip-transformer'),\n",
       " (262678, 16, '結構', 'Na', None, 'ckip-transformer'),\n",
       " (262678, 17, '下', 'Ng', None, 'ckip-transformer'),\n",
       " (262678, 18, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262678, 19, '提高', 'VC', None, 'ckip-transformer'),\n",
       " (262678, 20, '公司', 'Nc', None, 'ckip-transformer'),\n",
       " (262678, 21, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (262678, 22, '富人', 'Na', None, 'ckip-transformer'),\n",
       " (262678, 23, '稅負', 'Na', None, 'ckip-transformer'),\n",
       " (262678, 24, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262678, 25, '強化', 'VHC', None, 'ckip-transformer'),\n",
       " (262678, 26, '租稅', 'Na', None, 'ckip-transformer'),\n",
       " (262678, 27, '促進', 'VK', None, 'ckip-transformer'),\n",
       " (262678, 28, '分配', 'VD', None, 'ckip-transformer'),\n",
       " (262678, 29, '公平', 'VH', None, 'ckip-transformer'),\n",
       " (262678, 30, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262678, 31, '功能', 'Na', None, 'ckip-transformer'),\n",
       " (262678, 32, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 0, '強', 'VC', None, 'ckip-transformer'),\n",
       " (262679, 1, '打', 'VC', None, 'ckip-transformer'),\n",
       " (262679, 2, '經濟', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 3, '優先', 'VH', None, 'ckip-transformer'),\n",
       " (262679, 4, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262679, 5, '川普', 'Nb', None, 'ckip-transformer'),\n",
       " (262679, 6, '政權', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 7, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 8, '在', 'P', None, 'ckip-transformer'),\n",
       " (262679, 9, '疫情', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 10, '前', 'Ng', None, 'ckip-transformer'),\n",
       " (262679, 11, '為', 'P', None, 'ckip-transformer'),\n",
       " (262679, 12, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262679, 13, '經濟', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 14, '帶來', 'VC', None, 'ckip-transformer'),\n",
       " (262679, 15, '平均', 'VH', None, 'ckip-transformer'),\n",
       " (262679, 16, '百分之二點四', 'Neqa', None, 'ckip-transformer'),\n",
       " (262679, 17, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262679, 18, '成長', 'Nv', None, 'ckip-transformer'),\n",
       " (262679, 19, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 20, '並', 'Cbb', None, 'ckip-transformer'),\n",
       " (262679, 21, '使', 'VL', None, 'ckip-transformer'),\n",
       " (262679, 22, '美股', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 23, '迭', 'D', None, 'ckip-transformer'),\n",
       " (262679, 24, '創', 'VC', None, 'ckip-transformer'),\n",
       " (262679, 25, '新高', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 26, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 27, '但', 'Cbb', None, 'ckip-transformer'),\n",
       " (262679, 28, '付出', 'VC', None, 'ckip-transformer'),\n",
       " (262679, 29, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262679, 30, '是', 'SHI', None, 'ckip-transformer'),\n",
       " (262679, 31, '分配面', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 32, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262679, 33, '代價', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 34, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 35, '根據', 'P', None, 'ckip-transformer'),\n",
       " (262679, 36, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262679, 37, '普查局', 'Nc', None, 'ckip-transformer'),\n",
       " (262679, 38, '資料', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 39, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 40, '二○一八年', 'Nd', None, 'ckip-transformer'),\n",
       " (262679, 41, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262679, 42, '家庭', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 43, '所得', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 44, '分配', 'VD', None, 'ckip-transformer'),\n",
       " (262679, 45, '不均', 'VH', None, 'ckip-transformer'),\n",
       " (262679, 46, '指標', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 47, '（', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 48, '吉尼', 'Nb', None, 'ckip-transformer'),\n",
       " (262679, 49, '係數', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 50, '）', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 51, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 52, '攀升', 'VCL', None, 'ckip-transformer'),\n",
       " (262679, 53, '至', 'P', None, 'ckip-transformer'),\n",
       " (262679, 54, '五十', 'Neu', None, 'ckip-transformer'),\n",
       " (262679, 55, '年', 'Nf', None, 'ckip-transformer'),\n",
       " (262679, 56, '來', 'Ng', None, 'ckip-transformer'),\n",
       " (262679, 57, '最', 'Dfa', None, 'ckip-transformer'),\n",
       " (262679, 58, '糟', 'VH', None, 'ckip-transformer'),\n",
       " (262679, 59, '；', 'SEMICOLONCATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 60, '全職', 'VH', None, 'ckip-transformer'),\n",
       " (262679, 61, '工作者', 'VA', None, 'ckip-transformer'),\n",
       " (262679, 62, '個人', 'Nh', None, 'ckip-transformer'),\n",
       " (262679, 63, '所得', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 64, '分配', 'VD', None, 'ckip-transformer'),\n",
       " (262679, 65, '不均', 'VH', None, 'ckip-transformer'),\n",
       " (262679, 66, '指標', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 67, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 68, '也', 'D', None, 'ckip-transformer'),\n",
       " (262679, 69, '於', 'P', None, 'ckip-transformer'),\n",
       " (262679, 70, '二○一九年', 'Nd', None, 'ckip-transformer'),\n",
       " (262679, 71, '升', 'VCL', None, 'ckip-transformer'),\n",
       " (262679, 72, '至', 'P', None, 'ckip-transformer'),\n",
       " (262679, 73, '最高點', 'Dfa', None, 'ckip-transformer'),\n",
       " (262679, 74, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 75, '根據', 'P', None, 'ckip-transformer'),\n",
       " (262679, 76, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262679, 77, '國會', 'Nc', None, 'ckip-transformer'),\n",
       " (262679, 78, '預算局', 'Nc', None, 'ckip-transformer'),\n",
       " (262679, 79, '二○一九年', 'Nd', None, 'ckip-transformer'),\n",
       " (262679, 80, '底', 'Ng', None, 'ckip-transformer'),\n",
       " (262679, 81, '試算', 'VF', None, 'ckip-transformer'),\n",
       " (262679, 82, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 83, '二○二一年', 'Nd', None, 'ckip-transformer'),\n",
       " (262679, 84, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262679, 85, '所得', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 86, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (262679, 87, '財富', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 88, '分配', 'VD', None, 'ckip-transformer'),\n",
       " (262679, 89, '不均', 'VH', None, 'ckip-transformer'),\n",
       " (262679, 90, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262679, 91, '問題', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 92, '會', 'D', None, 'ckip-transformer'),\n",
       " (262679, 93, '較', 'P', None, 'ckip-transformer'),\n",
       " (262679, 94, '二○一六年', 'Nd', None, 'ckip-transformer'),\n",
       " (262679, 95, '更為', 'Dfa', None, 'ckip-transformer'),\n",
       " (262679, 96, '嚴重', 'VH', None, 'ckip-transformer'),\n",
       " (262679, 97, '；', 'SEMICOLONCATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 98, '如', 'Cbb', None, 'ckip-transformer'),\n",
       " (262679, 99, '納入', 'VJ', None, 'ckip-transformer'),\n",
       " (262679, 100, '疫情', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 101, '對於', 'P', None, 'ckip-transformer'),\n",
       " (262679, 102, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262679, 103, '經濟', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 104, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262679, 105, '衝擊', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 106, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262679, 107, '分配', 'VD', None, 'ckip-transformer'),\n",
       " (262679, 108, '惡化', 'VH', None, 'ckip-transformer'),\n",
       " (262679, 109, '情形', 'Na', None, 'ckip-transformer'),\n",
       " (262679, 110, '勢必', 'D', None, 'ckip-transformer'),\n",
       " (262679, 111, '雪上加霜', 'VA', None, 'ckip-transformer'),\n",
       " (262679, 112, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262680, 0, '長久以來', 'D', None, 'ckip-transformer'),\n",
       " (262680, 1, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262680, 2, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262680, 3, '社會', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 4, '本', 'D', None, 'ckip-transformer'),\n",
       " (262680, 5, '就', 'D', None, 'ckip-transformer'),\n",
       " (262680, 6, '面臨', 'VK', None, 'ckip-transformer'),\n",
       " (262680, 7, '較', 'P', None, 'ckip-transformer'),\n",
       " (262680, 8, '其他', 'Neqa', None, 'ckip-transformer'),\n",
       " (262680, 9, '已', 'D', None, 'ckip-transformer'),\n",
       " (262680, 10, '開發', 'VC', None, 'ckip-transformer'),\n",
       " (262680, 11, '國家', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 12, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262680, 13, '更為', 'Dfa', None, 'ckip-transformer'),\n",
       " (262680, 14, '嚴重', 'VH', None, 'ckip-transformer'),\n",
       " (262680, 15, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262680, 16, '分配', 'VD', None, 'ckip-transformer'),\n",
       " (262680, 17, '不均', 'VH', None, 'ckip-transformer'),\n",
       " (262680, 18, '問題', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 19, '；', 'SEMICOLONCATEGORY', None, 'ckip-transformer'),\n",
       " (262680, 20, '日益', 'D', None, 'ckip-transformer'),\n",
       " (262680, 21, '惡化', 'VH', None, 'ckip-transformer'),\n",
       " (262680, 22, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262680, 23, '分配', 'VD', None, 'ckip-transformer'),\n",
       " (262680, 24, '情形', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 25, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262680, 26, '正在', 'D', None, 'ckip-transformer'),\n",
       " (262680, 27, '以', 'P', None, 'ckip-transformer'),\n",
       " (262680, 28, '讓', 'VL', None, 'ckip-transformer'),\n",
       " (262680, 29, '人', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 30, '相當', 'Dfa', None, 'ckip-transformer'),\n",
       " (262680, 31, '不安', 'VH', None, 'ckip-transformer'),\n",
       " (262680, 32, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262680, 33, '速度', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 34, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262680, 35, '侵蝕', 'VC', None, 'ckip-transformer'),\n",
       " (262680, 36, '著', 'Di', None, 'ckip-transformer'),\n",
       " (262680, 37, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262680, 38, '經濟', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 39, '發展', 'VC', None, 'ckip-transformer'),\n",
       " (262680, 40, '所', 'D', None, 'ckip-transformer'),\n",
       " (262680, 41, '奠基', 'VB', None, 'ckip-transformer'),\n",
       " (262680, 42, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262680, 43, '自由', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 44, '市場', 'Nc', None, 'ckip-transformer'),\n",
       " (262680, 45, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262680, 46, '拜登增稅', 'VC', None, 'ckip-transformer'),\n",
       " (262680, 47, '所', 'D', None, 'ckip-transformer'),\n",
       " (262680, 48, '取得', 'VC', None, 'ckip-transformer'),\n",
       " (262680, 49, '之', 'DE', None, 'ckip-transformer'),\n",
       " (262680, 50, '稅收', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 51, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262680, 52, '可', 'D', None, 'ckip-transformer'),\n",
       " (262680, 53, '遏抑', 'VC', None, 'ckip-transformer'),\n",
       " (262680, 54, '分配', 'VD', None, 'ckip-transformer'),\n",
       " (262680, 55, '不均', 'VH', None, 'ckip-transformer'),\n",
       " (262680, 56, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262680, 57, '問題', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 58, '進一步', 'D', None, 'ckip-transformer'),\n",
       " (262680, 59, '失控', 'VH', None, 'ckip-transformer'),\n",
       " (262680, 60, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262680, 61, '避免', 'VE', None, 'ckip-transformer'),\n",
       " (262680, 62, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (262680, 63, '民主', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 64, '制度', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 65, '走進', 'VCL', None, 'ckip-transformer'),\n",
       " (262680, 66, '毀滅', 'VC', None, 'ckip-transformer'),\n",
       " (262680, 67, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262680, 68, '死胡同', 'Na', None, 'ckip-transformer'),\n",
       " (262680, 69, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262681, 0, '拜登', 'Nc', None, 'ckip-transformer'),\n",
       " (262681, 1, '如何', 'D', None, 'ckip-transformer'),\n",
       " (262681, 2, '推展', 'VC', None, 'ckip-transformer'),\n",
       " (262681, 3, '稅改', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 4, '修法', 'VA', None, 'ckip-transformer'),\n",
       " (262681, 5, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262681, 6, '非常', 'Dfa', None, 'ckip-transformer'),\n",
       " (262681, 7, '值得', 'VH', None, 'ckip-transformer'),\n",
       " (262681, 8, '觀察', 'VE', None, 'ckip-transformer'),\n",
       " (262681, 9, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262681, 10, '眾議院', 'Nc', None, 'ckip-transformer'),\n",
       " (262681, 11, '部分', 'Neqa', None, 'ckip-transformer'),\n",
       " (262681, 12, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262681, 13, '由於', 'Cbb', None, 'ckip-transformer'),\n",
       " (262681, 14, '民主黨', 'Nb', None, 'ckip-transformer'),\n",
       " (262681, 15, '占有', 'VJ', None, 'ckip-transformer'),\n",
       " (262681, 16, '過半', 'VH', None, 'ckip-transformer'),\n",
       " (262681, 17, '席次', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 18, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262681, 19, '修法', 'VA', None, 'ckip-transformer'),\n",
       " (262681, 20, '提案', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 21, '當', 'D', None, 'ckip-transformer'),\n",
       " (262681, 22, '可', 'D', None, 'ckip-transformer'),\n",
       " (262681, 23, '輕騎', 'VH', None, 'ckip-transformer'),\n",
       " (262681, 24, '過關', 'VA', None, 'ckip-transformer'),\n",
       " (262681, 25, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262681, 26, '參議院', 'Nc', None, 'ckip-transformer'),\n",
       " (262681, 27, '部分', 'Neqa', None, 'ckip-transformer'),\n",
       " (262681, 28, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262681, 29, '目前', 'Nd', None, 'ckip-transformer'),\n",
       " (262681, 30, '形成', 'VG', None, 'ckip-transformer'),\n",
       " (262681, 31, '民主', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 32, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (262681, 33, '共和', 'VH', None, 'ckip-transformer'),\n",
       " (262681, 34, '兩黨', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 35, '各', 'D', None, 'ckip-transformer'),\n",
       " (262681, 36, '有', 'V_2', None, 'ckip-transformer'),\n",
       " (262681, 37, '五十', 'Neu', None, 'ckip-transformer'),\n",
       " (262681, 38, '席', 'Nf', None, 'ckip-transformer'),\n",
       " (262681, 39, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262681, 40, '對峙', 'VH', None, 'ckip-transformer'),\n",
       " (262681, 41, '僵局', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 42, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262681, 43, '由於', 'Cbb', None, 'ckip-transformer'),\n",
       " (262681, 44, '兼任', 'VG', None, 'ckip-transformer'),\n",
       " (262681, 45, '參議院', 'Nc', None, 'ckip-transformer'),\n",
       " (262681, 46, '議長', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 47, '身分', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 48, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262681, 49, '副總統', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 50, '賀錦麗', 'Nb', None, 'ckip-transformer'),\n",
       " (262681, 51, '是', 'SHI', None, 'ckip-transformer'),\n",
       " (262681, 52, '打破', 'VC', None, 'ckip-transformer'),\n",
       " (262681, 53, '僵局', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 54, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262681, 55, '關鍵', 'Na', None, 'ckip-transformer'),\n",
       " (262681, 56, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262681, 57, '民主黨', 'Nb', None, 'ckip-transformer'),\n",
       " (262681, 58, '在', 'P', None, 'ckip-transformer'),\n",
       " (262681, 59, '參議院', 'Nc', None, 'ckip-transformer'),\n",
       " (262681, 60, '實質', 'D', None, 'ckip-transformer'),\n",
       " (262681, 61, '過半', 'VH', None, 'ckip-transformer'),\n",
       " (262681, 62, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262681, 63, '但', 'Cbb', None, 'ckip-transformer'),\n",
       " (262681, 64, '仍', 'D', None, 'ckip-transformer'),\n",
       " (262681, 65, '少於', 'VJ', None, 'ckip-transformer'),\n",
       " (262681, 66, '可', 'D', None, 'ckip-transformer'),\n",
       " (262681, 67, '避免', 'VE', None, 'ckip-transformer'),\n",
       " (262681, 68, '共和黨', 'Nb', None, 'ckip-transformer'),\n",
       " (262681, 69, '杯葛', 'VC', None, 'ckip-transformer'),\n",
       " (262681, 70, '需', 'D', None, 'ckip-transformer'),\n",
       " (262681, 71, '有', 'V_2', None, 'ckip-transformer'),\n",
       " (262681, 72, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262681, 73, '六十', 'Neu', None, 'ckip-transformer'),\n",
       " (262681, 74, '票', 'Nf', None, 'ckip-transformer'),\n",
       " (262681, 75, '超級', 'A', None, 'ckip-transformer'),\n",
       " (262681, 76, '多數', 'Neqa', None, 'ckip-transformer'),\n",
       " (262681, 77, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262682, 0, '參議員', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 1, '拜登', 'VCL', None, 'ckip-transformer'),\n",
       " (262682, 2, '卅六', 'Neu', None, 'ckip-transformer'),\n",
       " (262682, 3, '年', 'Nf', None, 'ckip-transformer'),\n",
       " (262682, 4, '國會', 'Nc', None, 'ckip-transformer'),\n",
       " (262682, 5, '資歷', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 6, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262682, 7, '曾', 'D', None, 'ckip-transformer'),\n",
       " (262682, 8, '為', 'VG', None, 'ckip-transformer'),\n",
       " (262682, 9, '多', 'Neqa', None, 'ckip-transformer'),\n",
       " (262682, 10, '任', 'Nf', None, 'ckip-transformer'),\n",
       " (262682, 11, '同', 'Nes', None, 'ckip-transformer'),\n",
       " (262682, 12, '黨籍', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 13, '總統', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 14, '作嫁', 'VB', None, 'ckip-transformer'),\n",
       " (262682, 15, '、', 'PAUSECATEGORY', None, 'ckip-transformer'),\n",
       " (262682, 16, '協調', 'VC', None, 'ckip-transformer'),\n",
       " (262682, 17, '折衝', 'VA', None, 'ckip-transformer'),\n",
       " (262682, 18, '國會', 'Nc', None, 'ckip-transformer'),\n",
       " (262682, 19, '修法', 'VA', None, 'ckip-transformer'),\n",
       " (262682, 20, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262682, 21, '素', 'D', None, 'ckip-transformer'),\n",
       " (262682, 22, '享', 'VJ', None, 'ckip-transformer'),\n",
       " (262682, 23, '美譽', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 24, '；', 'SEMICOLONCATEGORY', None, 'ckip-transformer'),\n",
       " (262682, 25, '副總統', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 26, '拜登', 'VC', None, 'ckip-transformer'),\n",
       " (262682, 27, '八', 'Neu', None, 'ckip-transformer'),\n",
       " (262682, 28, '年', 'Nf', None, 'ckip-transformer'),\n",
       " (262682, 29, '任內', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 30, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262682, 31, '也', 'D', None, 'ckip-transformer'),\n",
       " (262682, 32, '成功', 'VH', None, 'ckip-transformer'),\n",
       " (262682, 33, '推動', 'VC', None, 'ckip-transformer'),\n",
       " (262682, 34, '包括', 'VK', None, 'ckip-transformer'),\n",
       " (262682, 35, '歐巴馬', 'Nb', None, 'ckip-transformer'),\n",
       " (262682, 36, '健保', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 37, '改革', 'VC', None, 'ckip-transformer'),\n",
       " (262682, 38, '在', 'P', None, 'ckip-transformer'),\n",
       " (262682, 39, '內', 'Ncd', None, 'ckip-transformer'),\n",
       " (262682, 40, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262682, 41, '諸多', 'Neqa', None, 'ckip-transformer'),\n",
       " (262682, 42, '重大', 'VH', None, 'ckip-transformer'),\n",
       " (262682, 43, '法案', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 44, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (262682, 45, '總統', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 46, '拜登', 'VC', None, 'ckip-transformer'),\n",
       " (262682, 47, '將', 'D', None, 'ckip-transformer'),\n",
       " (262682, 48, '如何', 'D', None, 'ckip-transformer'),\n",
       " (262682, 49, '運籌帷幄', 'VA', None, 'ckip-transformer'),\n",
       " (262682, 50, '自己', 'Nh', None, 'ckip-transformer'),\n",
       " (262682, 51, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262682, 52, '稅改', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 53, '工程', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 54, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262682, 55, '考驗', 'VC', None, 'ckip-transformer'),\n",
       " (262682, 56, '著', 'Di', None, 'ckip-transformer'),\n",
       " (262682, 57, '老拜登', 'Nb', None, 'ckip-transformer'),\n",
       " (262682, 58, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262682, 59, '政治', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 60, '智慧', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 61, '；', 'SEMICOLONCATEGORY', None, 'ckip-transformer'),\n",
       " (262682, 62, '拜登', 'VC', None, 'ckip-transformer'),\n",
       " (262682, 63, '稅改', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 64, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262682, 65, '成敗', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 66, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (262682, 67, '更', 'Dfa', None, 'ckip-transformer'),\n",
       " (262682, 68, '攸關', 'VK', None, 'ckip-transformer'),\n",
       " (262682, 69, '全球', 'Nc', None, 'ckip-transformer'),\n",
       " (262682, 70, '稅制', 'Na', None, 'ckip-transformer'),\n",
       " (262682, 71, '發展', 'VC', None, 'ckip-transformer'),\n",
       " (262682, 72, '的', 'DE', None, 'ckip-transformer'),\n",
       " (262682, 73, '未來', 'Nd', None, 'ckip-transformer'),\n",
       " (262682, 74, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (32652, 0, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (32652, 1, '總統', 'Na', None, 'ckip-transformer'),\n",
       " (32652, 2, '當選人', 'Na', None, 'ckip-transformer'),\n",
       " (32652, 3, '拜登', 'VCL', None, 'ckip-transformer'),\n",
       " (32652, 4, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32652, 5, '國家', 'Na', None, 'ckip-transformer'),\n",
       " (32652, 6, '情報', 'Na', None, 'ckip-transformer'),\n",
       " (32652, 7, '總監', 'Na', None, 'ckip-transformer'),\n",
       " (32652, 8, '提名人', 'Na', None, 'ckip-transformer'),\n",
       " (32652, 9, '海恩斯', 'Nb', None, 'ckip-transformer'),\n",
       " (32652, 10, '今天', 'Nd', None, 'ckip-transformer'),\n",
       " (32652, 11, '在', 'P', None, 'ckip-transformer'),\n",
       " (32652, 12, '參議院', 'Nc', None, 'ckip-transformer'),\n",
       " (32652, 13, '聽證會', 'Na', None, 'ckip-transformer'),\n",
       " (32652, 14, '上', 'Ng', None, 'ckip-transformer'),\n",
       " (32652, 15, '表示', 'VE', None, 'ckip-transformer'),\n",
       " (32652, 16, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (32652, 17, '面對', 'VC', None, 'ckip-transformer'),\n",
       " (32652, 18, '好鬥', 'VH', None, 'ckip-transformer'),\n",
       " (32652, 19, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (32652, 20, '強勢', 'Na', None, 'ckip-transformer'),\n",
       " (32652, 21, '獨斷', 'VH', None, 'ckip-transformer'),\n",
       " (32652, 22, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32652, 23, '中國', 'Nc', None, 'ckip-transformer'),\n",
       " (32652, 24, '所', 'D', None, 'ckip-transformer'),\n",
       " (32652, 25, '構成', 'VG', None, 'ckip-transformer'),\n",
       " (32652, 26, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32652, 27, '威脅', 'Na', None, 'ckip-transformer'),\n",
       " (32652, 28, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (32652, 29, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (32652, 30, '應', 'D', None, 'ckip-transformer'),\n",
       " (32652, 31, '採取', 'VC', None, 'ckip-transformer'),\n",
       " (32652, 32, '「', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (32652, 33, '積極', 'VH', None, 'ckip-transformer'),\n",
       " (32652, 34, '進取', 'VH', None, 'ckip-transformer'),\n",
       " (32652, 35, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32652, 36, '立場', 'Na', None, 'ckip-transformer'),\n",
       " (32652, 37, '」', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (32652, 38, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (32653, 0, '海恩斯', 'Nb', None, 'ckip-transformer'),\n",
       " (32653, 1, '（', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (32653, 2, 'Avril', 'FW', None, 'ckip-transformer'),\n",
       " (32653, 3, ' Haines', 'FW', None, 'ckip-transformer'),\n",
       " (32653, 4, '）', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (32653, 5, '也', 'D', None, 'ckip-transformer'),\n",
       " (32653, 6, '表示', 'VE', None, 'ckip-transformer'),\n",
       " (32653, 7, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (32653, 8, '伊朗', 'Nc', None, 'ckip-transformer'),\n",
       " (32653, 9, '距離', 'Na', None, 'ckip-transformer'),\n",
       " (32653, 10, '恢復', 'VHC', None, 'ckip-transformer'),\n",
       " (32653, 11, '嚴格', 'VH', None, 'ckip-transformer'),\n",
       " (32653, 12, '遵守', 'VJ', None, 'ckip-transformer'),\n",
       " (32653, 13, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (32653, 14, '列強', 'Na', None, 'ckip-transformer'),\n",
       " (32653, 15, '達成', 'VC', None, 'ckip-transformer'),\n",
       " (32653, 16, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32653, 17, '核子', 'Na', None, 'ckip-transformer'),\n",
       " (32653, 18, '協議', 'Na', None, 'ckip-transformer'),\n",
       " (32653, 19, '仍', 'D', None, 'ckip-transformer'),\n",
       " (32653, 20, '很', 'Dfa', None, 'ckip-transformer'),\n",
       " (32653, 21, '遙遠', 'VH', None, 'ckip-transformer'),\n",
       " (32653, 22, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (32653, 23, '如果', 'Cbb', None, 'ckip-transformer'),\n",
       " (32653, 24, '伊朗', 'Nc', None, 'ckip-transformer'),\n",
       " (32653, 25, '嚴格', 'VH', None, 'ckip-transformer'),\n",
       " (32653, 26, '遵守', 'VJ', None, 'ckip-transformer'),\n",
       " (32653, 27, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (32653, 28, '拜登', 'Nc', None, 'ckip-transformer'),\n",
       " (32653, 29, '政府', 'Na', None, 'ckip-transformer'),\n",
       " (32653, 30, '可能', 'D', None, 'ckip-transformer'),\n",
       " (32653, 31, '也', 'D', None, 'ckip-transformer'),\n",
       " (32653, 32, '會', 'D', None, 'ckip-transformer'),\n",
       " (32653, 33, '重返', 'VCL', None, 'ckip-transformer'),\n",
       " (32653, 34, '核', 'Na', None, 'ckip-transformer'),\n",
       " (32653, 35, '協議', 'Na', None, 'ckip-transformer'),\n",
       " (32653, 36, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (32653, 37, '即將', 'D', None, 'ckip-transformer'),\n",
       " (32653, 38, '卸任', 'VH', None, 'ckip-transformer'),\n",
       " (32653, 39, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32653, 40, '總統', 'Na', None, 'ckip-transformer'),\n",
       " (32653, 41, '川普', 'Nb', None, 'ckip-transformer'),\n",
       " (32653, 42, '在', 'P', None, 'ckip-transformer'),\n",
       " (32653, 43, '2018年', 'Nd', None, 'ckip-transformer'),\n",
       " (32653, 44, '退出', 'VCL', None, 'ckip-transformer'),\n",
       " (32653, 45, '這', 'Nep', None, 'ckip-transformer'),\n",
       " (32653, 46, '項', 'Nf', None, 'ckip-transformer'),\n",
       " (32653, 47, '核子', 'Na', None, 'ckip-transformer'),\n",
       " (32653, 48, '協議', 'Na', None, 'ckip-transformer'),\n",
       " (32653, 49, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (32654, 0, '曾', 'D', None, 'ckip-transformer'),\n",
       " (32654, 1, '任', 'VG', None, 'ckip-transformer'),\n",
       " (32654, 2, '中央', 'Nc', None, 'ckip-transformer'),\n",
       " (32654, 3, '情報局', 'Nc', None, 'ckip-transformer'),\n",
       " (32654, 4, '（', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (32654, 5, 'CIA', 'FW', None, 'ckip-transformer'),\n",
       " (32654, 6, '）', 'PARENTHESISCATEGORY', None, 'ckip-transformer'),\n",
       " (32654, 7, '副局長', 'Na', None, 'ckip-transformer'),\n",
       " (32654, 8, '和', 'Caa', None, 'ckip-transformer'),\n",
       " (32654, 9, '白宮', 'Nc', None, 'ckip-transformer'),\n",
       " (32654, 10, '助理', 'Na', None, 'ckip-transformer'),\n",
       " (32654, 11, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32654, 12, '海恩斯', 'Nb', None, 'ckip-transformer'),\n",
       " (32654, 13, '說', 'VE', None, 'ckip-transformer'),\n",
       " (32654, 14, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (32654, 15, '她', 'Nh', None, 'ckip-transformer'),\n",
       " (32654, 16, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32654, 17, '優先', 'VH', None, 'ckip-transformer'),\n",
       " (32654, 18, '要務', 'Na', None, 'ckip-transformer'),\n",
       " (32654, 19, '包括', 'VK', None, 'ckip-transformer'),\n",
       " (32654, 20, '恢復', 'VHC', None, 'ckip-transformer'),\n",
       " (32654, 21, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (32654, 22, '情報圈', 'Nc', None, 'ckip-transformer'),\n",
       " (32654, 23, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32654, 24, '信任', 'VJ', None, 'ckip-transformer'),\n",
       " (32654, 25, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (32654, 26, '信心', 'Na', None, 'ckip-transformer'),\n",
       " (32654, 27, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (32654, 28, '以及', 'Caa', None, 'ckip-transformer'),\n",
       " (32654, 29, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (32654, 30, '人民', 'Na', None, 'ckip-transformer'),\n",
       " (32654, 31, '對', 'P', None, 'ckip-transformer'),\n",
       " (32654, 32, '情報圈', 'Nc', None, 'ckip-transformer'),\n",
       " (32654, 33, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32654, 34, '信任', 'VJ', None, 'ckip-transformer'),\n",
       " (32654, 35, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (32654, 36, '信心', 'Na', None, 'ckip-transformer'),\n",
       " (32654, 37, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (32654, 38, '美國', 'Nc', None, 'ckip-transformer'),\n",
       " (32654, 39, '情報圈', 'Nc', None, 'ckip-transformer'),\n",
       " (32654, 40, '有時', 'D', None, 'ckip-transformer'),\n",
       " (32654, 41, '遭到', 'VJ', None, 'ckip-transformer'),\n",
       " (32654, 42, '總統', 'Na', None, 'ckip-transformer'),\n",
       " (32654, 43, '川普', 'Nb', None, 'ckip-transformer'),\n",
       " (32654, 44, '詆毀', 'VC', None, 'ckip-transformer'),\n",
       " (32654, 45, '。', 'PERIODCATEGORY', None, 'ckip-transformer'),\n",
       " (32655, 0, '海恩斯', 'Nb', None, 'ckip-transformer'),\n",
       " (32655, 1, '在', 'P', None, 'ckip-transformer'),\n",
       " (32655, 2, '被', 'P', None, 'ckip-transformer'),\n",
       " (32655, 3, '共和黨', 'Nb', None, 'ckip-transformer'),\n",
       " (32655, 4, '與', 'Caa', None, 'ckip-transformer'),\n",
       " (32655, 5, '民主黨籍', 'Na', None, 'ckip-transformer'),\n",
       " (32655, 6, '參議員', 'Na', None, 'ckip-transformer'),\n",
       " (32655, 7, '追問', 'VE', None, 'ckip-transformer'),\n",
       " (32655, 8, '中國', 'Nc', None, 'ckip-transformer'),\n",
       " (32655, 9, '情報', 'Na', None, 'ckip-transformer'),\n",
       " (32655, 10, '威脅', 'Na', None, 'ckip-transformer'),\n",
       " (32655, 11, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32655, 12, '重要性', 'Na', None, 'ckip-transformer'),\n",
       " (32655, 13, '時', 'Ng', None, 'ckip-transformer'),\n",
       " (32655, 14, '，', 'COMMACATEGORY', None, 'ckip-transformer'),\n",
       " (32655, 15, '表示', 'VE', None, 'ckip-transformer'),\n",
       " (32655, 16, '將', 'P', None, 'ckip-transformer'),\n",
       " (32655, 17, '更多', 'Neqa', None, 'ckip-transformer'),\n",
       " (32655, 18, '資源', 'Na', None, 'ckip-transformer'),\n",
       " (32655, 19, '用於', 'VCL', None, 'ckip-transformer'),\n",
       " (32655, 20, '中國', 'Nc', None, 'ckip-transformer'),\n",
       " (32655, 21, '是', 'SHI', None, 'ckip-transformer'),\n",
       " (32655, 22, '她', 'Nh', None, 'ckip-transformer'),\n",
       " (32655, 23, '的', 'DE', None, 'ckip-transformer'),\n",
       " (32655, 24, '優先', 'VH', None, 'ckip-transformer'),\n",
       " (32655, 25, '要務', 'Na', None, 'ckip-transformer'),\n",
       " (32655, 26, '。', 'PERIODCATEGORY', None, 'ckip-transformer')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_words_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize drivers\n",
    "ws_driver  = CkipWordSegmenter(level=3)\n",
    "pos_driver = CkipPosTagger(level=3)\n",
    "ner_driver = CkipNerChunker(level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"\"\"中國大陸23日共新增80例2019冠狀病毒疾病（COVID-19）確診，其中65例為本土病例。首都北京和經濟大城上海仍不斷有疫情，各新增2例、3例本土確診。\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os, sys\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from db_func import query_from_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = query_from_db(\"\"\"SELECT nns.news_sent_id, nns.sent FROM news_db.news_sents nns \n",
    "WHERE NOT EXISTS (SELECT 1 FROM news_db.sent_processes sp WHERE sp.process_name = 'sent-analysis' and sp.news_sent_id = nns.news_sent_id) LIMIT 100\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']= 'True'\n",
    "\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "# Initialize drivers\n",
    "ws_driver  = CkipWordSegmenter(level=3)\n",
    "pos_driver = CkipPosTagger(level=3)\n",
    "ner_driver = CkipNerChunker(level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1510.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1366.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 832.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2045.00it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  文化部長李永得日前要求審查陸書，讓出版業相當不滿，而事後李永得也找來業者開會尋求配套，不過有業者不滿李永得疑似玩兩面手法，因為開會當下李永得是允諾形式審查，不過事後接受專訪卻又說「反對審查的業者，根本是在台灣，幫中國強權維護他們的威權」，對於質疑，李永得回應是外界抹黑！\n",
      "[['文化部長', '李永得', '日前', '要求', '審查', '陸書', '，', '讓', '出版業', '相當', '不滿', '，', '而', '事', '後', '李永得', '也', '找來', '業者', '開會', '尋求', '配套', '，', '不過', '有', '業者', '不滿', '李永得', '疑似', '玩', '兩', '面', '手法', '，', '因為', '開會', '當下', '李永得', '是', '允諾', '形式', '審查', '，', '不過', '事', '後', '接受', '專訪', '卻', '又', '說', '「', '反對', '審查', '的', '業者', '，', '根本', '是', '在', '台灣', '，', '幫', '中國', '強權', '維護', '他們', '的', '威權', '」', '，', '對於', '質疑', '，', '李永得', '回應', '是', '外界', '抹黑', '！']] [['Na', 'Nb', 'Nd', 'VF', 'VC', 'Na', 'COMMACATEGORY', 'VL', 'Na', 'Dfa', 'VK', 'COMMACATEGORY', 'Cbb', 'Na', 'Ng', 'Nb', 'D', 'VC', 'Na', 'VA', 'VC', 'VA', 'COMMACATEGORY', 'Cbb', 'V_2', 'Na', 'VK', 'Nb', 'VG', 'VC', 'Neu', 'Na', 'Na', 'COMMACATEGORY', 'Cbb', 'VA', 'D', 'Nb', 'SHI', 'VE', 'Na', 'VC', 'COMMACATEGORY', 'Cbb', 'Na', 'Ng', 'VC', 'VC', 'D', 'D', 'VE', 'PARENTHESISCATEGORY', 'VE', 'VC', 'DE', 'Na', 'COMMACATEGORY', 'D', 'SHI', 'P', 'Nc', 'COMMACATEGORY', 'P', 'Nc', 'Na', 'VC', 'Nh', 'DE', 'Na', 'PARENTHESISCATEGORY', 'COMMACATEGORY', 'P', 'VE', 'COMMACATEGORY', 'Nb', 'VC', 'SHI', 'Ncd', 'VC', 'EXCLAMATIONCATEGORY']] [[NerToken(word='文化部長', ner='ORG', idx=(0, 4)), NerToken(word='李永得', ner='PERSON', idx=(4, 7)), NerToken(word='李永得', ner='PERSON', idx=(28, 31)), NerToken(word='李永得', ner='PERSON', idx=(50, 53)), NerToken(word='李永得', ner='PERSON', idx=(67, 70)), NerToken(word='台灣', ner='GPE', idx=(102, 104)), NerToken(word='中國', ner='GPE', idx=(106, 108)), NerToken(word='李永得', ner='PERSON', idx=(124, 127))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1927.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 318.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2906.66it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  台灣文學基地開幕，部長李永得碰上文資團體來陳情，只是部長允諾好好處理的同時，出版業的聲音，是否也能比照辦理，因為李永得要求審查陸書，讓出版業覺得言論自由，被限制！\n",
      "[['台灣', '文學', '基地', '開幕', '，', '部長', '李永得', '碰上', '文資', '團體', '來', '陳情', '，', '只是', '部長', '允諾', '好好', '處理', '的', '同時', '，', '出版業', '的', '聲音', '，', '是否', '也', '能', '比照', '辦理', '，', '因為', '李永得', '要求', '審查', '陸書', '，', '讓', '出版業', '覺得', '言論', '自由', '，', '被', '限制', '！']] [['Nc', 'Na', 'Nc', 'VH', 'COMMACATEGORY', 'Na', 'Nb', 'VC', 'Na', 'Na', 'D', 'VF', 'COMMACATEGORY', 'Cbb', 'Na', 'VE', 'D', 'VC', 'DE', 'Nd', 'COMMACATEGORY', 'Na', 'DE', 'Na', 'COMMACATEGORY', 'D', 'D', 'D', 'P', 'VC', 'COMMACATEGORY', 'Cbb', 'Nb', 'VF', 'VC', 'Na', 'COMMACATEGORY', 'VL', 'Na', 'VK', 'Na', 'VH', 'COMMACATEGORY', 'P', 'VE', 'EXCLAMATIONCATEGORY']] [[NerToken(word='台灣文學基地', ner='FAC', idx=(0, 6)), NerToken(word='李永得', ner='PERSON', idx=(11, 14)), NerToken(word='李永得', ner='PERSON', idx=(56, 59))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1709.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 124.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 788.70it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  文化部長李永得：「(審陸書)本身的法源啊，就在兩岸關係人民條例的，第37條但一直都沒有執行嘛，過去30年(都沒執行)。」\n",
      "[['文化部長', '李永得', '：', '「', '(', '審陸書', ')', '本身', '的', '法源', '啊', '，', '就', '在', '兩岸', '關係', '人民', '條例', '的', '，', '第37', '條', '但', '一直', '都', '沒有', '執行', '嘛', '，', '過去', '30', '年', '(', '都', '沒', '執行', ')', '。', '」']] [['Na', 'Nb', 'COLONCATEGORY', 'PARENTHESISCATEGORY', 'PARENTHESISCATEGORY', 'VC', 'PARENTHESISCATEGORY', 'Nh', 'DE', 'Na', 'T', 'COMMACATEGORY', 'D', 'P', 'Nc', 'Na', 'Na', 'Na', 'T', 'COMMACATEGORY', 'Neu', 'Nf', 'Cbb', 'D', 'D', 'D', 'VC', 'T', 'COMMACATEGORY', 'Nd', 'Neu', 'Nf', 'PARENTHESISCATEGORY', 'D', 'D', 'VC', 'PARENTHESISCATEGORY', 'PERIODCATEGORY', 'PARENTHESISCATEGORY']] [[NerToken(word='文化部長', ner='ORG', idx=(0, 4)), NerToken(word='李永得', ner='PERSON', idx=(4, 7)), NerToken(word='兩岸關係人民條例', ner='LAW', idx=(23, 31)), NerToken(word='第37', ner='ORDINAL', idx=(33, 36)), NerToken(word='過去30年', ner='DATE', idx=(47, 52))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1237.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 197.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2105.57it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  李永得指的，正是這一條，根據民國82年制定兩岸人民關係條例，明確規定大陸地區出版品等，經「主管機關許可，得進入臺灣地區，發行、銷售，但這些年因涉及言論自由，幾乎無人執行，也難怪李永得想力行，出版業大反彈，更質疑原本只說要形式審查的李永得，接受專訪時卻提到，反對審查的業者，根本是在台灣，幫中國強權維護他們的威權，且為何禁止中國出版品會違憲？若大法官會議解釋違憲，那「我下台」，根本是在玩兩面手法！\n",
      "[['李永得', '指', '的', '，', '正', '是', '這', '一', '條', '，', '根據', '民國', '82年', '制定', '兩岸', '人民', '關係', '條例', '，', '明確', '規定', '大陸', '地區', '出版品', '等', '，', '經', '「', '主管', '機關', '許可', '，', '得', '進入', '臺灣', '地區', '，', '發行', '、', '銷售', '，', '但', '這些', '年', '因', '涉及', '言論', '自由', '，', '幾乎', '無', '人', '執行', '，', '也', '難怪', '李永得', '想', '力行', '，', '出版業', '大', '反彈', '，', '更', '質疑', '原本', '只', '說', '要', '形式', '審查', '的', '李永得', '，', '接受', '專訪', '時', '卻', '提到', '，', '反對', '審查', '的', '業者', '，', '根本', '是', '在', '台灣', '，', '幫', '中國', '強權', '維護', '他們', '的', '威權', '，', '且', '為何', '禁止', '中國', '出版品', '會', '違憲', '？', '若', '大法官', '會議', '解釋', '違憲', '，', '那', '「', '我', '下台', '」', '，', '根本', '是', '在', '玩', '兩', '面', '手法', '！']] [['Nb', 'VG', 'DE', 'COMMACATEGORY', 'D', 'SHI', 'Nep', 'Neu', 'Nf', 'COMMACATEGORY', 'P', 'Nd', 'Nd', 'VC', 'Nc', 'Na', 'Na', 'Na', 'COMMACATEGORY', 'VH', 'VE', 'Nc', 'Nc', 'Na', 'Cab', 'COMMACATEGORY', 'P', 'PARENTHESISCATEGORY', 'Na', 'Na', 'VE', 'COMMACATEGORY', 'D', 'VCL', 'Nc', 'Nc', 'COMMACATEGORY', 'VC', 'PAUSECATEGORY', 'VC', 'COMMACATEGORY', 'Cbb', 'Neqa', 'Nf', 'P', 'VK', 'Na', 'Na', 'COMMACATEGORY', 'Da', 'VJ', 'Na', 'VC', 'COMMACATEGORY', 'D', 'D', 'Nb', 'VE', 'VC', 'COMMACATEGORY', 'Na', 'VH', 'VA', 'COMMACATEGORY', 'Dfa', 'VE', 'D', 'Da', 'VE', 'D', 'Na', 'VC', 'DE', 'Nb', 'COMMACATEGORY', 'VC', 'VC', 'Ng', 'D', 'VE', 'COMMACATEGORY', 'VE', 'VC', 'DE', 'Na', 'COMMACATEGORY', 'D', 'SHI', 'P', 'Nc', 'COMMACATEGORY', 'P', 'Nc', 'Na', 'VC', 'Nh', 'DE', 'Na', 'COMMACATEGORY', 'Cbb', 'D', 'VE', 'Nc', 'Na', 'D', 'VA', 'QUESTIONCATEGORY', 'Cbb', 'Na', 'Na', 'VE', 'VA', 'COMMACATEGORY', 'Nep', 'PARENTHESISCATEGORY', 'Nh', 'VA', 'PARENTHESISCATEGORY', 'COMMACATEGORY', 'D', 'SHI', 'P', 'VC', 'Neu', 'Na', 'Na', 'EXCLAMATIONCATEGORY']] [[NerToken(word='李永得', ner='PERSON', idx=(0, 3)), NerToken(word='民國82年', ner='DATE', idx=(14, 19)), NerToken(word='兩岸人民關係條例', ner='LAW', idx=(21, 29)), NerToken(word='大陸', ner='GPE', idx=(34, 36)), NerToken(word='臺灣', ner='GPE', idx=(55, 57)), NerToken(word='這些年', ner='DATE', idx=(67, 70)), NerToken(word='李永得', ner='PERSON', idx=(88, 91)), NerToken(word='李永得', ner='PERSON', idx=(115, 118)), NerToken(word='台灣', ner='GPE', idx=(140, 142)), NerToken(word='中國', ner='GPE', idx=(144, 146)), NerToken(word='中國', ner='GPE', idx=(161, 163)), NerToken(word='兩', ner='CARDINAL', idx=(193, 194))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 711.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2082.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5468.45it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  時報周刊董事長趙振岷：「大家放了吧，如果我擱置爭議，維持現狀，如果說大家就維持過去，22年的做法，大家22年來都這樣嘛，相安無事。」\n",
      "[['時報周刊', '董事長', '趙振岷', '：', '「', '大家', '放', '了', '吧', '，', '如果', '我', '擱置', '爭議', '，', '維持', '現狀', '，', '如果說', '大家', '就', '維持', '過去', '，', '22', '年', '的', '做法', '，', '大家', '22', '年', '來', '都', '這樣', '嘛', '，', '相安無事', '。', '」']] [['Nb', 'Na', 'Nb', 'COLONCATEGORY', 'PARENTHESISCATEGORY', 'Nh', 'VC', 'Di', 'T', 'COMMACATEGORY', 'Cbb', 'Nh', 'VC', 'Na', 'COMMACATEGORY', 'VJ', 'Na', 'COMMACATEGORY', 'Cbb', 'Nh', 'D', 'VJ', 'Nd', 'COMMACATEGORY', 'Neu', 'Nf', 'DE', 'Na', 'COMMACATEGORY', 'Nh', 'Neu', 'Nf', 'Ng', 'D', 'VH', 'T', 'COMMACATEGORY', 'VH', 'PERIODCATEGORY', 'PARENTHESISCATEGORY']] [[NerToken(word='時報周刊', ner='WORK_OF_ART', idx=(0, 4)), NerToken(word='趙振岷', ner='PERSON', idx=(7, 10)), NerToken(word='22年', ner='DATE', idx=(42, 45)), NerToken(word='22年', ner='DATE', idx=(51, 54))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1471.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2087.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4655.17it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  文化部長李永得：「(說兩面手法)就是抹黑嘛，我其實鼓勵台灣成為世界華文出版中心，因為台灣是全世界最自由的出版處。」\n",
      "[['文化部長', '李永得', '：', '「', '(', '說', '兩', '面', '手法', ')', '就是', '抹黑', '嘛', '，', '我', '其實', '鼓勵', '台灣', '成為', '世界', '華文', '出版', '中心', '，', '因為', '台灣', '是', '全', '世界', '最', '自由', '的', '出版處', '。', '」']] [['Na', 'Nb', 'COLONCATEGORY', 'PARENTHESISCATEGORY', 'PARENTHESISCATEGORY', 'VE', 'Neu', 'Na', 'Na', 'PARENTHESISCATEGORY', 'D', 'VC', 'T', 'COMMACATEGORY', 'Nh', 'D', 'VF', 'Nc', 'VG', 'Nc', 'Na', 'VC', 'Nc', 'COMMACATEGORY', 'Cbb', 'Nc', 'SHI', 'Neqa', 'Nc', 'Dfa', 'VH', 'DE', 'Nc', 'PERIODCATEGORY', 'PARENTHESISCATEGORY']] [[NerToken(word='文化部長', ner='ORG', idx=(0, 4)), NerToken(word='李永得', ner='PERSON', idx=(4, 7)), NerToken(word='兩', ner='CARDINAL', idx=(11, 12)), NerToken(word='台灣', ner='GPE', idx=(27, 29)), NerToken(word='華文', ner='LANGUAGE', idx=(33, 35)), NerToken(word='台灣', ner='GPE', idx=(42, 44))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 550.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1524.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1757.88it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  不認自己給出版業扣上紅帽子，但李永得也坦言，現行規定確實該修，畢竟文化部也沒這麼多人力，實質審查，倒是出版業認為，李永得重申法令，有警惕作用！\n",
      "[['不', '認', '自己', '給', '出版業', '扣上', '紅帽子', '，', '但', '李永得', '也', '坦言', '，', '現行', '規定', '確實', '該', '修', '，', '畢竟', '文化部', '也', '沒', '這麼多', '人力', '，', '實質', '審查', '，', '倒是', '出版業', '認為', '，', '李永得', '重申', '法令', '，', '有', '警惕', '作用', '！']] [['D', 'VC', 'Nh', 'P', 'Na', 'VC', 'Na', 'COMMACATEGORY', 'Cbb', 'Nb', 'D', 'VE', 'COMMACATEGORY', 'A', 'Na', 'D', 'D', 'VC', 'COMMACATEGORY', 'D', 'Nc', 'D', 'VJ', 'Neqa', 'Na', 'COMMACATEGORY', 'D', 'VC', 'COMMACATEGORY', 'D', 'Na', 'VE', 'COMMACATEGORY', 'Nb', 'VE', 'Na', 'COMMACATEGORY', 'V_2', 'Na', 'Na', 'EXCLAMATIONCATEGORY']] [[NerToken(word='李永得', ner='PERSON', idx=(15, 18)), NerToken(word='文化部', ner='ORG', idx=(33, 36)), NerToken(word='李永得', ner='PERSON', idx=(57, 60))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1078.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1821.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8630.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2695.57it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  時報出版董事長趙振岷：「就是我們的出版業者，我們的讀者，我們都知道什麼該出，什麼不能出，什麼能買，什麼不能買。」\n",
      "[['時報', '出版', '董事長', '趙振岷', '：', '「', '就是', '我們', '的', '出版', '業者', '，', '我們', '的', '讀者', '，', '我們', '都', '知道', '什麼', '該', '出', '，', '什麼', '不能', '出', '，', '什麼', '能', '買', '，', '什麼', '不能', '買', '。', '」']] [['Nb', 'Nv', 'Na', 'Nb', 'COLONCATEGORY', 'PARENTHESISCATEGORY', 'D', 'Nh', 'DE', 'Nv', 'Na', 'COMMACATEGORY', 'Nh', 'DE', 'Na', 'COMMACATEGORY', 'Nh', 'D', 'VK', 'Nep', 'D', 'VC', 'COMMACATEGORY', 'Nep', 'D', 'VC', 'COMMACATEGORY', 'Nep', 'D', 'VC', 'COMMACATEGORY', 'Nep', 'D', 'VC', 'PERIODCATEGORY', 'PARENTHESISCATEGORY']] [[NerToken(word='時報出版', ner='ORG', idx=(0, 4)), NerToken(word='趙振岷', ner='PERSON', idx=(7, 10))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2811.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1101.16it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  言論自由跟法規的拉扯下，其平衡點難解！\n",
      "[['言論', '自由', '跟', '法規', '的', '拉扯', '下', '，', '其', '平衡點', '難解', '！']] [['Na', 'Na', 'Caa', 'Na', 'DE', 'Nv', 'Ng', 'COMMACATEGORY', 'Nep', 'Na', 'VH', 'EXCLAMATIONCATEGORY']] [[]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1113.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2247.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1446.31it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  針對近日台灣青年民主協會在進行「學生禦寒衣物調查」時發現，有公立學校教官喝令學生「若要依照教育部的規定，請去教育部上課！」的爭議言論，目前是大學生、之前是行政院兒童及少年福利與權益推動小組委員謝有朋感到很痛心！\n",
      "[['針對', '近日', '台灣', '青年', '民主', '協會', '在', '進行', '「', '學生', '禦寒', '衣物', '調查', '」', '時', '發現', '，', '有', '公立', '學校', '教官', '喝令', '學生', '「', '若要', '依照', '教育部', '的', '規定', '，', '請', '去', '教育部', '上課', '！', '」', '的', '爭議', '言論', '，', '目前', '是', '大學生', '、', '之前', '是', '行政院', '兒童', '及', '少年', '福利', '與', '權益', '推動', '小組', '委員', '謝有朋', '感到', '很', '痛心', '！']] [['P', 'Nd', 'Nc', 'Na', 'Na', 'Nc', 'P', 'VC', 'PARENTHESISCATEGORY', 'Na', 'VH', 'Na', 'Na', 'PARENTHESISCATEGORY', 'Ng', 'VE', 'COMMACATEGORY', 'V_2', 'A', 'Nc', 'Na', 'VE', 'Na', 'PARENTHESISCATEGORY', 'Cbb', 'P', 'Nc', 'DE', 'Na', 'COMMACATEGORY', 'VF', 'VCL', 'Nc', 'VA', 'EXCLAMATIONCATEGORY', 'PARENTHESISCATEGORY', 'DE', 'Na', 'Na', 'COMMACATEGORY', 'Nd', 'SHI', 'Na', 'PAUSECATEGORY', 'Nd', 'SHI', 'Nc', 'Na', 'Caa', 'Na', 'Na', 'Caa', 'Na', 'VC', 'Na', 'Na', 'Nb', 'VK', 'Dfa', 'VK', 'EXCLAMATIONCATEGORY']] [[NerToken(word='台灣青年民主協會', ner='ORG', idx=(4, 12)), NerToken(word='學生禦寒衣物調查', ner='EVENT', idx=(16, 24)), NerToken(word='教育部', ner='ORG', idx=(45, 48)), NerToken(word='教育部', ner='ORG', idx=(54, 57)), NerToken(word='行政院兒童及少年福利與權益推動小組', ner='ORG', idx=(77, 94)), NerToken(word='謝有朋', ner='PERSON', idx=(96, 99))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2985.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 260.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1565.04it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  謝有朋說，事發至今，雖然有不少學校已經順應教育部去年的新規，不再對學生的服裝儀容違規約束。不過他還是很感慨：「透過此次的事件，顯示學生為了在校穿著禦寒衣物免受限制是如此得來不易，事件至今已有很多學校逐漸開放學生可以穿著禦寒衣物上學，對此我不知道該感到開心還是難過，痛心的地方在於原來只是想在求學過程當中穿的保暖而已是那麼的困難，那麼的奢侈。」\n",
      "[['謝有朋', '說', '，', '事發', '至今', '，', '雖然', '有', '不少', '學校', '已經', '順應', '教育部', '去年', '的', '新', '規', '，', '不再', '對', '學生', '的', '服裝', '儀容', '違規', '約束', '。', '不過', '他', '還是', '很', '感慨', '：', '「', '透過', '此', '次', '的', '事件', '，', '顯示', '學生', '為了', '在', '校', '穿著', '禦寒', '衣物', '免', '受', '限制', '是', '如此', '得來', '不易', '，', '事件', '至今', '已', '有', '很多', '學校', '逐漸', '開放', '學生', '可以', '穿著', '禦寒', '衣物', '上學', '，', '對', '此', '我', '不', '知道', '該', '感到', '開心', '還是', '難過', '，', '痛心', '的', '地方', '在於', '原來', '只是', '想', '在', '求學', '過程', '當中', '穿', '的', '保暖', '而已', '是', '那麼', '的', '困難', '，', '那麼', '的', '奢侈', '。', '」']] [['Nb', 'VE', 'COMMACATEGORY', 'VH', 'D', 'COMMACATEGORY', 'Cbb', 'V_2', 'Neqa', 'Nc', 'D', 'VJ', 'Nc', 'Nd', 'DE', 'Na', 'Na', 'COMMACATEGORY', 'D', 'P', 'Na', 'DE', 'Na', 'Na', 'VA', 'VC', 'PERIODCATEGORY', 'Cbb', 'Nh', 'D', 'Dfa', 'VE', 'COLONCATEGORY', 'PARENTHESISCATEGORY', 'P', 'Nep', 'Nf', 'DE', 'Na', 'COMMACATEGORY', 'VK', 'Na', 'P', 'P', 'Nc', 'VC', 'VH', 'Na', 'D', 'VJ', 'Na', 'SHI', 'Dfa', 'VI', 'VH', 'COMMACATEGORY', 'Na', 'D', 'D', 'V_2', 'Neqa', 'Nc', 'D', 'VC', 'Na', 'D', 'VC', 'VH', 'Na', 'VA', 'COMMACATEGORY', 'P', 'Nep', 'Nh', 'D', 'VK', 'D', 'VK', 'VH', 'Caa', 'VK', 'COMMACATEGORY', 'VK', 'DE', 'Na', 'VK', 'D', 'D', 'VE', 'P', 'Nv', 'Na', 'Ng', 'VC', 'DE', 'VH', 'T', 'SHI', 'VH', 'DE', 'VH', 'COMMACATEGORY', 'VH', 'DE', 'VH', 'PERIODCATEGORY', 'PARENTHESISCATEGORY']] [[NerToken(word='謝有朋', ner='PERSON', idx=(0, 3)), NerToken(word='教育部', ner='ORG', idx=(21, 24)), NerToken(word='去年', ner='DATE', idx=(24, 26))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 755.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1062.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1608.25it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  謝有朋指出，自己去年12月22日前往行政院參加兒童及少年福利與權益推動小組第四屆第1次會議討論學生服裝儀容管理列管事項時，自己就曾要求教育部，應該對於落實新頒服儀規定原則確實督導，將受申訴學校納入加強查核對象，並研議具體有效罰則，例如處罰涉案人員、減班減招等，才能對不法學校具一定程度之約束力。謝有朋說，教育部應該督導違法之學校就不法之規章進行形式上及實質上的改正，要求該校將相關資料提報教育主管機關並由機關主動審查，絕不可備而不查。\n",
      "[['謝有朋', '指出', '，', '自己', '去年', '12月', '22日', '前往', '行政院', '參加', '兒童', '及', '少年', '福利', '與', '權益', '推動', '小組', '第四', '屆', '第1', '次', '會議', '討論', '學生', '服裝', '儀容', '管理', '列管', '事項', '時', '，', '自己', '就', '曾', '要求', '教育部', '，', '應該', '對於', '落實', '新', '頒', '服儀', '規定', '原則', '確實', '督導', '，', '將', '受', '申訴', '學校', '納入', '加強', '查核', '對象', '，', '並', '研議', '具體', '有效', '罰則', '，', '例如', '處罰', '涉案', '人員', '、', '減班', '減招', '等', '，', '才', '能', '對', '不法', '學校', '具', '一定', '程度', '之', '約束力', '。', '謝有朋', '說', '，', '教育部', '應該', '督導', '違法', '之', '學校', '就', '不法', '之', '規章', '進行', '形式', '上', '及', '實質', '上', '的', '改正', '，', '要求', '該', '校', '將', '相關', '資料', '提報', '教育', '主管', '機關', '並', '由', '機關', '主動', '審查', '，', '絕不', '可', '備而不查', '。']] [['Nb', 'VE', 'COMMACATEGORY', 'Nh', 'Nd', 'Neu', 'Neu', 'VCL', 'Nc', 'VC', 'Na', 'Caa', 'Na', 'Na', 'Caa', 'Na', 'VC', 'Na', 'Neu', 'Nf', 'Neu', 'Nf', 'Na', 'VE', 'Na', 'Na', 'Na', 'VC', 'VB', 'Na', 'Ng', 'COMMACATEGORY', 'Nh', 'D', 'D', 'VF', 'Nc', 'COMMACATEGORY', 'D', 'P', 'VHC', 'VH', 'VD', 'Na', 'Na', 'Na', 'D', 'VC', 'COMMACATEGORY', 'D', 'P', 'VE', 'Nc', 'VJ', 'VC', 'VE', 'Na', 'COMMACATEGORY', 'Cbb', 'VE', 'VH', 'VH', 'Na', 'COMMACATEGORY', 'P', 'VC', 'VA', 'Na', 'PAUSECATEGORY', 'VJ', 'VHC', 'Cab', 'COMMACATEGORY', 'Na', 'D', 'P', 'A', 'Nc', 'VJ', 'A', 'Na', 'DE', 'Na', 'PERIODCATEGORY', 'Nb', 'VE', 'COMMACATEGORY', 'Nc', 'D', 'VC', 'VA', 'DE', 'Nc', 'D', 'A', 'DE', 'Na', 'VC', 'Na', 'Ncd', 'Caa', 'Na', 'Ng', 'DE', 'VC', 'COMMACATEGORY', 'VF', 'Nes', 'Nc', 'P', 'VH', 'Na', 'VC', 'Na', 'Na', 'Na', 'Cbb', 'P', 'Na', 'VH', 'VC', 'COMMACATEGORY', 'D', 'D', 'VH', 'PERIODCATEGORY']] [[NerToken(word='謝有朋', ner='PERSON', idx=(0, 3)), NerToken(word='去年12月22日', ner='DATE', idx=(8, 16)), NerToken(word='行政院', ner='ORG', idx=(18, 21)), NerToken(word='第四', ner='ORDINAL', idx=(37, 39)), NerToken(word='第1', ner='ORDINAL', idx=(40, 42)), NerToken(word='教育部', ner='ORG', idx=(67, 70)), NerToken(word='謝有朋', ner='PERSON', idx=(147, 150)), NerToken(word='教育部', ner='ORG', idx=(152, 155))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1414.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1251.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1558.06it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  至於那些仍然堅持一意孤行的不法學校，兒少委員謝有朋也說：「期待他們（不法學校）回頭是岸，也請社會大眾特別是學生可以勇敢舉發，或許改變並不是一蹴可幾，但我知道如果此刻我們不勇敢，這一切就不會有所改變，在學生權益保障的議題上，我們沒有人是局外人，期待校園人權走向黎明的那日。」\n",
      "[['至於', '那些', '仍然', '堅持', '一意孤行', '的', '不法', '學校', '，', '兒少', '委員', '謝有朋', '也', '說', '：', '「', '期待', '他們', '（', '不法', '學校', '）', '回頭是岸', '，', '也', '請', '社會', '大眾', '特別', '是', '學生', '可以', '勇敢', '舉發', '，', '或許', '改變', '並', '不', '是', '一蹴可幾', '，', '但', '我', '知道', '如果', '此刻', '我們', '不', '勇敢', '，', '這', '一切', '就', '不會', '有所', '改變', '，', '在', '學生', '權益', '保障', '的', '議題', '上', '，', '我們', '沒有', '人', '是', '局外人', '，', '期待', '校園', '人權', '走向', '黎明', '的', '那', '日', '。', '」']] [['P', 'Neqa', 'D', 'VK', 'VA', 'DE', 'A', 'Nc', 'COMMACATEGORY', 'Na', 'Na', 'Nb', 'D', 'VE', 'COLONCATEGORY', 'PARENTHESISCATEGORY', 'VK', 'Nh', 'PARENTHESISCATEGORY', 'A', 'Nc', 'PARENTHESISCATEGORY', 'VH', 'COMMACATEGORY', 'D', 'VF', 'Na', 'Nh', 'VH', 'SHI', 'Na', 'D', 'VH', 'VC', 'COMMACATEGORY', 'D', 'VC', 'D', 'D', 'SHI', 'VH', 'COMMACATEGORY', 'Cbb', 'Nh', 'VK', 'Cbb', 'Nd', 'Nh', 'D', 'VH', 'COMMACATEGORY', 'Nep', 'Neqa', 'D', 'D', 'VJ', 'VC', 'COMMACATEGORY', 'P', 'Na', 'Na', 'Na', 'DE', 'Na', 'Ng', 'COMMACATEGORY', 'Nh', 'VJ', 'Na', 'SHI', 'Na', 'COMMACATEGORY', 'VK', 'Nc', 'Na', 'VCL', 'Nd', 'DE', 'Nep', 'Nf', 'PERIODCATEGORY', 'PARENTHESISCATEGORY']] [[NerToken(word='謝有朋', ner='PERSON', idx=(22, 25))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 654.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 613.11it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  上述對於教育部所提服裝儀容管理列管事項報告建議，已經在今（18日）於行政院官網中公布，當時教育部曾經允諾將持續督導各縣市精進，並於下次會議報告落實情形。\n",
      "[['上述', '對於', '教育部', '所', '提', '服裝', '儀容', '管理', '列管', '事項', '報告', '建議', '，', '已經', '在', '今', '（', '18日', '）', '於', '行政院', '官網', '中', '公布', '，', '當時', '教育部', '曾經', '允諾', '將', '持續', '督導', '各', '縣市', '精進', '，', '並', '於', '下', '次', '會議', '報告', '落實', '情形', '。']] [['Na', 'P', 'Nc', 'D', 'VE', 'Na', 'Na', 'VC', 'VB', 'Na', 'Na', 'VE', 'COMMACATEGORY', 'D', 'P', 'Nd', 'PARENTHESISCATEGORY', 'Neu', 'PARENTHESISCATEGORY', 'P', 'Nc', 'Na', 'Ng', 'VE', 'COMMACATEGORY', 'Nd', 'Nc', 'D', 'VE', 'D', 'VL', 'VC', 'Nes', 'Nc', 'VH', 'COMMACATEGORY', 'Cbb', 'P', 'Nes', 'Nf', 'Na', 'Na', 'VHC', 'Na', 'PERIODCATEGORY']] [[NerToken(word='教育部', ner='ORG', idx=(4, 7)), NerToken(word='今（18日', ner='DATE', idx=(27, 32)), NerToken(word='行政院', ner='ORG', idx=(34, 37)), NerToken(word='教育部', ner='ORG', idx=(45, 48))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1246.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1073.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 459.35it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  新北市本來想向中央申請前瞻預算興建4座停車場，沒想到卻卡關，因為10億元的經費撥給桃園和台南，就這麼巧剛好都是綠營執政縣市，就有立委質疑難道是政治考量嗎？公路總局則澄清，預算都是經過審議小組討論，而且如果加上前一次，新北市是目前全台拿最多的！\n",
      "[['新北市', '本來', '想', '向', '中央', '申請', '前瞻', '預算', '興建', '4', '座', '停車場', '，', '沒想到', '卻', '卡關', '，', '因為', '10億', '元', '的', '經費', '撥給', '桃園', '和', '台南', '，', '就', '這麼', '巧', '剛好', '都', '是', '綠營', '執政', '縣市', '，', '就', '有', '立委', '質疑', '難道', '是', '政治', '考量', '嗎', '？', '公路總局', '則', '澄清', '，', '預算', '都', '是', '經過', '審議', '小組', '討論', '，', '而且', '如果', '加上', '前', '一', '次', '，', '新北市', '是', '目前', '全', '台', '拿', '最多', '的', '！']] [['VH', 'D', 'VE', 'P', 'Nc', 'VF', 'VC', 'Na', 'VC', 'Neu', 'Nf', 'Nc', 'COMMACATEGORY', 'D', 'D', 'VA', 'COMMACATEGORY', 'Cbb', 'Neu', 'Nf', 'DE', 'Na', 'VD', 'Nc', 'Caa', 'Nc', 'COMMACATEGORY', 'D', 'Dfa', 'VH', 'Da', 'D', 'SHI', 'Nb', 'VA', 'Nc', 'COMMACATEGORY', 'D', 'V_2', 'Na', 'VE', 'D', 'SHI', 'Na', 'VE', 'T', 'QUESTIONCATEGORY', 'Na', 'D', 'VE', 'COMMACATEGORY', 'Na', 'D', 'SHI', 'VCL', 'VC', 'Na', 'VE', 'COMMACATEGORY', 'Cbb', 'Cbb', 'Cbb', 'Nes', 'Neu', 'Nf', 'COMMACATEGORY', 'Nc', 'SHI', 'Nd', 'Neqa', 'Nc', 'VC', 'VH', 'T', 'EXCLAMATIONCATEGORY']] [[NerToken(word='新北市', ner='GPE', idx=(0, 3)), NerToken(word='4', ner='CARDINAL', idx=(17, 18)), NerToken(word='10億元', ner='MONEY', idx=(32, 36)), NerToken(word='桃園', ner='GPE', idx=(41, 43)), NerToken(word='台南', ner='GPE', idx=(44, 46)), NerToken(word='公路總局', ner='ORG', idx=(77, 81)), NerToken(word='新北市', ner='GPE', idx=(108, 111)), NerToken(word='台', ner='GPE', idx=(115, 116))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3223.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 475.28it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2242.94it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  新北市長侯友宜，親自勉勵第一線醫護人員，不過自家市政恐怕有讓他更頭疼的，因為向中央申請前瞻2.0預算，想興建4個停車場可能卡關！\n",
      "[['新北', '市長', '侯友宜', '，', '親自', '勉勵', '第一線', '醫護', '人員', '，', '不過', '自家', '市政', '恐怕', '有', '讓', '他', '更', '頭疼', '的', '，', '因為', '向', '中央', '申請', '前瞻', '2.0', '預算', '，', '想', '興建', '4', '個', '停車場', '可能', '卡關', '！']] [['Na', 'Na', 'Nb', 'COMMACATEGORY', 'D', 'VC', 'Nc', 'A', 'Na', 'COMMACATEGORY', 'Cbb', 'Nc', 'Na', 'D', 'V_2', 'VL', 'Nh', 'Dfa', 'VH', 'T', 'COMMACATEGORY', 'Cbb', 'P', 'Nc', 'VF', 'VC', 'Neu', 'Na', 'COMMACATEGORY', 'VE', 'VC', 'Neu', 'Nf', 'Nc', 'D', 'VA', 'EXCLAMATIONCATEGORY']] [[NerToken(word='新北', ner='GPE', idx=(0, 2)), NerToken(word='侯友宜', ner='PERSON', idx=(4, 7)), NerToken(word='第一', ner='ORDINAL', idx=(12, 14)), NerToken(word='4', ner='CARDINAL', idx=(54, 55))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2487.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2880.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2113.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1432.48it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  新北市長侯友宜：「政府如果多給我們新北市，給我們加油打氣，給我們一點資源我都非常感恩。」\n",
      "[['新北', '市長', '侯友宜', '：', '「', '政府', '如果', '多', '給', '我們', '新北市', '，', '給', '我們', '加油', '打氣', '，', '給', '我們', '一點', '資源', '我', '都', '非常', '感恩', '。', '」']] [['Nc', 'Na', 'Nb', 'COLONCATEGORY', 'PARENTHESISCATEGORY', 'Na', 'Cbb', 'D', 'VD', 'Nh', 'Nc', 'COMMACATEGORY', 'VD', 'Nh', 'VB', 'VB', 'COMMACATEGORY', 'VD', 'Nh', 'Neqa', 'Na', 'Nh', 'D', 'Dfa', 'VI', 'PERIODCATEGORY', 'PARENTHESISCATEGORY']] [[NerToken(word='新北', ner='GPE', idx=(0, 2)), NerToken(word='侯友宜', ner='PERSON', idx=(4, 7)), NerToken(word='新北市', ner='GPE', idx=(17, 20))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2054.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1412.22it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  喊話中央，畢竟錢如果討無，就得靠自己自立自強，只是就這麼巧，這回拿到補助款的都是綠營執政縣市。\n",
      "[['喊話', '中央', '，', '畢竟', '錢', '如果', '討', '無', '，', '就', '得', '靠', '自己', '自立自強', '，', '只是', '就', '這麼', '巧', '，', '這', '回', '拿到', '補助款', '的', '都', '是', '綠營', '執政', '縣市', '。']] [['VA', 'Nc', 'COMMACATEGORY', 'D', 'Na', 'Cbb', 'VD', 'VJ', 'COMMACATEGORY', 'D', 'D', 'P', 'Nh', 'VH', 'COMMACATEGORY', 'Cbb', 'D', 'Dfa', 'VH', 'COMMACATEGORY', 'Nep', 'Nf', 'VC', 'Na', 'DE', 'D', 'SHI', 'Nb', 'VA', 'Nc', 'PERIODCATEGORY']] [[NerToken(word='中央', ner='ORG', idx=(2, 4))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1369.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1791.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1402.31it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  記者李作珩：「像是我現在在的三重玫瑰停車場，原本要改成立體式，可以容納更多的車輛，不過現在因為中央預算關係卡關，而拿到補助的分別是桃園和台南。」\n",
      "[['記者', '李作珩', '：', '「', '像是', '我', '現在', '在', '的', '三重', '玫瑰', '停車場', '，', '原本', '要', '改成', '立體式', '，', '可以', '容納', '更多', '的', '車輛', '，', '不過', '現在', '因為', '中央', '預算', '關係', '卡關', '，', '而', '拿到', '補助', '的', '分別', '是', '桃園', '和', '台南', '。', '」']] [['Na', 'Nb', 'COLONCATEGORY', 'PARENTHESISCATEGORY', 'P', 'Nh', 'Nd', 'VCL', 'DE', 'Nc', 'Na', 'Nc', 'COMMACATEGORY', 'D', 'D', 'VG', 'A', 'COMMACATEGORY', 'D', 'VJ', 'Neqa', 'DE', 'Na', 'COMMACATEGORY', 'Cbb', 'Nd', 'Cbb', 'Nc', 'Na', 'Na', 'VA', 'COMMACATEGORY', 'Cbb', 'VC', 'VD', 'DE', 'D', 'SHI', 'Nc', 'Caa', 'Nc', 'PERIODCATEGORY', 'PARENTHESISCATEGORY']] [[NerToken(word='李作珩', ner='PERSON', idx=(2, 5)), NerToken(word='三重玫瑰停車場', ner='FAC', idx=(14, 21)), NerToken(word='中央', ner='ORG', idx=(47, 49)), NerToken(word='桃園', ner='GPE', idx=(65, 67)), NerToken(word='台南', ner='GPE', idx=(68, 70))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1233.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1811.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2090.88it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  前瞻2.0經費約10億元，補助對象分別是桃園中正公園、台南永康、台南北區以及桃園區文化局，這樣算下來10億元用得差不多，新北的永平國小、板橋海山高中、永平立體、三重玫瑰停車場，順列排在第5、6、7、9但明明這幾個地方也有不少人居住，究竟核發標準是甚麼？\n",
      "[['前瞻', '2.0', '經費', '約', '10億', '元', '，', '補助', '對象', '分別', '是', '桃園', '中正', '公園', '、', '台南', '永康', '、', '台南', '北區', '以及', '桃園區', '文化局', '，', '這樣', '算下來', '10億', '元', '用', '得', '差不多', '，', '新北', '的', '永平', '國小', '、', '板橋', '海山', '高中', '、', '永平', '立體', '、', '三重', '玫瑰', '停車場', '，', '順列', '排', '在', '第5', '、', '6', '、', '7', '、', '9', '但', '明明', '這', '幾', '個', '地方', '也', '有', '不少', '人', '居住', '，', '究竟', '核發', '標準', '是', '甚麼', '？']] [['VC', 'Neu', 'Na', 'Da', 'Neu', 'Nf', 'COMMACATEGORY', 'VD', 'Na', 'D', 'SHI', 'Nc', 'Nb', 'Nc', 'PAUSECATEGORY', 'Nc', 'Nc', 'PAUSECATEGORY', 'Nc', 'Nc', 'Caa', 'Nc', 'Nc', 'COMMACATEGORY', 'VH', 'VG', 'Neu', 'Nf', 'VC', 'DE', 'VH', 'COMMACATEGORY', 'Nc', 'DE', 'Nc', 'Nc', 'PAUSECATEGORY', 'Nc', 'Nc', 'Nc', 'PAUSECATEGORY', 'Nc', 'VH', 'PAUSECATEGORY', 'Nc', 'Na', 'Nc', 'COMMACATEGORY', 'D', 'VG', 'P', 'Neu', 'PAUSECATEGORY', 'Neu', 'PAUSECATEGORY', 'Neu', 'PAUSECATEGORY', 'Neu', 'Cbb', 'D', 'Nep', 'Neu', 'Nf', 'Na', 'D', 'V_2', 'Neqa', 'Na', 'VA', 'COMMACATEGORY', 'D', 'VC', 'Na', 'SHI', 'Nep', 'QUESTIONCATEGORY']] [[NerToken(word='10億元', ner='MONEY', idx=(8, 12)), NerToken(word='桃園中正公園', ner='FAC', idx=(20, 26)), NerToken(word='台南永康', ner='GPE', idx=(27, 31)), NerToken(word='台南', ner='GPE', idx=(32, 34)), NerToken(word='桃園區文化局', ner='ORG', idx=(38, 44)), NerToken(word='10億元', ner='MONEY', idx=(50, 54)), NerToken(word='新北', ner='GPE', idx=(60, 62)), NerToken(word='三重玫瑰停車場', ner='FAC', idx=(80, 87)), NerToken(word='第5', ner='ORDINAL', idx=(92, 94)), NerToken(word='6', ner='CARDINAL', idx=(95, 96)), NerToken(word='7', ner='CARDINAL', idx=(97, 98))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1242.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 859.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2410.52it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:  立委(國)洪孟楷：「新北市是403萬人口，也是人口數最多的一個直轄市，卻被排在5、6、7、8(9)名，這難免會讓大家覺得說，奇怪這個分配上面是不是有所不均。」\n",
      "[['立委', '(', '國', ')', '洪孟楷', '：', '「', '新北市', '是', '403萬', '人口', '，', '也', '是', '人口數', '最多', '的', '一', '個', '直轄市', '，', '卻', '被', '排', '在', '5', '、', '6', '、', '7', '、', '8', '(', '9)', '名', '，', '這', '難免', '會', '讓', '大家', '覺得', '說', '，', '奇怪', '這', '個', '分配', '上面', '是', '不', '是', '有所', '不均', '。', '」']] [['Na', 'PARENTHESISCATEGORY', 'Nc', 'PARENTHESISCATEGORY', 'Nb', 'COLONCATEGORY', 'PARENTHESISCATEGORY', 'Nc', 'SHI', 'Neu', 'Na', 'COMMACATEGORY', 'D', 'SHI', 'Na', 'VH', 'DE', 'Neu', 'Nf', 'Na', 'COMMACATEGORY', 'D', 'P', 'VC', 'P', 'Neu', 'PAUSECATEGORY', 'Neu', 'PAUSECATEGORY', 'Neu', 'PAUSECATEGORY', 'Neu', 'PARENTHESISCATEGORY', 'Neu', 'Nf', 'COMMACATEGORY', 'Nep', 'D', 'D', 'VL', 'Nh', 'VK', 'VE', 'COMMACATEGORY', 'VK', 'Nep', 'Nf', 'VD', 'Ncd', 'SHI', 'D', 'SHI', 'VJ', 'VH', 'PERIODCATEGORY', 'PARENTHESISCATEGORY']] [[NerToken(word='洪孟楷', ner='PERSON', idx=(5, 8)), NerToken(word='新北市', ner='GPE', idx=(10, 13)), NerToken(word='403萬', ner='CARDINAL', idx=(14, 18)), NerToken(word='一', ner='CARDINAL', idx=(29, 30)), NerToken(word='5', ner='CARDINAL', idx=(39, 40)), NerToken(word='6', ner='CARDINAL', idx=(41, 42)), NerToken(word='7', ner='CARDINAL', idx=(43, 44)), NerToken(word='8', ner='CARDINAL', idx=(45, 46)), NerToken(word='9', ner='CARDINAL', idx=(47, 48))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2036.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 739.74it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a06e16fba383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Disable sentence segmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sent: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/ckip_transformers/nlp/driver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_text, use_delim, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mindex_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         ) = super().__call__(input_text, use_delim=use_delim, **kwargs)\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Get labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/ckip_transformers/nlp/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_text, use_delim, delim_set, batch_size, max_length, show_progress)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 (\n\u001b[1;32m    187\u001b[0m                     \u001b[0mbatch_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 ) = self.model(**dict(zip(encoded_input.keys(), batch)), return_dict=False)\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0mbatch_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Remove [CLS]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1672\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1675\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         )\n\u001b[0;32m--> 966\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 )\n\u001b[1;32m    566\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    568\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1785\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/analyzer/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, (sent_id, sent) in raw_df.iterrows():\n",
    "    # Enable sentence segmentation\n",
    "    ws  = ws_driver([sent], use_delim=False)\n",
    "    ner = ner_driver([sent], use_delim=False)\n",
    "\n",
    "    # Disable sentence segmentation\n",
    "    pos = pos_driver(ws, use_delim=False)\n",
    "    print('Sent: ', sent)\n",
    "    print(ws, pos, ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 352.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1062.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 387.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "ws  = ws_driver(text, use_delim=False)\n",
    "ner = ner_driver(text, use_delim=False)\n",
    "\n",
    "# Disable sentence segmentation\n",
    "pos = pos_driver(ws, use_delim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NerToken(word='中國大陸', ner='GPE', idx=(0, 4)), NerToken(word='23日', ner='DATE', idx=(4, 7)), NerToken(word='80', ner='CARDINAL', idx=(10, 12)), NerToken(word='2019冠狀病毒疾病', ner='EVENT', idx=(13, 23)), NerToken(word='65', ner='CARDINAL', idx=(38, 40)), NerToken(word='北京', ner='GPE', idx=(49, 51)), NerToken(word='上海', ner='GPE', idx=(56, 58)), NerToken(word='2', ner='CARDINAL', idx=(68, 69)), NerToken(word='3', ner='CARDINAL', idx=(71, 72))]\n"
     ]
    }
   ],
   "source": [
    "for word, ner[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 145.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['中國',\n",
       "  '大陸',\n",
       "  '23日',\n",
       "  '共',\n",
       "  '新增',\n",
       "  '80',\n",
       "  '例',\n",
       "  '2019',\n",
       "  '冠狀病毒',\n",
       "  '疾病',\n",
       "  '（',\n",
       "  'COVID-19',\n",
       "  '）',\n",
       "  '確診',\n",
       "  '，',\n",
       "  '其中',\n",
       "  '65',\n",
       "  '例',\n",
       "  '為',\n",
       "  '本土',\n",
       "  '病例',\n",
       "  '。',\n",
       "  '首都',\n",
       "  '北京',\n",
       "  '和',\n",
       "  '經濟',\n",
       "  '大城',\n",
       "  '上海',\n",
       "  '仍',\n",
       "  '不斷',\n",
       "  '有',\n",
       "  '疫情',\n",
       "  '，',\n",
       "  '各',\n",
       "  '新增',\n",
       "  '2',\n",
       "  '例',\n",
       "  '、',\n",
       "  '3',\n",
       "  '例',\n",
       "  '本土',\n",
       "  '確診',\n",
       "  '。']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_driver(text, use_delim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import spacy\n",
    "nlp = spacy.load(\"zh_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中國大陸 GPE\n",
      "23日 DATE\n",
      "80 CARDINAL\n",
      "2019冠狀病毒疾病 EVENT\n",
      "65 CARDINAL\n",
      "北京 GPE\n",
      "上海 GPE\n",
      "2例 CARDINAL\n",
      "3例 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "for ent in nlp(text[0]).ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中國 nmod:assmod NR\n",
      "大陸 nsubj NN\n",
      "23日 nmod:tmod NT\n",
      "共 advmod AD\n",
      "新增 ROOT VV\n",
      "80 nummod CD\n",
      "例 mark:clf M\n",
      "2019 compound:nn NT\n",
      "冠狀 compound:nn NN\n",
      "病毒 compound:nn NN\n",
      "疾病 dep NN\n",
      "（ punct PU\n",
      "COVID parataxis:prnmod PU\n",
      "- dobj NR\n",
      "19 parataxis:prnmod NT\n",
      "） punct PU\n",
      "確診 ccomp VV\n",
      "， punct PU\n",
      "其中 dep NN\n",
      "65 dep CD\n",
      "例為 cop M\n",
      "本土 compound:nn NN\n",
      "病例 conj NN\n",
      "。 punct PU\n",
      "首都 appos NN\n",
      "北京 conj NR\n",
      "和 cc CC\n",
      "經濟 compound:nn NN\n",
      "大城 appos NN\n",
      "上海 nsubj NR\n",
      "仍 advmod AD\n",
      "不 advmod AD\n",
      "斷 advmod AD\n",
      "有 ROOT VE\n",
      "疫情 dobj NN\n",
      "， punct PU\n",
      "各 advmod AD\n",
      "新增 conj VV\n",
      "2例 dep M\n",
      "、 punct PU\n",
      "3例 mark:clf M\n",
      "本土 dep NN\n",
      "確診 dobj VV\n",
      "。 punct PU\n"
     ]
    }
   ],
   "source": [
    "for token in nlp(text[0]):\n",
    "    print(token, token.dep_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = \"\"\"\n",
    "'ETtoday','財經新聞'\n",
    "'PCHOME','財經'\n",
    "'Yahoo奇摩新聞','財經新聞'\n",
    "'Yahoo奇摩新聞','運動新聞'\n",
    "'Yahoo奇摩股市','台股動態'\n",
    "'Yahoo奇摩股市','國際財經'\n",
    "'Yahoo奇摩股市','基金動態'\n",
    "'Yahoo奇摩股市','專家專欄'\n",
    "'Yahoo奇摩股市','小資理財'\n",
    "'Yahoo奇摩股市','最新股市'\n",
    "'Yahoo奇摩股市','研究報導'\n",
    "'中央社即時新聞','財經新聞'\n",
    "'台視新聞','財經'\n",
    "'新浪台灣新聞中心','財經'\n",
    "'新頭殼要聞','財經'\n",
    "'民報 Taiwan People News','財經新聞'\n",
    "'經濟日報','2021全球名家瞭望'\n",
    "'經濟日報','2021投資前瞻'\n",
    "'經濟日報','2021新春財經特輯'\n",
    "'經濟日報','ECFA十周年'\n",
    "'經濟日報','ESG'\n",
    "'經濟日報','Fintech'\n",
    "'經濟日報','Focus |防疫國家隊'\n",
    "'經濟日報','ICT趨勢'\n",
    "'經濟日報','K型復甦'\n",
    "'經濟日報','一分鐘看世界'\n",
    "'經濟日報','上市櫃公司熱門股排行'\n",
    "'經濟日報','五金新訊'\n",
    "'經濟日報','企業CEO'\n",
    "'經濟日報','個人理財'\n",
    "'經濟日報','健康元氣'\n",
    "'經濟日報','光電半導體'\n",
    "'經濟日報','兩岸快遞'\n",
    "'經濟日報','兩岸焦點'\n",
    "'經濟日報','公司治理'\n",
    "'經濟日報','創業之星'\n",
    "'經濟日報','化工科技'\n",
    "'經濟日報','十四五突圍'\n",
    "'經濟日報','台中韓大戰'\n",
    "'經濟日報','品味生活'\n",
    "'經濟日報','國內共同基金淨值'\n",
    "'經濟日報','國際期貨'\n",
    "'經濟日報','國際焦點'\n",
    "'經濟日報','國際現場'\n",
    "'經濟日報','基金天地'\n",
    "'經濟日報','外匯市場'\n",
    "'經濟日報','大數字'\n",
    "'經濟日報','就市論勢'\n",
    "'經濟日報','工具機'\n",
    "'經濟日報','市場焦點'\n",
    "'經濟日報','幸福城市大調查'\n",
    "'經濟日報','建材新訊'\n",
    "'經濟日報','建案開箱'\n",
    "'經濟日報','房市情報'\n",
    "'經濟日報','房市焦點'\n",
    "'經濟日報','房市點線面'\n",
    "'經濟日報','房產投資'\n",
    "'經濟日報','投行看大陸'\n",
    "'經濟日報','投資報告'\n",
    "'經濟日報','指標焦點股'\n",
    "'經濟日報','政經焦點'\n",
    "'經濟日報','新冠肺炎防疫'\n",
    "'經濟日報','新南向專題'\n",
    "'經濟日報','日經中文網'\n",
    "'經濟日報','時尚生活'\n",
    "'經濟日報','智慧製造'\n",
    "'經濟日報','智能股市'\n",
    "'經濟日報','期貨商論壇'\n",
    "'經濟日報','期貨市場'\n",
    "'經濟日報','樂活旅遊'\n",
    "'經濟日報','櫃買市場'\n",
    "'經濟日報','權證特區'\n",
    "'經濟日報','消費生活'\n",
    "'經濟日報','熱門亮點'\n",
    "'經濟日報','熱門話題'\n",
    "'經濟日報','理財部落客'\n",
    "'經濟日報','生技醫藥'\n",
    "'經濟日報','產學研訓'\n",
    "'經濟日報','產業動態'\n",
    "'經濟日報','產業熱點'\n",
    "'經濟日報','產業達人'\n",
    "'經濟日報','社論'\n",
    "'經濟日報','科技新視野'\n",
    "'經濟日報','稅務法規'\n",
    "'經濟日報','精品時尚'\n",
    "'經濟日報','經濟周報'\n",
    "'經濟日報','經營管理'\n",
    "'經濟日報','綠色產業'\n",
    "'經濟日報','總經趨勢'\n",
    "'經濟日報','總編推薦'\n",
    "'經濟日報','美中貿易戰'\n",
    "'經濟日報','美國總統大選'\n",
    "'經濟日報','職場風向'\n",
    "'經濟日報','自動化產業'\n",
    "'經濟日報','英語大進化'\n",
    "'經濟日報','論壇'\n",
    "'經濟日報','財富管理'\n",
    "'經濟日報','責任投資／企業永續'\n",
    "'經濟日報','資訊圖表'\n",
    "'經濟日報','退休理財'\n",
    "'經濟日報','運動休旅'\n",
    "'經濟日報','醫藥生技'\n",
    "'經濟日報','金融脈動'\n",
    "'經濟日報','銀行保險'\n",
    "'經濟日報','陸港行情'\n",
    "'經濟日報','集中市場'\n",
    "'經濟日報','願景工程'\n",
    "'經濟日報','風格人物'\n",
    "'經濟日報','食品餐飲'\n",
    "'經濟日報','食尚饗宴'\n",
    "'自由時報','財經新聞'\n",
    "'風傳媒','財經'\n",
    "\"\"\".split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_category_lst = []\n",
    "for lst in source_list:\n",
    "    if lst:\n",
    "        source_category_lst.append([element.strip(\"'\") for element in lst.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ''\n",
    "for source, category in source_category_lst:\n",
    "    res += \"OR (nrf.news_source = '{}' AND nrf.news_category = '{}')\".format(source, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OR (nrf.news_source = 'ETtoday' AND nrf.news_category = '財經新聞')OR (nrf.news_source = 'PCHOME' AND nrf.news_category = '財經')OR (nrf.news_source = 'Yahoo奇摩新聞' AND nrf.news_category = '財經新聞')OR (nrf.news_source = 'Yahoo奇摩新聞' AND nrf.news_category = '運動新聞')OR (nrf.news_source = 'Yahoo奇摩股市' AND nrf.news_category = '台股動態')OR (nrf.news_source = 'Yahoo奇摩股市' AND nrf.news_category = '國際財經')OR (nrf.news_source = 'Yahoo奇摩股市' AND nrf.news_category = '基金動態')OR (nrf.news_source = 'Yahoo奇摩股市' AND nrf.news_category = '專家專欄')OR (nrf.news_source = 'Yahoo奇摩股市' AND nrf.news_category = '小資理財')OR (nrf.news_source = 'Yahoo奇摩股市' AND nrf.news_category = '最新股市')OR (nrf.news_source = 'Yahoo奇摩股市' AND nrf.news_category = '研究報導')OR (nrf.news_source = '中央社即時新聞' AND nrf.news_category = '財經新聞')OR (nrf.news_source = '台視新聞' AND nrf.news_category = '財經')OR (nrf.news_source = '新浪台灣新聞中心' AND nrf.news_category = '財經')OR (nrf.news_source = '新頭殼要聞' AND nrf.news_category = '財經')OR (nrf.news_source = '民報 Taiwan People News' AND nrf.news_category = '財經新聞')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '2021全球名家瞭望')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '2021投資前瞻')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '2021新春財經特輯')OR (nrf.news_source = '經濟日報' AND nrf.news_category = 'ECFA十周年')OR (nrf.news_source = '經濟日報' AND nrf.news_category = 'ESG')OR (nrf.news_source = '經濟日報' AND nrf.news_category = 'Fintech')OR (nrf.news_source = '經濟日報' AND nrf.news_category = 'Focus |防疫國家隊')OR (nrf.news_source = '經濟日報' AND nrf.news_category = 'ICT趨勢')OR (nrf.news_source = '經濟日報' AND nrf.news_category = 'K型復甦')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '一分鐘看世界')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '上市櫃公司熱門股排行')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '五金新訊')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '企業CEO')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '個人理財')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '健康元氣')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '光電半導體')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '兩岸快遞')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '兩岸焦點')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '公司治理')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '創業之星')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '化工科技')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '十四五突圍')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '台中韓大戰')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '品味生活')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '國內共同基金淨值')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '國際期貨')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '國際焦點')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '國際現場')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '基金天地')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '外匯市場')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '大數字')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '就市論勢')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '工具機')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '市場焦點')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '幸福城市大調查')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '建材新訊')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '建案開箱')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '房市情報')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '房市焦點')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '房市點線面')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '房產投資')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '投行看大陸')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '投資報告')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '指標焦點股')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '政經焦點')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '新冠肺炎防疫')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '新南向專題')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '日經中文網')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '時尚生活')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '智慧製造')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '智能股市')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '期貨商論壇')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '期貨市場')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '樂活旅遊')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '櫃買市場')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '權證特區')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '消費生活')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '熱門亮點')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '熱門話題')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '理財部落客')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '生技醫藥')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '產學研訓')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '產業動態')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '產業熱點')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '產業達人')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '社論')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '科技新視野')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '稅務法規')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '精品時尚')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '經濟周報')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '經營管理')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '綠色產業')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '總經趨勢')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '總編推薦')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '美中貿易戰')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '美國總統大選')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '職場風向')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '自動化產業')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '英語大進化')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '論壇')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '財富管理')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '責任投資／企業永續')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '資訊圖表')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '退休理財')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '運動休旅')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '醫藥生技')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '金融脈動')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '銀行保險')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '陸港行情')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '集中市場')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '願景工程')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '風格人物')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '食品餐飲')OR (nrf.news_source = '經濟日報' AND nrf.news_category = '食尚饗宴')OR (nrf.news_source = '自由時報' AND nrf.news_category = '財經新聞')OR (nrf.news_source = '風傳媒' AND nrf.news_category = '財經')\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'記者施怡妏／綜合報導 台積電（2330）先前衝破600元大關後不斷創新高，令不少投資人心動跟著上車，甚至不惜高價買進，怎料近期卻接連下跌，有男網友當時看到股價狂飆心癢癢，在台積電歷史新高679元時花136萬買了兩張，沒想到之後股價竟然一路下跌，他因此感到很苦惱，「大概什麼時候可以解套呢？」 男網友在Dcard上發文，在台積電歷史新高679元時買進兩張，大概花了136萬元，但股價卻沒有繼續漲，「如果把台積電發的股利也算進去，每三個月能領5000元」，想知道大概要過多久才可以解套，「能把賣掉兩張台積電的錢，加上台積電兩張的股利收入，總金額大於成本136萬元」。 貼文一出後，不少人驚訝表示「怎麼那麼剛好買在最高點」，「你怎麼可能剛好買在歷史最高點，679元買兩張，你是神嗎」、「下次要買麻煩告知一下，謝謝，我想賣在最高點」、「知道股票解套時間的早就賺翻了，還在這邊回你」、「股神吧，買在最高點欸」。 有人建議他直接賣掉停損，「勸你現在下車，現在籌碼太亂了出來觀望比較好」、「上來問就是沒信心抱住啦，建議你認賠換現金好過年，重新省思策略」。也有人建議他可以繼續放著，「買在466元的那時候很多人說會套十年，結果才過幾個月就破600，466一解套就賣掉的現在還在擦眼淚」、「大家都說上看1000，你可以慢慢等」、「不缺錢哦的話就繼續放著」。'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"記者施怡妏／綜合報導 台積電（2330）先前衝破600元大關後不斷創新高，令不少投資人心動跟著上車，甚至不惜高價買進，怎料近期卻接連下跌，有男網友當時看到股價狂飆心癢癢，在台積電歷史新高679元時花136萬買了兩張，沒想到之後股價竟然一路下跌，他因此感到很苦惱，「大概什麼時候可以解套呢？」 男網友在Dcard上發文，在台積電歷史新高679元時買進兩張，大概花了136萬元，但股價卻沒有繼續漲，「如果把台積電發的股利也算進去，每三個月能領5000元」，想知道大概要過多久才可以解套，「能把賣掉兩張台積電的錢，加上台積電兩張的股利收入，總金額大於成本136萬元」。 貼文一出後，不少人驚訝表示「怎麼那麼剛好買在最高點」，「你怎麼可能剛好買在歷史最高點，679元買兩張，你是神嗎」、「下次要買麻煩告知一下，謝謝，我想賣在最高點」、「知道股票解套時間的早就賺翻了，還在這邊回你」、「股神吧，買在最高點欸」。 有人建議他直接賣掉停損，「勸你現在下車，現在籌碼太亂了出來觀望比較好」、「上來問就是沒信心抱住啦，建議你認賠換現金好過年，重新省思策略」。也有人建議他可以繼續放著，「買在466元的那時候很多人說會套十年，結果才過幾個月就破600，466一解套就賣掉的現在還在擦眼淚」、「大家都說上看1000，你可以慢慢等」、「不缺錢哦的話就繼續放著」。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-02-19'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "str(datetime.now(tz = pytz.timezone('Asia/Taipei')).date() - timedelta(days = 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(days=7)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
